<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Quantitative Methods for Linguistic Data</title>
  <meta name="description" content="Quantitative Methods for Linguistic Data">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Quantitative Methods for Linguistic Data" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Quantitative Methods for Linguistic Data" />
  
  
  

<meta name="author" content="Morgan Sonderegger, Michael Wagner, Francisco Torreira">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="inferential-statistics-introduction.html">
<link rel="next" href="linear-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods for Linguistic Data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html"><i class="fa fa-check"></i><b>1</b> Inferential statistics: Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sample-population"><i class="fa fa-check"></i><b>1.1</b> Population vs.Â sample</a><ul>
<li class="chapter" data-level="1.1.1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sample-to-population-high-level"><i class="fa fa-check"></i><b>1.1.1</b> Sample <span class="math inline">\(\to\)</span> population: High level</a></li>
<li class="chapter" data-level="1.1.2" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sdsm"><i class="fa fa-check"></i><b>1.1.2</b> Sampling distribution of the sample mean</a></li>
<li class="chapter" data-level="1.1.3" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sampling-from-a-non-normal-distribution"><i class="fa fa-check"></i><b>1.1.3</b> Sampling from a non-normal distribution</a></li>
<li class="chapter" data-level="" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#confidence-intervals"><i class="fa fa-check"></i><b>1.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.3" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#t-distribution"><i class="fa fa-check"></i><b>1.3</b> <span class="math inline">\(t\)</span> distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#t-based-confidence-intervals"><i class="fa fa-check"></i><b>1.3.1</b> <span class="math inline">\(t\)</span>-based confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#other-reading"><i class="fa fa-check"></i><b>1.4</b> Other reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>2</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="2.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-testing-high-level"><i class="fa fa-check"></i><b>2.1</b> Hypothesis testing: High-level</a></li>
<li class="chapter" data-level="2.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#z-scores"><i class="fa fa-check"></i><b>2.2</b> <span class="math inline">\(z\)</span>-scores</a></li>
<li class="chapter" data-level="2.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#t-tests"><i class="fa fa-check"></i><b>2.3</b> <span class="math inline">\(t\)</span>-tests</a><ul>
<li class="chapter" data-level="2.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#single-sample-t-test-setup"><i class="fa fa-check"></i><b>2.3.1</b> Single-sample <span class="math inline">\(t\)</span>-test: Setup</a></li>
<li class="chapter" data-level="2.3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-testing-in-general"><i class="fa fa-check"></i><b>2.3.2</b> Hypothesis testing in general</a></li>
<li class="chapter" data-level="2.3.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#two-sample-t-test"><i class="fa fa-check"></i><b>2.3.3</b> Two-sample <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#welch-example"><i class="fa fa-check"></i><b>2.3.4</b> Unequal variances: Welch <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#t-test-assumptions"><i class="fa fa-check"></i><b>2.3.5</b> Assumptions behind <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#paired-t-test"><i class="fa fa-check"></i><b>2.3.6</b> Paired <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#reporting-a-hypothesis-test"><i class="fa fa-check"></i><b>2.3.7</b> Reporting a hypothesis test</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#checking-normality"><i class="fa fa-check"></i><b>2.4</b> Checking normality</a><ul>
<li class="chapter" data-level="2.4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#visual-methods"><i class="fa fa-check"></i><b>2.4.1</b> Visual methods</a></li>
<li class="chapter" data-level="2.4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#q-q-plots"><i class="fa fa-check"></i><b>2.4.2</b> Q-Q plots</a></li>
<li class="chapter" data-level="2.4.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#shapiro-wilk-example"><i class="fa fa-check"></i><b>2.4.3</b> Hypothesis test</a></li>
<li class="chapter" data-level="2.4.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#other-parametric-tests"><i class="fa fa-check"></i><b>2.4.4</b> Other parametric tests</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#non-parametric-tests"><i class="fa fa-check"></i><b>2.5</b> Non-parametric tests</a><ul>
<li class="chapter" data-level="2.5.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#wilcoxson-tests"><i class="fa fa-check"></i><b>2.5.1</b> Wilcoxson tests</a></li>
<li class="chapter" data-level="2.5.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#two-sample-wilcoxson-test"><i class="fa fa-check"></i><b>2.5.2</b> Two-sample Wilcoxson test</a></li>
<li class="chapter" data-level="2.5.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#parametric-versus-non-parametric-tests"><i class="fa fa-check"></i><b>2.5.3</b> Parametric versus non-parametric tests</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#other-reading-1"><i class="fa fa-check"></i><b>2.6</b> Other reading</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#regression-general-introduction"><i class="fa fa-check"></i><b>3.1</b> Regression: General introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="linear-regression.html"><a href="linear-regression.html#linear-models"><i class="fa fa-check"></i><b>3.1.1</b> Linear models</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear-regression.html"><a href="linear-regression.html#terminology"><i class="fa fa-check"></i><b>3.1.2</b> Terminology</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear-regression.html"><a href="linear-regression.html#steps-and-assumptions-of-regression-analysis"><i class="fa fa-check"></i><b>3.1.3</b> Steps and assumptions of regression analysis</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple linear regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#slr-continuous-predictor"><i class="fa fa-check"></i><b>3.2.1</b> SLR: Continuous predictor</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#slr-parameter-estimation"><i class="fa fa-check"></i><b>3.2.2</b> SLR: Parameter estimation</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.2.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.2.4" data-path="linear-regression.html"><a href="linear-regression.html#quality-of-fit"><i class="fa fa-check"></i><b>3.2.4</b> Quality of fit</a></li>
<li class="chapter" data-level="3.2.5" data-path="linear-regression.html"><a href="linear-regression.html#categorical-predictor"><i class="fa fa-check"></i><b>3.2.5</b> Categorical predictor</a></li>
<li class="chapter" data-level="3.2.6" data-path="linear-regression.html"><a href="linear-regression.html#slr-with-a-binary-categorical-predictor-vs.two-sample-t-test"><i class="fa fa-check"></i><b>3.2.6</b> SLR with a binary categorical predictor vs.Â two-sample <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression.html"><a href="linear-regression.html#goodness-of-fit-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Goodness of fit metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression.html"><a href="linear-regression.html#interactions-and-factors"><i class="fa fa-check"></i><b>3.3.2</b> Interactions and factors</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-regression.html"><a href="linear-regression.html#plotting-interactions"><i class="fa fa-check"></i><b>3.3.3</b> Plotting interactions</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-regression.html"><a href="linear-regression.html#categorical-factors-with-more-than-two-levels"><i class="fa fa-check"></i><b>3.3.4</b> Categorical factors with more than two levels</a></li>
<li class="chapter" data-level="3.3.5" data-path="linear-regression.html"><a href="linear-regression.html#releveling-factors"><i class="fa fa-check"></i><b>3.3.5</b> Releveling factors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-assumptions"><i class="fa fa-check"></i><b>3.4</b> Linear regression assumptions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="linear-regression.html"><a href="linear-regression.html#visual-methods-1"><i class="fa fa-check"></i><b>3.4.1</b> Visual methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-regression.html"><a href="linear-regression.html#assumption-1-linearity"><i class="fa fa-check"></i><b>3.4.2</b> Assumption 1: Linearity</a></li>
<li class="chapter" data-level="3.4.3" data-path="linear-regression.html"><a href="linear-regression.html#c2ioe"><i class="fa fa-check"></i><b>3.4.3</b> Assumption 2: Independence of errors</a></li>
<li class="chapter" data-level="3.4.4" data-path="linear-regression.html"><a href="linear-regression.html#assumption-3-normality-of-errors"><i class="fa fa-check"></i><b>3.4.4</b> Assumption 3: Normality of errors</a></li>
<li class="chapter" data-level="3.4.5" data-path="linear-regression.html"><a href="linear-regression.html#assumtion-4-constancy-of-variance"><i class="fa fa-check"></i><b>3.4.5</b> Assumtion 4: Constancy of variance</a></li>
<li class="chapter" data-level="3.4.6" data-path="linear-regression.html"><a href="linear-regression.html#interim-summary"><i class="fa fa-check"></i><b>3.4.6</b> Interim summary</a></li>
<li class="chapter" data-level="3.4.7" data-path="linear-regression.html"><a href="linear-regression.html#transforming-to-normality"><i class="fa fa-check"></i><b>3.4.7</b> Transforming to normality</a></li>
<li class="chapter" data-level="3.4.8" data-path="linear-regression.html"><a href="linear-regression.html#assumption-5-linear-independence-of-predictors"><i class="fa fa-check"></i><b>3.4.8</b> Assumption 5: Linear independence of predictors</a></li>
<li class="chapter" data-level="3.4.9" data-path="linear-regression.html"><a href="linear-regression.html#collinearity"><i class="fa fa-check"></i><b>3.4.9</b> Collinearity</a></li>
<li class="chapter" data-level="3.4.10" data-path="linear-regression.html"><a href="linear-regression.html#assumption-6-observations"><i class="fa fa-check"></i><b>3.4.10</b> Assumption 6: Observations</a></li>
<li class="chapter" data-level="3.4.11" data-path="linear-regression.html"><a href="linear-regression.html#lin-reg-measuring-influence"><i class="fa fa-check"></i><b>3.4.11</b> Measuring influence</a></li>
<li class="chapter" data-level="3.4.12" data-path="linear-regression.html"><a href="linear-regression.html#outliers"><i class="fa fa-check"></i><b>3.4.12</b> Outliers</a></li>
<li class="chapter" data-level="3.4.13" data-path="linear-regression.html"><a href="linear-regression.html#regression-assumptions-reassurance"><i class="fa fa-check"></i><b>3.4.13</b> Regression assumptions: Reassurance</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#lm-model-comparison"><i class="fa fa-check"></i><b>3.5</b> Model comparison</a><ul>
<li class="chapter" data-level="3.5.1" data-path="linear-regression.html"><a href="linear-regression.html#nested-model-comparison"><i class="fa fa-check"></i><b>3.5.1</b> Nested model comparison</a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-regression.html"><a href="linear-regression.html#non-nested-model-comparison"><i class="fa fa-check"></i><b>3.5.2</b> Non-nested model comparison</a></li>
<li class="chapter" data-level="3.5.3" data-path="linear-regression.html"><a href="linear-regression.html#c2varselect"><i class="fa fa-check"></i><b>3.5.3</b> Variable selection</a></li>
<li class="chapter" data-level="3.5.4" data-path="linear-regression.html"><a href="linear-regression.html#interpretability-issues"><i class="fa fa-check"></i><b>3.5.4</b> Interpretability issues</a></li>
<li class="chapter" data-level="3.5.5" data-path="linear-regression.html"><a href="linear-regression.html#interim-recipe-building-a-multiple-linear-regression-model"><i class="fa fa-check"></i><b>3.5.5</b> Interim recipe: Building a multiple linear regression model</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="linear-regression.html"><a href="linear-regression.html#c2solns"><i class="fa fa-check"></i><b>3.6</b> Solutions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-solutions"><i class="fa fa-check"></i><b>3.6.1</b> Multiple linear regression: Solutions</a></li>
<li class="chapter" data-level="3.6.2" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-assumptions-solutions"><i class="fa fa-check"></i><b>3.6.2</b> Linear regression assumptions: Solutions</a></li>
<li class="chapter" data-level="3.6.3" data-path="linear-regression.html"><a href="linear-regression.html#model-comparison-solutions"><i class="fa fa-check"></i><b>3.6.3</b> Model comparison: Solutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>4</b> Categorical data analysis: Preliminaries</a><ul>
<li class="chapter" data-level="4.1" data-path="cda.html"><a href="cda.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="cda.html"><a href="cda.html#x2-contingency-tables"><i class="fa fa-check"></i><b>4.1.1</b> 2x2 contingency tables</a></li>
<li class="chapter" data-level="4.1.2" data-path="cda.html"><a href="cda.html#the-chi-squared-test"><i class="fa fa-check"></i><b>4.1.2</b> The chi-squared test</a></li>
<li class="chapter" data-level="4.1.3" data-path="cda.html"><a href="cda.html#fishers-exact-test"><i class="fa fa-check"></i><b>4.1.3</b> Fisherâs exact test</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="cda.html"><a href="cda.html#towards-logistic-regression"><i class="fa fa-check"></i><b>4.2</b> Towards logistic regression</a><ul>
<li class="chapter" data-level="4.2.1" data-path="cda.html"><a href="cda.html#odds"><i class="fa fa-check"></i><b>4.2.1</b> Odds</a></li>
<li class="chapter" data-level="4.2.2" data-path="cda.html"><a href="cda.html#log-odds"><i class="fa fa-check"></i><b>4.2.2</b> Log-odds</a></li>
<li class="chapter" data-level="4.2.3" data-path="cda.html"><a href="cda.html#odds-ratios"><i class="fa fa-check"></i><b>4.2.3</b> Odds ratios</a></li>
<li class="chapter" data-level="4.2.4" data-path="cda.html"><a href="cda.html#log-odds-sample-and-population"><i class="fa fa-check"></i><b>4.2.4</b> Log odds: sample and population</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="cda.html"><a href="cda.html#cda-other-readings"><i class="fa fa-check"></i><b>4.3</b> Other readings</a></li>
<li class="chapter" data-level="4.4" data-path="cda.html"><a href="cda.html#c3solns"><i class="fa fa-check"></i><b>4.4</b> Solutions</a><ul>
<li class="chapter" data-level="4.4.1" data-path="cda.html"><a href="cda.html#solutions-to-exercise-1"><i class="fa fa-check"></i><b>4.4.1</b> Solutions to Exercise 1:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5</b> Logistic regression</a><ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>5.1</b> Simple logistic regression</a><ul>
<li class="chapter" data-level="5.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#log-reg-hyp-test"><i class="fa fa-check"></i><b>5.1.1</b> Hypothesis testing</a></li>
<li class="chapter" data-level="5.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#interpreting-the-coefficients-logit-odds-and-probability"><i class="fa fa-check"></i><b>5.1.2</b> Interpreting the coefficients: Logit, odds, and probability</a></li>
<li class="chapter" data-level="5.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-as-a-glm"><i class="fa fa-check"></i><b>5.1.3</b> Logistic regression as a GLM</a></li>
<li class="chapter" data-level="5.1.4" data-path="logistic-regression.html"><a href="logistic-regression.html#c4differences"><i class="fa fa-check"></i><b>5.1.4</b> Differences from linear regression: Fitting and interpretation</a></li>
<li class="chapter" data-level="5.1.5" data-path="logistic-regression.html"><a href="logistic-regression.html#fitting-a-logistic-regression-model"><i class="fa fa-check"></i><b>5.1.5</b> Fitting a logistic regression model</a></li>
<li class="chapter" data-level="5.1.6" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation"><i class="fa fa-check"></i><b>5.1.6</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression.html"><a href="logistic-regression.html#evaluating-logistic-regression-models"><i class="fa fa-check"></i><b>5.2</b> Evaluating logistic regression models</a><ul>
<li class="chapter" data-level="5.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#c4lrt"><i class="fa fa-check"></i><b>5.2.1</b> Likelihood ratio test</a></li>
<li class="chapter" data-level="5.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#classification-accuracy"><i class="fa fa-check"></i><b>5.2.2</b> Classification accuracy</a></li>
<li class="chapter" data-level="5.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-pseudo-r2"><i class="fa fa-check"></i><b>5.2.3</b> Pseudo-<span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression.html"><a href="logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>5.3</b> Multiple logistic regression</a><ul>
<li class="chapter" data-level="5.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#likelihood-ratio-test-general-case"><i class="fa fa-check"></i><b>5.3.1</b> Likelihood ratio test: General case</a></li>
<li class="chapter" data-level="5.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#log-reg-worked-example"><i class="fa fa-check"></i><b>5.3.2</b> Worked example</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression.html"><a href="logistic-regression.html#model-criticism-logistic-regression"><i class="fa fa-check"></i><b>5.4</b> Model criticism for logistic regression</a><ul>
<li class="chapter" data-level="5.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#residual-plots"><i class="fa fa-check"></i><b>5.4.1</b> Residual plots</a></li>
<li class="chapter" data-level="5.4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-cooks-distance"><i class="fa fa-check"></i><b>5.4.2</b> Cookâs distance</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="logistic-regression.html"><a href="logistic-regression.html#other-readings"><i class="fa fa-check"></i><b>5.5</b> Other readings</a></li>
<li class="chapter" data-level="5.6" data-path="logistic-regression.html"><a href="logistic-regression.html#c4solns"><i class="fa fa-check"></i><b>5.6</b> Solutions</a></li>
<li class="chapter" data-level="5.7" data-path="logistic-regression.html"><a href="logistic-regression.html#c4appendix2"><i class="fa fa-check"></i><b>5.7</b> Appendix: Other Generalized Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><i class="fa fa-check"></i><b>6</b> Practical Regression Topics 1: Multi-level factors, contrast coding, interactions</a><ul>
<li class="chapter" data-level="6.1" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#multi-level-factors-introduction"><i class="fa fa-check"></i><b>6.1</b> Multi-level factors: Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#contrast-coding"><i class="fa fa-check"></i><b>6.2</b> Contrast coding</a><ul>
<li class="chapter" data-level="6.2.1" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#first-examples"><i class="fa fa-check"></i><b>6.2.1</b> First examples</a></li>
<li class="chapter" data-level="6.2.2" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#basic-interpretation-of-contrasts"><i class="fa fa-check"></i><b>6.2.2</b> Basic interpretation of contrasts</a></li>
<li class="chapter" data-level="6.2.3" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#contrast-coding-schemes"><i class="fa fa-check"></i><b>6.2.3</b> Contrast coding schemes</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#c5mlf"><i class="fa fa-check"></i><b>6.3</b> Assessing a multi-level factorâs contribution</a></li>
<li class="chapter" data-level="6.4" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#practice-with-interactions"><i class="fa fa-check"></i><b>6.4</b> Practice with interactions</a></li>
<li class="chapter" data-level="6.5" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#c5solns"><i class="fa fa-check"></i><b>6.5</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lmem.html"><a href="lmem.html"><i class="fa fa-check"></i><b>7</b> Linear mixed models</a><ul>
<li class="chapter" data-level="7.1" data-path="lmem.html"><a href="lmem.html#mixed-effects-models-motivation"><i class="fa fa-check"></i><b>7.1</b> Mixed-effects models: Motivation</a><ul>
<li class="chapter" data-level="7.1.1" data-path="lmem.html"><a href="lmem.html#simpsons-paradox"><i class="fa fa-check"></i><b>7.1.1</b> Simpsonâs paradox</a></li>
<li class="chapter" data-level="7.1.2" data-path="lmem.html"><a href="lmem.html#repeated-measure-anovas"><i class="fa fa-check"></i><b>7.1.2</b> Repeated-measure ANOVAs</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="lmem.html"><a href="lmem.html#linear-mixed-models-1-one-grouping-factor-random-intercepts"><i class="fa fa-check"></i><b>7.2</b> Linear mixed models 1: One grouping factor, random intercepts</a><ul>
<li class="chapter" data-level="7.2.1" data-path="lmem.html"><a href="lmem.html#c6model1A"><i class="fa fa-check"></i><b>7.2.1</b> Model 1A: Simple linear regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="lmem.html"><a href="lmem.html#c6model1b"><i class="fa fa-check"></i><b>7.2.2</b> Model 1B: Random intercept only</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="lmem.html"><a href="lmem.html#c6lmm2"><i class="fa fa-check"></i><b>7.3</b> Linear mixed models 2: One grouping factor, random intercepts and slopes</a><ul>
<li class="chapter" data-level="7.3.1" data-path="lmem.html"><a href="lmem.html#c6model1c"><i class="fa fa-check"></i><b>7.3.1</b> Model 1C</a></li>
<li class="chapter" data-level="7.3.2" data-path="lmem.html"><a href="lmem.html#fitting-model-1c"><i class="fa fa-check"></i><b>7.3.2</b> Fitting Model 1C</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="lmem.html"><a href="lmem.html#linear-mixed-models-3-two-grouping-factors"><i class="fa fa-check"></i><b>7.4</b> Linear mixed models 3: Two grouping factors</a><ul>
<li class="chapter" data-level="7.4.1" data-path="lmem.html"><a href="lmem.html#c6model2A"><i class="fa fa-check"></i><b>7.4.1</b> Model 2A: By-participant and by-item random intercepts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="lmem.html"><a href="lmem.html#evaluating-lmms"><i class="fa fa-check"></i><b>7.5</b> Evaluating LMMs</a><ul>
<li class="chapter" data-level="7.5.1" data-path="lmem.html"><a href="lmem.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>7.5.1</b> Hypothesis testing</a></li>
<li class="chapter" data-level="7.5.2" data-path="lmem.html"><a href="lmem.html#significance-of-a-random-effect-term"><i class="fa fa-check"></i><b>7.5.2</b> Significance of a random effect term</a></li>
<li class="chapter" data-level="7.5.3" data-path="lmem.html"><a href="lmem.html#c6fixedp"><i class="fa fa-check"></i><b>7.5.3</b> Significance of fixed effects</a></li>
<li class="chapter" data-level="7.5.4" data-path="lmem.html"><a href="lmem.html#evaluating-goodness-of-fit"><i class="fa fa-check"></i><b>7.5.4</b> Evaluating goodness of fit</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="lmem.html"><a href="lmem.html#linear-mixed-models-4-multiple-predictors"><i class="fa fa-check"></i><b>7.6</b> Linear mixed models 4: Multiple predictors</a><ul>
<li class="chapter" data-level="7.6.1" data-path="lmem.html"><a href="lmem.html#types-of-predictors"><i class="fa fa-check"></i><b>7.6.1</b> Types of predictors</a></li>
<li class="chapter" data-level="7.6.2" data-path="lmem.html"><a href="lmem.html#c6model3A"><i class="fa fa-check"></i><b>7.6.2</b> Model 3A: Random intercepts only</a></li>
<li class="chapter" data-level="7.6.3" data-path="lmem.html"><a href="lmem.html#c6model3B"><i class="fa fa-check"></i><b>7.6.3</b> Model 3B: Random intercepts and all possible random slopes</a></li>
<li class="chapter" data-level="7.6.4" data-path="lmem.html"><a href="lmem.html#assessing-variability"><i class="fa fa-check"></i><b>7.6.4</b> Assessing variability</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="lmem.html"><a href="lmem.html#more-on-random-slopes"><i class="fa fa-check"></i><b>7.7</b> More on random slopes</a><ul>
<li class="chapter" data-level="7.7.1" data-path="lmem.html"><a href="lmem.html#what-does-adding-a-random-slope-term-do"><i class="fa fa-check"></i><b>7.7.1</b> What does adding a random slope term do?</a></li>
<li class="chapter" data-level="7.7.2" data-path="lmem.html"><a href="lmem.html#adding-a-random-slope"><i class="fa fa-check"></i><b>7.7.2</b> Discussion: Adding a random slope</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="lmem.html"><a href="lmem.html#random-effect-correlations"><i class="fa fa-check"></i><b>7.8</b> Random effect correlations</a><ul>
<li class="chapter" data-level="7.8.1" data-path="lmem.html"><a href="lmem.html#model-1e-correlated-random-slope-intercept"><i class="fa fa-check"></i><b>7.8.1</b> Model 1E: <strong>Correlated</strong> random slope &amp; intercept</a></li>
<li class="chapter" data-level="7.8.2" data-path="lmem.html"><a href="lmem.html#c6discuss"><i class="fa fa-check"></i><b>7.8.2</b> Dicussion: Adding a correlation</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="lmem.html"><a href="lmem.html#model-criticism-for-linear-mixed-models"><i class="fa fa-check"></i><b>7.9</b> Model criticism for linear mixed models</a><ul>
<li class="chapter" data-level="7.9.1" data-path="lmem.html"><a href="lmem.html#model-3b-residual-plots"><i class="fa fa-check"></i><b>7.9.1</b> Model 3B: Residual plots</a></li>
<li class="chapter" data-level="7.9.2" data-path="lmem.html"><a href="lmem.html#model-3b-random-effect-distribution"><i class="fa fa-check"></i><b>7.9.2</b> Model 3B: Random effect distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="lmem.html"><a href="lmem.html#c6factorsissue"><i class="fa fa-check"></i><b>7.10</b> Random slopes for factors</a><ul>
<li class="chapter" data-level="7.10.1" data-path="lmem.html"><a href="lmem.html#model-with-random-effect-correlations"><i class="fa fa-check"></i><b>7.10.1</b> Model with random-effect correlations</a></li>
<li class="chapter" data-level="7.10.2" data-path="lmem.html"><a href="lmem.html#lmem-mwrec"><i class="fa fa-check"></i><b>7.10.2</b> Models without random-effect correlations</a></li>
</ul></li>
<li class="chapter" data-level="7.11" data-path="lmem.html"><a href="lmem.html#other-readings-1"><i class="fa fa-check"></i><b>7.11</b> Other readings</a></li>
<li class="chapter" data-level="7.12" data-path="lmem.html"><a href="lmem.html#c6extraexamples"><i class="fa fa-check"></i><b>7.12</b> Appendix: Extra examples</a><ul>
<li class="chapter" data-level="7.12.1" data-path="lmem.html"><a href="lmem.html#lmm-simulation-confint"><i class="fa fa-check"></i><b>7.12.1</b> Predicting confidence intervals by simulation</a></li>
<li class="chapter" data-level="7.12.2" data-path="lmem.html"><a href="lmem.html#random-intercept-and-slope-model-for-givenness-data"><i class="fa fa-check"></i><b>7.12.2</b> Random intercept and slope model for <code>givenness</code> data</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="lmem.html"><a href="lmem.html#c6extendedexercise"><i class="fa fa-check"></i><b>7.13</b> Appendix: Extended exercise</a></li>
<li class="chapter" data-level="7.14" data-path="lmem.html"><a href="lmem.html#c6solns"><i class="fa fa-check"></i><b>7.14</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Mixed-effects logistic regression</a><ul>
<li class="chapter" data-level="8.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#preliminaries"><i class="fa fa-check"></i><b>8.1</b> Preliminaries</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#motivation"><i class="fa fa-check"></i><b>8.1.1</b> Motivation</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#basics"><i class="fa fa-check"></i><b>8.2</b> Basics</a><ul>
<li class="chapter" data-level="8.2.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7m1"><i class="fa fa-check"></i><b>8.2.1</b> Model 1: <code>givenness</code> data, crossed random effects (intercepts + slopes)</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>8.3</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="8.3.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#fixed-effects"><i class="fa fa-check"></i><b>8.3.1</b> Fixed effects</a></li>
<li class="chapter" data-level="8.3.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#random-effects"><i class="fa fa-check"></i><b>8.3.2</b> Random effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#fixed-and-random-effects"><i class="fa fa-check"></i><b>8.4</b> Fixed and random effects</a></li>
<li class="chapter" data-level="8.5" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#melr-practice"><i class="fa fa-check"></i><b>8.5</b> MELR Practice</a><ul>
<li class="chapter" data-level="8.5.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7ex1"><i class="fa fa-check"></i><b>8.5.1</b> Exercise 1: tapping</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#model-criticism-for-mixed-effects-logistic-regression"><i class="fa fa-check"></i><b>8.6</b> Model criticism for mixed-effects logistic regression</a><ul>
<li class="chapter" data-level="8.6.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#random-effect-distributions"><i class="fa fa-check"></i><b>8.6.1</b> Random-effect distributions</a></li>
<li class="chapter" data-level="8.6.2" data-path="logistic-regression.html"><a href="logistic-regression.html#residual-plots"><i class="fa fa-check"></i><b>8.6.2</b> Residual plots</a></li>
<li class="chapter" data-level="8.6.3" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#influence"><i class="fa fa-check"></i><b>8.6.3</b> Influence</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#evaluation-measures"><i class="fa fa-check"></i><b>8.7</b> Evaluation measures</a><ul>
<li class="chapter" data-level="8.7.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#evaluation-measure-1-likelihood-ratio-test"><i class="fa fa-check"></i><b>8.7.1</b> Evaluation measure 1: Likelihood ratio test</a></li>
<li class="chapter" data-level="8.7.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#evaluation-measure-2-classification-accuracy"><i class="fa fa-check"></i><b>8.7.2</b> Evaluation measure 2: Classification accuracy</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#miscellaneous-mixed-effects-regression-topics"><i class="fa fa-check"></i><b>8.8</b> Miscellaneous mixed-effects regression topics</a><ul>
<li class="chapter" data-level="8.8.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7m2"><i class="fa fa-check"></i><b>8.8.1</b> Random-effect correlation issues</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#other-readings-2"><i class="fa fa-check"></i><b>8.9</b> Other readings</a></li>
<li class="chapter" data-level="8.10" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#appendices"><i class="fa fa-check"></i><b>8.10</b> Appendices</a><ul>
<li class="chapter" data-level="8.10.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#melr-random-slopes-for-factors"><i class="fa fa-check"></i><b>8.10.1</b> Appendix: Random slopes for factors</a></li>
<li class="chapter" data-level="8.10.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7appendix2"><i class="fa fa-check"></i><b>8.10.2</b> Appendix: Multi-level factors and uncorrelated random effects</a></li>
<li class="chapter" data-level="8.10.3" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#appendix-what-can-happen-if-a-random-slope-isnt-included"><i class="fa fa-check"></i><b>8.10.3</b> Appendix: What can happen if a random slope isnât included?</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7solns"><i class="fa fa-check"></i><b>8.11</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><i class="fa fa-check"></i><b>9</b> Practical regression topics 2: Ordered factors, nonlinear effects, model predictions, post-hoc tests</a><ul>
<li class="chapter" data-level="9.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#ordered-factors"><i class="fa fa-check"></i><b>9.2</b> Ordered factors</a><ul>
<li class="chapter" data-level="9.2.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#orthogonal-polynomial-contrasts"><i class="fa fa-check"></i><b>9.2.1</b> Orthogonal polynomial contrasts</a></li>
<li class="chapter" data-level="9.2.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#using-an-ordered-factor-as-a-predictor"><i class="fa fa-check"></i><b>9.2.2</b> Using an ordered factor as a predictor</a></li>
<li class="chapter" data-level="9.2.3" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#further-points"><i class="fa fa-check"></i><b>9.2.3</b> Further points</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#nonlinear-effects"><i class="fa fa-check"></i><b>9.3</b> Nonlinear effects</a><ul>
<li class="chapter" data-level="9.3.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#splines-definition-and-benefits"><i class="fa fa-check"></i><b>9.3.1</b> Splines: Definition and benefits</a></li>
<li class="chapter" data-level="9.3.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#restricted-cubic-splines"><i class="fa fa-check"></i><b>9.3.2</b> Restricted cubic splines</a></li>
<li class="chapter" data-level="9.3.3" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#choosing-spline-complexity"><i class="fa fa-check"></i><b>9.3.3</b> Choosing spline complexity</a></li>
<li class="chapter" data-level="9.3.4" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#rcs-components"><i class="fa fa-check"></i><b>9.3.4</b> RCS components</a></li>
<li class="chapter" data-level="9.3.5" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#using-rcs-in-a-mixed-model"><i class="fa fa-check"></i><b>9.3.5</b> Using RCS in a mixed model</a></li>
<li class="chapter" data-level="9.3.6" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#random-slopes-for-rcs-terms"><i class="fa fa-check"></i><b>9.3.6</b> Random slopes for RCS terms</a></li>
<li class="chapter" data-level="9.3.7" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#nonlinear-effects-summary"><i class="fa fa-check"></i><b>9.3.7</b> Nonlinear effects: Summary</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#predictions-from-mixed-models"><i class="fa fa-check"></i><b>9.4</b> Predictions from mixed models</a><ul>
<li class="chapter" data-level="9.4.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#making-model-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Making Model Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#simulation-based-predictions"><i class="fa fa-check"></i><b>9.4.2</b> Simulation-based predictions</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#post-hoc-mult-comp"><i class="fa fa-check"></i><b>9.5</b> Post-hoc tests and multiple comparisons</a></li>
<li class="chapter" data-level="9.6" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#c8indivpreds"><i class="fa fa-check"></i><b>9.6</b> Appendix: Model predictions for indiviudal participants</a><ul>
<li class="chapter" data-level="9.6.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#predictions-incorporating-offsets-for-individual-speakers"><i class="fa fa-check"></i><b>9.6.1</b> Predictions incorporating offsets for individual speakers</a></li>
<li class="chapter" data-level="9.6.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#predicted-williams-effect-for-each-speaker"><i class="fa fa-check"></i><b>9.6.2</b> Predicted Williams effect for each speaker</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#c8slopesForFactors"><i class="fa fa-check"></i><b>9.7</b> Appendix: Random slopes for factors</a></li>
<li class="chapter" data-level="9.8" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#c8solns"><i class="fa fa-check"></i><b>9.8</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="datasets-appendix.html"><a href="datasets-appendix.html"><i class="fa fa-check"></i><b>10</b> Appendix: Datasets and packages</a><ul>
<li class="chapter" data-level="10.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#engdata"><i class="fa fa-check"></i><b>10.1</b> <code>english</code> lexical decision and naming latencies</a></li>
<li class="chapter" data-level="10.2" data-path="datasets-appendix.html"><a href="datasets-appendix.html#dutch-regularity"><i class="fa fa-check"></i><b>10.2</b> Dutch <code id="dregdata">regularity</code></a></li>
<li class="chapter" data-level="10.3" data-path="datasets-appendix.html"><a href="datasets-appendix.html#european-french-phrase-medial-vowel-devoicing"><i class="fa fa-check"></i><b>10.3</b> European French phrase-medial vowel <code id="devdata">devoicing</code></a><ul>
<li class="chapter" data-level="10.3.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background"><i class="fa fa-check"></i><b>10.3.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="datasets-appendix.html"><a href="datasets-appendix.html#north-american-english-tapping"><i class="fa fa-check"></i><b>10.4</b> North American English <code id="tapdata">tapping</code></a><ul>
<li class="chapter" data-level="10.4.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-1"><i class="fa fa-check"></i><b>10.4.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="datasets-appendix.html"><a href="datasets-appendix.html#halfdata"><i class="fa fa-check"></i><b>10.5</b> <code>halfrhyme</code>: English half-rhymes</a></li>
<li class="chapter" data-level="10.6" data-path="datasets-appendix.html"><a href="datasets-appendix.html#givedata"><i class="fa fa-check"></i><b>10.6</b> <code>givenness</code> data: the Williams Effect</a><ul>
<li class="chapter" data-level="10.6.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-2"><i class="fa fa-check"></i><b>10.6.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="datasets-appendix.html"><a href="datasets-appendix.html#alternatives"><i class="fa fa-check"></i><b>10.7</b> <code id="altdata">alternatives</code></a><ul>
<li class="chapter" data-level="10.7.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-3"><i class="fa fa-check"></i><b>10.7.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="datasets-appendix.html"><a href="datasets-appendix.html#votdata"><i class="fa fa-check"></i><b>10.8</b> VOT</a><ul>
<li class="chapter" data-level="10.8.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-4"><i class="fa fa-check"></i><b>10.8.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="datasets-appendix.html"><a href="datasets-appendix.html#transitionsdata"><i class="fa fa-check"></i><b>10.9</b> Transitions</a></li>
<li class="chapter" data-level="10.10" data-path="datasets-appendix.html"><a href="datasets-appendix.html#packages"><i class="fa fa-check"></i><b>10.10</b> Packages</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Methods for Linguistic Data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hypothesis-testing" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Hypothesis testing</h1>
<p><strong>Preliminary code</strong></p>
<p>This code is needed to make other code below work:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gridExtra) <span class="co"># for grid.arrange() to print plots side-by-side</span>
<span class="kw">library</span>(languageR)
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)

## loads transitions.txt from OSF project for Roberts, Torreira, &amp; Levinson (2015) data
transitions &lt;-<span class="st"> </span><span class="kw">read.delim</span>(<span class="kw">url</span>(<span class="st">&quot;https://osf.io/4v8r7/download&quot;</span>))</code></pre></div>
<script src="js/hideOutput.js"></script>
<p>In this chapter we will introduce:</p>
<ul>
<li><p>hypothesis testing, at a high level,</p></li>
<li><p><span class="math inline">\(t\)</span> tests, the most commonly-used hypothesis test, in some detail;</p></li>
<li><p>other hypothesis tests, including non-parametric tests;</p></li>
<li><p>and some necessary adjacent concepts, such as <span class="math inline">\(Z\)</span>-scores.</p></li>
</ul>
<!-- TODO FUTURE: solutions -->
<div id="hypothesis-testing-high-level" class="section level2">
<h2><span class="header-section-number">2.1</span> Hypothesis testing: High-level</h2>
<p>Based on a sample of <span class="math inline">\(n\)</span> observations, we can compute the:</p>
<ul>
<li><p><em>sample mean</em> (<span class="math inline">\(\bar{x}\)</span>)</p></li>
<li><p>and the <em>standard error</em> (<span class="math inline">\(SE\)</span> = <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>)</p></li>
</ul>
<p>concepts introduced <a href="inferential-statistics-introduction.html#sdsm">in the previous chapter</a>. The sample mean is normally distributed (by the central limit theorem), so <span class="math inline">\(\mu \pm 2\frac{\sigma}{\sqrt{n}}\)</span> contains approximately 95% of the probability mass.</p>
<p>For concreteness, say we find that <span class="math inline">\(\bar{x}\)</span> is 10 and SE = 5.</p>
<p>We donât know the population mean <span class="math inline">\(\mu\)</span>, but suppose we have a <strong>hypothesis</strong> about it that is meaningful: say <span class="math inline">\(\mu = 0\)</span>, which could mean âthere is no effectâ (of whatever weâre measuring).</p>
<p><strong>If</strong> we knew the population standard deviation <span class="math inline">\(\sigma\)</span>, we could answer the question: âis the sample mean far enough away from 0 to be 95% sure that <span class="math inline">\(\mu \neq 0\)</span>?â</p>
<p>However, we donât know <span class="math inline">\(\sigma\)</span>. Instead, we can approximate it by using the <span class="math inline">\(SE\)</span> of the sample (see <a href="inferential-statistics-introduction.html#sdsm">here</a>). Then we can calculate the probability distribution of values the sample mean could take on, given <span class="math inline">\(\mu = 0\)</span>, and given some uncertainty in using <span class="math inline">\(SE\)</span>âand use this distribution to answer our question. This procedure is called <em>hypothesis testing</em>.</p>
<div id="example-1" class="section level4 unnumbered">
<h4>Example</h4>
<p>Suppose that for some sample:</p>
<ul>
<li><p>Sample mean = 10</p></li>
<li><p>SE = 5</p></li>
<li><p>Hypothesized <span class="math inline">\(\mu\)</span> = 0.</p></li>
</ul>
<p>In this example, we are assuming that we (magically) know the the true value of the standard error (using <span class="math inline">\(\sigma\)</span>).</p>
<p>This plot shows how far out the sample mean (10) lies, for a normal distribution with mean 0 and standard deviation 5:</p>
<p><img src="02-hypothesis-testing_files/figure-html/unnamed-chunk-2-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>We would like to be able to answer: is the sample mean far enough away from <span class="math inline">\(\mu\)</span> to be 95% sure that <span class="math inline">\(\mu\)</span> isnât 0?</p>
</div>
</div>
<div id="z-scores" class="section level2">
<h2><span class="header-section-number">2.2</span> <span class="math inline">\(z\)</span>-scores</h2>
<p>A <em>z-score</em> measures how many standard deviations an observation <span class="math inline">\(x_i\)</span> is from the mean:</p>
<span class="math display">\[\begin{equation*}
  z_i = \frac{x_i - \mu}{\sigma}
\end{equation*}\]</span>
<p>If the observations are normally distributed, the <span class="math inline">\(z_i\)</span> values are normally distributed as well, with mean 0 and standard deviation 1. (Note that <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> here are the <strong>population</strong> parameters.)</p>
<p>When you draw from any random variable <span class="math inline">\(z\)</span> with a normal distribution:</p>
<ul>
<li><p>The probability of a value with <span class="math inline">\(|z| &gt; 1.96\)</span> is 0.05</p></li>
<li><p>Probability of a value with <span class="math inline">\(|z| &gt; 2\)</span> is 0.04</p></li>
<li><p>Probability of a value with <span class="math inline">\(|z| &gt; 3\)</span> is 0.002</p></li>
</ul>
<div id="example-2" class="section level4 unnumbered">
<h4>Example</h4>
<p>Returning to the question posed in our example above: âIs the sample mean far enough away from <span class="math inline">\(\mu\)</span> to be 95% sure that <span class="math inline">\(\mu\)</span> isnât 0?â</p>
<p>The sample mean has standard deviation = SE. Thus, a <span class="math inline">\(z\)</span> score for the sample mean weâve computed is (sample mean - 0)/SE :</p>
<p><span class="math inline">\(z = (10-0)/5 = 2\)</span></p>
<p>Thus, the probability of observing a sample mean at least this far from 0 is 0.04. The letter <span class="math inline">\(p\)</span> (the <em>significance</em>) is conventionally used for âthe probability of observing a value at least this bigâ in hypothesis testing, and written <span class="math inline">\(p = 0.04\)</span>.</p>
<p>In this informal example, we assumed that we somehow knew SEâthe population standard deviationâbut this isnât usually the case.</p>
</div>
</div>
<div id="t-tests" class="section level2">
<h2><span class="header-section-number">2.3</span> <span class="math inline">\(t\)</span>-tests</h2>
<div id="single-sample-t-test-setup" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Single-sample <span class="math inline">\(t\)</span>-test: Setup</h3>
<p>In the setting for a (single-sample) <span class="math inline">\(t\)</span>-test, we assume that we have <span class="math inline">\(n\)</span> observations of some normally distributed random variable, such as log-transformed reaction times (<a href="datasets-appendix.html#engdata"><code>english</code> dataset</a>) or duration of vowels (<a href="datasets-appendix.html#tapdata"><code>tapping</code> dataset</a>). In reality the random variable has some mean and standard deviation <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, population values that we donât know.</p>
<p>Our question is: is the mean of the variable (<span class="math inline">\(\mu\)</span>) different from some constant <span class="math inline">\(\mu_0\)</span> (usually 0)?</p>
To test this, as in the informal example above, we would like to calculate a <span class="math inline">\(z\)</span> score:<br />

<span class="math display">\[\begin{equation*}
  z = \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}}
\end{equation*}\]</span>
However, we typically do not know the population standard deviation <span class="math inline">\(\sigma\)</span>, so we estimate it using the sample standard deviation:
<span class="math display" id="eq:hyptest1">\[\begin{equation}
  s = \sqrt{\frac{\sum^n_{i = 1} \left(x_i - \bar{x}\right)^2}{n - 1}}
    \tag{2.1}
\end{equation}\]</span>
The <span class="math inline">\(t\)</span>-statistic is:
<span class="math display">\[\begin{equation*}
  t = \frac{\bar{x}-\mu_0}{s/\sqrt{n}}
\end{equation*}\]</span>
<p>which is an estimate of the <span class="math inline">\(z\)</span>-score that can be calculated just using the data in the sample.</p>
<p>The <span class="math inline">\(z\)</span>-score and <span class="math inline">\(t\)</span>-statistic are random variables:</p>
<ul>
<li><p>The <span class="math inline">\(z\)</span>-score follows an <span class="math inline">\(N(0,1)\)</span> distribution (normal distribution with mean 0, standard deviation 1).</p></li>
<li><p>The <span class="math inline">\(t\)</span>-statistic follows the <em><span class="math inline">\(t\)</span>-distribution</em> with <span class="math inline">\(n - 1\)</span> <em>degrees of freedom</em>.</p></li>
</ul>
<p>This plot shows the <span class="math inline">\(t\)</span> distribution for several different degrees of freedom values, as well as the standard normal distribution (<span class="math inline">\(Z\)</span>):</p>
<p><img src="02-hypothesis-testing_files/figure-html/unnamed-chunk-3-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Note that the <span class="math inline">\(t\)</span> distributions have âfatter tailsâ than the normal distribution. (Why, intuitively?)</p>
<p><span class="math inline">\(t\)</span> in this case is the <em>test statistic</em>âa value we compute based on the sample, which we will then evaluate using a hypothesis test evaluating two options:</p>
<ul>
<li><p>The <em>null hypothesis</em> is <span class="math inline">\(H_0~:~\mu=\mu_0\)</span></p></li>
<li><p>The <em>alternative hypothesis</em> is <span class="math inline">\(H_a~:~\mu\neq\mu_0\)</span></p></li>
</ul>
<p>That is: the population mean is either the ânull valueâ (usually 0), or it isnât.</p>
<p>The logic of hypothesis testing, in this case:<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<ul>
<li><p><strong>If</strong> <span class="math inline">\(H_0\)</span> were true (âunder the null hypothesisâ), <span class="math inline">\(t\)</span> would follow a <span class="math inline">\(t_{n-1}\)</span> distribution.</p></li>
<li><p>Calculate how likely we are to get a value of <span class="math inline">\(t\)</span> at least as extreme as the value we observed, using the <span class="math inline">\(t\)</span>-distribution. This is <span class="math inline">\(p\)</span>, or the <em><span class="math inline">\(p\)</span>-value</em>.</p></li>
<li><p>If <span class="math inline">\(p\)</span> is less than the <em>significance level</em> <span class="math inline">\(\alpha\)</span> we reject the null hypothesis.</p></li>
</ul>
<p><span class="math inline">\(\alpha\)</span> is a number between 0 and 1 parametrizing how certain we have to be to reject the null. Usually <span class="math inline">\(\alpha\)</span> is taken to be 0.05 (95% certain), but this is just convention.</p>
<p>Note that âat least this extremeâ refers to a <em>two-tailed</em> significance test, which is by far the most commonly-used in practice. (For example, the default for Râs <code>t.test</code> is a two-tailed test.)</p>
<p>It is also possible to carry out a <em>one-tailed</em> significance test, which asks: âHow likely is a value at least this <strong>positive</strong> of the test statistic?â (Or âat least this negative.â) The <a href="https://en.wikipedia.org/wiki/One-_and_two-tailed_tests">Wikipedia page on one/two-tailed tests</a> has a good visualization of the one-tailed test.</p>
<p>Among the reasons two-tailed significance tests are the default:</p>
<ul>
<li><p>They are âmore conservativeâ: <span class="math inline">\(p\)</span> values will be higher, making it less likely to reject the null hypothesis in a case where it is actually true (a âType I errorâ).</p></li>
<li><p>They do not require you to choose a direction (âpositiveâ or ânegativeâ above) to apply a one-sided test, instead remaining agnostic on the direction of any observed effect.</p></li>
</ul>
<!-- # -->
<!-- ## Confidence intervals: refresher -->
<!-- Given $H_0$ and $\alpha$, we can also calculate *confidence intervals* for a hypothesis test: the values of  $\mu_0$ for which we would **not** reject $H_0$. -->
<!-- Most commonly calcualted are confidence intervals for: -->
<!-- * $\alpha = 0.05$: the "95% CI" -->
<!-- * $\alpha = 0.01$: the "99% CI" -->
<!-- #### Example -->
<!-- (From ) -->
<!-- Suppose that in a sample of $n=9$ points: -->
<!-- * Sample mean = 6.33 -->
<!-- * SE = 2.03 -->
<!-- Then the $t$ statistic is: -->
<!-- \begin{equation*} -->
<!--   t = \frac{\bar{x}-\mu_0}{s/\sqrt{n}} -->
<!-- \end{equation*} -->
<!-- * What values of $\mu_0$ would give $t$ this many SE's away? -->
<!-- * (Source: Maindonald & Braun) -->
<!-- --- -->
<!-- * In a $t_8$ distribution, 95% of the probability mass appears within [-2.31, 2.31] -->
<!-- * Then, our 95% CI is computed to be -->
<!-- \begin{equation*} -->
<!--   [6.33 - 2.03\cdot 2.31, 6.33 + 2.03 \cdot 2.31] = [1.64, 11.01] -->
<!-- \end{equation*} -->
<!-- * **Interpretation**: -->
<!--     * If this experiment were repeated many times, the population mean $\mu_0$ will lie within the CI 95% of the time. -->
<!-- * Note that the 95% CI does not include 0. We may conclude that $\mu_0$ is not equal to 0 with a 95% confidence level. -->
<!-- --- -->
<p>Classic textbooks at this point run through examples of calculating <span class="math inline">\(t\)</span> statistics and carrying out <span class="math inline">\(t\)</span> tests by hand, using tables to calculate <span class="math inline">\(p\)</span> for a given sample. This is a bit of a contrived exercise in 2018, when <span class="math inline">\(t\)</span>-tests can just be carried out automatically (even in Excel). But it remains very important to have a good <strong>conceptual</strong> understanding of how <span class="math inline">\(t\)</span>-statistics and <span class="math inline">\(t\)</span>-tests work (as well as the easier case of <span class="math inline">\(z\)</span>-statistics and <span class="math inline">\(z\)</span> tests), because the underlying concepts are fundamental to most statistical methods used in current practice in language sciences (e.g.Â any regression model).</p>
</div>
<div id="hypothesis-testing-in-general" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Hypothesis testing in general</h3>
<p>More generally, (null-significance) hypothesis testing follows these steps:</p>
<ol start="0" style="list-style-type: decimal">
<li><p>Choose a significance level, <span class="math inline">\(\alpha\)</span></p></li>
<li><p>Formulate a null hypothesis, <span class="math inline">\(H_0\)</span></p></li>
<li><p>Formulate an alternative hypothesis, <span class="math inline">\(H_a\)</span></p></li>
<li><p>Gather data, calculate a test statistic, <span class="math inline">\(T\)</span></p></li>
<li><p>Determine the probability of obtaining <span class="math inline">\(T\)</span> âor a more extreme valueâ under <span class="math inline">\(H_0\)</span>, the <span class="math inline">\(p\)</span>-value</p></li>
<li><p>If <span class="math inline">\(p \leq \alpha\)</span>, reject <span class="math inline">\(H_0\)</span></p></li>
</ol>
<p>This procedure underlies most inferential statistics used in language sciencesâ<span class="math inline">\(t\)</span>-tests, ANOVAs, linear regressions, mixed-effects regressionsâthough the steps are not usually explicitly stated.</p>
<p>In particular, it is often assumed that:</p>
<ul>
<li><p><span class="math inline">\(\alpha=0.05\)</span></p></li>
<li><p>The null hypothesis is âno differenceâ or âparameter is zeroâ</p></li>
<li><p>A two-tailed test is used</p></li>
</ul>
<p>unless stated otherwise. The reader of a paper or book (including this one) often must infer from context which test is being used: the type of data being analyzed and the results that are shown. For example, a comparison of two groups where <span class="math inline">\(t\)</span>, d.f., <span class="math inline">\(p\)</span> are reported probably means a two-sided <span class="math inline">\(t\)</span> test was used.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<div id="example-one-sample-t-test" class="section level4 unnumbered">
<h4>Example: One-sample <span class="math inline">\(t\)</span>-test</h4>
<p>For this example we use a new dataset: the Dutch verb regularity data (<code>regularity</code>) from the <code>languageR</code> package, described <a href="datasets-appendix.html#dregdata">here</a>. This dataset lists 700 Dutch irregular and regular verbs (the column <code>Regularity</code>), and includes variables which may help predict whether a verb is regular or not, including:</p>
<ul>
<li><p>What <code>Auxiliary</code> is used to form certain past/passive tenses (<em>hebben</em>, <em>zijn</em>, <em>zijnheb</em>)</p></li>
<li><p>The verbâs frequency (<code>WrittenFrequency</code>)</p></li>
</ul>
<p>The mean (log) frequency for <em>hebben</em> verbs (those where the auxiliary âhebbenâ is used) is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span>regularity %&gt;%<span class="st"> </span><span class="kw">filter</span>(Auxiliary ==<span class="st"> &quot;hebben&quot;</span>)
<span class="kw">mean</span>(d$WrittenFrequency)</code></pre></div>
<pre><code>## [1] 6.494323</code></pre>
<p>Suppose for this example that we knew that this is the <strong>true mean</strong>âthe population value.</p>
<p>For other verbs:</p>
<ul>
<li><p><code>Auxiliary</code> = <em>zijn</em> (those where the auxiliary âzijnâ is used): mean frequency is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span>regularity %&gt;%<span class="st"> </span><span class="kw">filter</span>(Auxiliary ==<span class="st"> &quot;zijn&quot;</span>)
<span class="kw">mean</span>(d$WrittenFrequency)</code></pre></div>
<pre><code>## [1] 7.737747</code></pre></li>
<li><p><code>Auxiliary</code> = <em>zijnheb</em> (those where either âzijnâ or âhebbenâ can be used as auxiliaries): mean frequency is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span>regularity %&gt;%<span class="st"> </span><span class="kw">filter</span>(Auxiliary ==<span class="st"> &quot;zijnheb&quot;</span>)
<span class="kw">mean</span>(d$WrittenFrequency)</code></pre></div>
<pre><code>## [1] 6.857975</code></pre></li>
</ul>
<p>These mean values suggest that both <em>zijn</em> and <em>zijnheb</em> verbs have higher frequency on average than <em>hebben</em> verbsâwith <em>zijn</em> verbs having the highest frequencyâbut we need to conduct hypothesis tests to conclude that these differences are statistically significant.</p>
<p>First, letâs test whether <em>zijn</em> verbs have significantly different frequency from <em>hebben</em> verbs (with <span class="math inline">\(\alpha = 0.05\)</span>). In this case the null hypothesis is â<em>zijn</em> verbs have mean frequency = 6.494â, and we carry out a one-sample <span class="math inline">\(t\)</span>-test (two-sided):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span>regularity %&gt;%<span class="st"> </span><span class="kw">filter</span>(Auxiliary ==<span class="st"> &quot;zijn&quot;</span>)

<span class="kw">t.test</span>(d$WrittenFrequency, <span class="dt">mu =</span> <span class="fl">6.494</span>)</code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  d$WrittenFrequency
## t = 2.1317, df = 19, p-value = 0.04631
## alternative hypothesis: true mean is not equal to 6.494
## 95 percent confidence interval:
##  6.516539 8.958954
## sample estimates:
## mean of x 
##  7.737747</code></pre>
<p>Which suggests we can reject the null hypothesis (<span class="math inline">\(p = 0.046\)</span>): <em>zijn</em> verbs have different frequencies from <em>hebben</em> verbs.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<p>To do the same test for <em>zijnheb</em> verbs:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span>regularity %&gt;%<span class="st"> </span><span class="kw">filter</span>(Auxiliary ==<span class="st"> &quot;zijnheb&quot;</span>)

<span class="kw">t.test</span>(d$WrittenFrequency, <span class="dt">mu =</span> <span class="fl">6.494</span>)</code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  d$WrittenFrequency
## t = 2.1505, df = 102, p-value = 0.03388
## alternative hypothesis: true mean is not equal to 6.494
## 95 percent confidence interval:
##  6.522260 7.193689
## sample estimates:
## mean of x 
##  6.857975</code></pre>
<p>Which suggests that <em>zijnheb</em> verbs have different frequencies from <em>hebben</em> verbs (<span class="math inline">\(p=0.034\)</span>).</p>
<p>Note that the <span class="math inline">\(p\)</span> value is <strong>higher</strong> for the <em>zijn</em>/<em>hebben</em> comparison (less confident we can reject the null hypothesis) than for the <em>zijn</em>/<em>zijnheb</em> comparison, even though the difference in <strong>sample means</strong> is larger for <em>zijn</em>/<em>hebben</em>âas shown in the boxplot below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(regularity, <span class="kw">aes</span>(<span class="dt">x=</span>Auxiliary, <span class="dt">y=</span>WrittenFrequency)) +
<span class="st">  </span><span class="kw">geom_boxplot</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">position=</span><span class="st">&quot;jitter&quot;</span>, <span class="dt">size=</span><span class="fl">0.75</span>, <span class="dt">alpha=</span><span class="fl">0.25</span>)</code></pre></div>
<p><img src="02-hypothesis-testing_files/figure-html/unnamed-chunk-9-1.png" width="480" style="display: block; margin: auto;" /></p>
<blockquote>
<p><strong>Question</strong>:</p>
<p>Why is this?</p>
</blockquote>
</div>
</div>
<div id="two-sample-t-test" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Two-sample <span class="math inline">\(t\)</span>-test</h3>
<p>The one-sample <span class="math inline">\(t\)</span>-test is mostly used as part of more complex statistical procedures (such as linear regression). More commonly used on its own is the <em>two-sample <span class="math inline">\(t\)</span>-test</em>, to examine the difference in means between two groups.</p>
<p>As an example, for the <code>regularity</code> data, letâs divide the data up into two âtypesâ, by whether the <code>Auxiliary</code> is <em>hebben</em> or not:<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">regularity &lt;-<span class="st"> </span><span class="kw">mutate</span>(regularity, <span class="dt">type=</span><span class="kw">factor</span>(<span class="kw">ifelse</span>(Auxiliary %in%<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;zijn&quot;</span>, <span class="st">&quot;zijnheb&quot;</span>),<span class="st">&quot;nonheb&quot;</span>, <span class="st">&quot;hebben&quot;</span>)))</code></pre></div>
<p>The two verb classes seem to have similar frequencies:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>WrittenFrequency), <span class="dt">data=</span>regularity) +<span class="st"> </span><span class="kw">geom_density</span>(<span class="kw">aes</span>(<span class="dt">fill=</span>type), <span class="dt">alpha=</span><span class="fl">0.5</span>)</code></pre></div>
<p><img src="02-hypothesis-testing_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>but <em>hebben</em> verbs may have lower frequencies on average. A two-sample <span class="math inline">\(t\)</span>-test lets us test whether this difference is significant.</p>
<div id="setup" class="section level4">
<h4><span class="header-section-number">2.3.3.1</span> Setup</h4>
<p>The two-sample <span class="math inline">\(t\)</span>-test assumes two <strong>independent</strong> samples, each drawn from a normal distribution with the same standard deviation:</p>
<ul>
<li><p>Sample 1: <span class="math inline">\(n_1\)</span> observations (<span class="math inline">\(x^1_1, \ldots, x^{n_1}_1\)</span>) from <span class="math inline">\(N(\mu_1, \sigma)\)</span></p></li>
<li><p>Sample 2: <span class="math inline">\(n_2\)</span> observations (<span class="math inline">\(x^1_2, \ldots, x^{n_2}_2\)</span>)from <span class="math inline">\(N(\mu_2, \sigma)\)</span></p></li>
</ul>
<p>(This part can be skipped if math is not useful for your understanding.)</p>
<p>The sample means are the averages of the observations in each sample: <span class="math display">\[
\bar{x}_1 = \frac{\sum_{i=1}^{n_1} x^i_1}{n_1}, \quad \bar{x}_2 \frac{\sum_{i=1}^{n_2} x^i_2}{n_2}
\]</span></p>
<p>and the difference in sample means is normally distributed (because <span class="math inline">\(\bar{x}_1\)</span> and <span class="math inline">\(\bar{x}_2\)</span> are). Recall that we want to calculate <span class="math inline">\(Z\)</span>: the difference in sample means divided by its standard deviation. If we knew the standard deviation <span class="math inline">\(\sigma\)</span>, the variance of the difference in sample means would be <span class="math inline">\(\sigma^2/n_1 + \sigma^2/n_2\)</span>, and so <span class="math display">\[
Z = \frac{\bar{x}_1-\bar{x}_2}{\sigma\sqrt{(1/n_1 + 1/n_2)}}
\]</span></p>
<p>As for the one-sample <span class="math inline">\(t\)</span>-test, we donât know <span class="math inline">\(\sigma\)</span> and must estimate it from the data in the two samples. The sample standard deviations <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> are calculated as in Equation <a href="hypothesis-testing.html#eq:hyptest1">(2.1)</a>. Since we are assuming the two samples have equal variance, we calculate a single sample standard deviation <span class="math inline">\(s\)</span>: <span class="math display">\[
s = \sqrt{\frac{(n_1-1) s_1^2 + (n_2-1) s_2^2}{n_1+n_2-2}}
\]</span> That is, we are estimating the variance <span class="math inline">\(s^2\)</span> as the average of the two sample variances <span class="math inline">\(s_1^2\)</span> and <span class="math inline">\(s_2^2\)</span>, weighted by their degrees of freedomâthis turns out to be an unbiased estimator of <span class="math inline">\(\sigma\)</span>.</p>
The test statistic <span class="math inline">\(t\)</span> corresponding to the difference in sample means is then:
<span class="math display" id="eq:hyptest2">\[\begin{equation}
t = \frac{\bar{x}_1-\bar{x}_2}{s\sqrt{(1/n_1 + 1/n_2)}}
    \tag{2.2}
\end{equation}\]</span>
<p>which turns out to follow a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n_1 + n_2 - 2\)</span> degrees of freedom.</p>
<p>To carry out the two-sample hypothesis test:</p>
<ul>
<li><p>Null hypothesis: * <span class="math inline">\(H_0 ~:~ \mu_1 - \mu_2 = 0\)</span> (equal means)</p></li>
<li><p><span class="math inline">\(H_a ~:~ \mu_1 - \mu_2 \neq 0\)</span>) (means are not equal)</p></li>
<li><p>Choose <span class="math inline">\(\alpha\)</span></p></li>
<li><p>Compute <span class="math inline">\(t\)</span></p></li>
<li><p>Compute <span class="math inline">\(p\)</span>-value by seeing how far out <span class="math inline">\(t\)</span> is on a <span class="math inline">\(t_{n_1 + n_2 - 2}\)</span> distribution.</p></li>
</ul>
<p>(And also calculate (1-<span class="math inline">\(\alpha\)</span>)% confidence intervals for the difference in means, if desired.)</p>
<p>As an aside: it is worth noting that there is more uncertainty when estimating the difference between the means of the two samples (<span class="math inline">\(\bar{x}_1\)</span> and <span class="math inline">\(\bar{x}_2\)</span>)âthe denominator of Equation <a href="hypothesis-testing.html#eq:hyptest2">(2.2)</a>âthan when estimating either of these values alone.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> This illustrates a very general fact: <strong>the more things are being estimated to determine a quantity <span class="math inline">\(X\)</span>, the more uncertainty there is in our estimate of <span class="math inline">\(X\)</span></strong>.</p>
</div>
<div id="example-3" class="section level4 unnumbered">
<h4>Example</h4>
<p>To test whether the difference seen in the plot above is significant, we carry out a two-sample <span class="math inline">\(t\)</span>-test of whether <em>hebben</em> and <em>non-hebben</em> verbs differ in frequency, assuming equal variances in each group:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(regularity$WrittenFrequency ~<span class="st"> </span>regularity$type, <span class="dt">var.equal =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  regularity$WrittenFrequency by regularity$type
## t = -2.6406, df = 698, p-value = 0.008462
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.8834591 -0.1299488
## sample estimates:
## mean in group hebben mean in group nonheb 
##             6.494323             7.001027</code></pre>
<p>suggesting that <em>hebben</em> and <em>zijn</em>/<em>zijnheb</em> verbs have significantly different frequencies.</p>
</div>
</div>
<div id="welch-example" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Unequal variances: Welch <span class="math inline">\(t\)</span>-test</h3>
<p>When testing whether two samples differ there is not usually any reason to think that the samples have equal variances. In practice one should assume by default that the samples have unequal variancesâas R does in the <code>t.test</code> function.</p>
<p>This default is the <em>Welch <span class="math inline">\(t\)</span>-test</em>, which corrects the degrees of freedom and SE calculations for unequal variances. (The formulas get complicated, which is why it is useful for exposition to assume equal variances.) To run the same <span class="math inline">\(t\)</span>-test as above without assuming equal variances:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(regularity$WrittenFrequency ~<span class="st"> </span>regularity$type)</code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  regularity$WrittenFrequency by regularity$type
## t = -2.6688, df = 179.82, p-value = 0.008309
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.8813494 -0.1320584
## sample estimates:
## mean in group hebben mean in group nonheb 
##             6.494323             7.001027</code></pre>
<p>(Note the <code>Welch Two Sample t-test</code> message prior to the results.)</p>
<p>This variant of the <span class="math inline">\(t\)</span>-testâtwo-sample, unequal variancesâis by far the most commonly used in practice, and is often just called âa <span class="math inline">\(t\)</span>-testâ in papers.</p>
</div>
<div id="t-test-assumptions" class="section level3">
<h3><span class="header-section-number">2.3.5</span> Assumptions behind <span class="math inline">\(t\)</span>-test</h3>
<p>There are two key assumptions made by <span class="math inline">\(t\)</span>-tests (here we assume two-sample):</p>
<ul>
<li><p><strong>Normality</strong>: Both groups are normally distributed</p></li>
<li><p><strong>Independence</strong>: Each observation is independently sampled</p></li>
</ul>
<p>It is important to be aware of these assumptionsâespecially the second one, which is not the case for many very basic questions addressable with linguistic datasets.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
<p>It is less clear how important the normality assumption is. If your samples are clearly not normally distributedâeither by visual inspection , or for a priori reasons (e.g.Â a rating taskâcanât be normally distributed because of upper and lower bounds), you should use a non-parametric test instead (like Wilcoxson, discussed below). A common case in practice where <span class="math inline">\(t\)</span> tests are suspect is when there are clear outliers in one or both samples. However, <span class="math inline">\(t\)</span>-tests are relatively ârobustâ<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> to non-normally-distributed samples, and <strong>many</strong> papers especially in older literature report <span class="math inline">\(t\)</span>-tests on (probably) non-normal observation distributions, in part because carrying out non-parametric tests was computationally difficult before the 1990s. One shouldnât immediately discount the results of <span class="math inline">\(t\)</span>-tests for the many types of linguistic data where normality is unlikely (like Likert scores, reaction times, durations of sounds/words), but do view these results criticallyâespecially where significances are near the <span class="math inline">\(\alpha\)</span> cutoff.</p>
</div>
<div id="paired-t-test" class="section level3">
<h3><span class="header-section-number">2.3.6</span> Paired <span class="math inline">\(t\)</span>-test</h3>
<p>A useful variant of the <span class="math inline">\(t\)</span>-test, which deals with a particular violation of the independence assumption, is for <em>paired</em> data, where the two samples consist of pairs of observations (A and B), and the difference between A and B is of interest.</p>
<p>For example:</p>
<ul>
<li><p>For the <code>tapping</code> data: âtapping rate in intransitive itemsâ and âtapping rate in transitive itemsâ for the <span class="math inline">\(i\)</span>th participant.</p>
<ul>
<li>Can be used to ask: âdoes tapping rate differ by-participant between transitive and intransitive items?â</li>
</ul></li>
<li><p>For the <a href="datasets-appendix.html#transitionsdata"><code>transitions</code> data</a>:</p>
<ul>
<li><p>âMean transition duration during the first minuteââ, andâmean transition duration after the first minuteâ, for each conversation</p></li>
<li><p>Can be used to ask: âdo floor transition times differ between the first minute of a conversation and the rest of the conversation?â&quot;</p></li>
</ul></li>
</ul>
<p>The <em>paired <span class="math inline">\(t\)</span>-test</em>, essentially a one-sample <span class="math inline">\(t\)</span>-test on pairwise differences between observations, is appropriate for this type of data.</p>
<div id="example-4" class="section level4 unnumbered">
<h4>Example</h4>
<p>For the <code>transitions</code> example just given, we first set up a dataframe with three columns:</p>
<ul>
<li><p>Conversation id</p></li>
<li><p>Mean transition duration during the first minute (= 60000 msec)</p></li>
<li><p>Mean transition duration after the first minute</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span>transitions %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(file) %&gt;%
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">meanDurFirst =</span> <span class="kw">mean</span>(dur[time &lt;<span class="st"> </span><span class="dv">60000</span>]),
            <span class="dt">meanDurAfter =</span> <span class="kw">mean</span>(dur[time &gt;<span class="st"> </span><span class="dv">60000</span>]))
d</code></pre></div>
<pre><code>## # A tibble: 349 x 3
##    file       meanDurFirst meanDurAfter
##    &lt;fct&gt;             &lt;dbl&gt;        &lt;dbl&gt;
##  1 sw3154.eaf        231.        245.  
##  2 sw3155.eaf         19         153.  
##  3 sw3156.eaf        131          -9.11
##  4 sw3159.eaf        166.        -28.3 
##  5 sw3161.eaf       -352.         34.3 
##  6 sw3168.eaf         31.7       120.  
##  7 sw3169.eaf        NaN         202.  
##  8 sw3171.eaf        -76.8      -120.  
##  9 sw3174.eaf        378.         75.7 
## 10 sw3182.eaf        330.        139.  
## # ... with 339 more rows</code></pre>
<p>Test whether <code>d$meanDurFirst - d$meanDurAfter</code> is statistically significantly different from 0 (<span class="math inline">\(\alpha = 0.05\)</span>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(d$meanDurFirst, d$meanDurAfter, <span class="dt">paired=</span>T)</code></pre></div>
<pre><code>## 
##  Paired t-test
## 
## data:  d$meanDurFirst and d$meanDurAfter
## t = 5.9893, df = 346, p-value = 5.283e-09
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  42.10084 83.27295
## sample estimates:
## mean of the differences 
##                 62.6869</code></pre>
<p>Thus, there is a significant difference (<span class="math inline">\(p&lt;0.0001\)</span>).</p>
<p>Is the difference positive or negative?</p>
<!-- --- -->
<!-- The paired $t$-test is used to test for a difference between two samples, in a particular case. What if we instead just used a (non-paired) two-sample $t$-test? -->
<!-- ```{r} -->
<!-- t.test(d$meanDurFirst, d$meanDurAfter) -->
<!-- ``` -->
<!-- The difference is still significant ($p<0.0001$), but using a paired $t$-test we are more certain of the difference between the two groups: -->
<!-- * Paired $t$-test: $t=5.28$ -->
<!-- * Two-sample $t$-test: $t=4.38$ -->
<!-- ---- -->
<!-- A few notes: -->
<!-- * In this case -->
<!-- * Two-sample $t$-test does not meet assumption of independence. -->
<!-- * Paired $t$-test does not require homogeneity of variance (no need for Welch correction) -->
<!-- * Also, paired $t$-test is more powerful! -->
<!--     * Paired $t$-test: $t$ = 5.28 -->
<!--     * Two-sample $t$-test: $t$ = 4.38 -->
<!-- This is because the paired $t$-test has greater *power* to detect differences.  In general, the more estimates/fewer assumptions go into a test statistic, the broader its distribution, hence the $p$-value is larger, hence less power to detect differences. -->
</div>
</div>
<div id="reporting-a-hypothesis-test" class="section level3">
<h3><span class="header-section-number">2.3.7</span> Reporting a hypothesis test</h3>
<p>When reporting a hypothesis testâs results in a paper, you include at least</p>
<ul>
<li><p>Test statistic value</p></li>
<li><p>Any important parameter values (for <span class="math inline">\(t\)</span>-test: degrees of freedom)</p></li>
<li><p>The <span class="math inline">\(p\)</span>-value</p></li>
</ul>
<p>which are all needed to interpret your result. <!-- It is optional whether you include the actual name of the test, as long as it's a commonly-used one. --></p>
<p>It is also recommended to include appropriate descriptive statistics (for two-sample <span class="math inline">\(t\)</span>-test: mean and SD of each group) when possible; this gives a sense of effect size, which is arguably as important as significance. Even better is to include a plot showing the relevant aspects of the data (like the histogram <a href="hypothesis-testing.html#two-sample-t-test">above</a>, for a two-sample <span class="math inline">\(t\)</span>-test), but this is not always possible when writing up results due to space limitations.</p>
<p>The most commonly-used format for reporting statistics is âAPA styleâ, described various places online (e.g. <a href="http://my.ilstu.edu/~jhkahn/apastats.html">here</a>) without purchasing the APA style guide.</p>
<p>For example, one could report the two-sample <span class="math inline">\(t\)</span>-test result above as:</p>
<blockquote>
<p>Verb frequencies differ significantly by type (<span class="math inline">\(t(180)=-2.67\)</span>, <span class="math inline">\(p=0.0083\)</span>), with non-hebben verbs having higher frequency (mean=7.0, D=1.9) than hebben verbs (mean=6.5, SD=1.9).</p>
</blockquote>
<p>Note how including the second clause, instead of the (more common format):</p>
<blockquote>
<p>Verb frequencies differ significantly by type (<span class="math inline">\(t(180)=-2.67\)</span>, <span class="math inline">\(p=0.0083\)</span>), with non-hebben verbs having higher frequency than hebben verbs.</p>
</blockquote>
<p>describes a crucial aspect of the result: even though the two verb groups differ significantly in frequency, the size of this between-group difference is very small in comparison to the <strong>within-group</strong> variation in frequency.</p>
<!-- ### Summary -->
<!-- * $t$-tests -->
<!--     * One-sample: Test for difference in means from known value -->
<!--     * Two-sample: Test for difference in means between two samples -->
<!--       * In general assume unequal variances -->
<!--     * Paired: Test if mean difference between paired observations is non-zero -->
<!-- * Always assumed samples are: -->
<!--     * From normal distributions -->
<!--     * Independent -->
<!-- * The more estimates/fewer assumptions go into a test statistic, the broader its distribution $\implies$ larger $p$-value (in general) -->
</div>
</div>
<div id="checking-normality" class="section level2">
<h2><span class="header-section-number">2.4</span> Checking normality</h2>
<p>It is often useful to check whether a sample is normally distributed, for example when checking the normality assumption for <span class="math inline">\(t\)</span>-tests.</p>
<p>As a running example, we use the frequencies of verbs which take the <em>hebben</em> auxiliary in the <code>regularity</code> dataset.</p>
<div id="visual-methods" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Visual methods</h3>
<p>The simplest method is to simply examine a histogram and eyeball whether it looks like a bell curve (= normally distributed):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hebben &lt;-<span class="st"> </span>regularity %&gt;%<span class="st"> </span><span class="kw">filter</span>(Auxiliary==<span class="st">&quot;hebben&quot;</span>)
<span class="kw">ggplot</span>(hebben, <span class="kw">aes</span>(WrittenFrequency)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>()</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="02-hypothesis-testing_files/figure-html/unnamed-chunk-16-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>The histogram in this case is clearly non-symmetric, and thus not normal. But perhaps it is close to normal? Or does the right tail decay too slowly? Histograms are fine for detecting gross deviations from normality, but not more subtle ones.</p>
</div>
<div id="q-q-plots" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Q-Q plots</h3>
<p>A better visual method for assessing normality is using a <em>quantile-quantile plot</em> (or <em>Q-Q plot</em>). For each observation, a Q-Q plot shows the <strong>sample</strong> quantile (the percentile of this point, in the sample) against the quantile that would be expected from a <strong>normal distribtuion</strong> with the same mean and standard deviation as the sample. If the sample were normally distributed, these two things would be the same, and the plot would just show a straight <span class="math inline">\(y=x\)</span> line. The degree of deviation from this line thus captures the non-normality of the sample, and allows us to see which data points are âresponsibleâ for deviations from normality.</p>
<p>For the âhebbenâ frequency data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qqnorm</span>(hebben$WrittenFrequency) ## make a Q-Q plot
<span class="kw">qqline</span>(hebben$WrittenFrequency) ## plot the y=x line</code></pre></div>
<p><img src="02-hypothesis-testing_files/figure-html/unnamed-chunk-17-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>The deviations from the line correspond to parts of the histogram that are non-normal:</p>
<center>
<img src="images/qq_vs_histogram.png" />
</center>
<p>There are:</p>
<ul>
<li><p><span style="color:red"> <strong>Fewer</strong> data in left tail than expected in a normal distribution</span></p></li>
<li><p><span style="color:blue"> <strong>More</strong> data in right tail than expected in a normal distribution</span></p></li>
</ul>
</div>
<div id="shapiro-wilk-example" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Hypothesis test</h3>
<p>Normality of a sample can also be assessed using the Shapiro-Wilk hypothesis test. The null hypothesis in this case is, âthe sample is drawn from a normal distributionâ.</p>
<p>For the âhebbenâ frequency sample:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(hebben$WrittenFrequency)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  hebben$WrittenFrequency
## W = 0.97396, p-value = 1.324e-08</code></pre>
<p>The very low <span class="math inline">\(p\)</span>-value suggests that the sample is <strong>not</strong> normally distributed, confirming our intuition from visual methods.</p>
<p>This test is less useful than it may seem, because:</p>
<ul>
<li><p><strong>Any</strong> sufficiently large sample will show some deviation from normalityâeven when a Q-Q plot suggests the distribution is very close to normal.</p>
<ul>
<li><p>We are never drawing data from a 100% normal distribution, in practice.</p></li>
<li><p>For this reason, <code>shapiro.test</code> in R only works when <span class="math inline">\(n&lt;5000\)</span>.</p></li>
<li><p>See discussion <a href="https://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless">here</a>.</p></li>
</ul></li>
<li><p>A non-significant result does not let us conclude that the sample <strong>is</strong> drawn from a normal distribution.</p></li>
<li><p>A significant result doesnât say anything about what the (non-normal) distribution looks like.</p></li>
</ul>
<p>For these reasons, visual methods are preferable to the Shapiro-Wilk test for assessing normality.</p>
</div>
<div id="other-parametric-tests" class="section level3">
<h3><span class="header-section-number">2.4.4</span> Other parametric tests</h3>
<p><span class="math inline">\(z\)</span>-tests and <span class="math inline">\(t\)</span>-tests are called <em>parametric</em> hypothesis tests, because it is assumed that the sample is drawn from a distribution with a particular âfunctional formââan equation characterized by a few parameters, like a normal distribution (<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(n\)</span>) or a binomial distribution (<span class="math inline">\(p\)</span>, <span class="math inline">\(n\)</span>). For example, a two-sample <span class="math inline">\(t\)</span>-test assumes that the samples are normally distributed with means <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span>, and variances <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span>âthese are the parameters, which are referred to in the null (<span class="math inline">\(\mu_1 = \mu_2\)</span>) and alternative (<span class="math inline">\(\mu_1 \neq \mu_2\)</span>) hypotheses.</p>
<p>Some other parametric tests in common use:</p>
<ul>
<li><p>The <a href="https://en.wikipedia.org/wiki/F-test_of_equality_of_variances"><span class="math inline">\(F\)</span>-test</a> of the equality of <strong>variance</strong> between two samples</p>
<ul>
<li><p>R: <code>var.test</code></p></li>
<li><p><span class="math inline">\(F\)</span> tests arise frequently in ANOVA analyses</p></li>
</ul></li>
<li><p>The <a href="http://www.r-tutor.com/elementary-statistics/inference-about-two-populations/comparison-two-population-proportions">proportion test</a> to compare the probability of success in two groups</p>
<ul>
<li><p>R: <code>prop.test</code></p></li>
<li><p>Ex: compare the rate of tapping for âtransitiveâ and âintransitiveâ sentences (<code>tapping</code> data)</p></li>
</ul></li>
<li><p>Pearsonâs <a href="https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test">chi-squared test</a>, which can be used to test the independence of two categorical variables (see the <a href="cda.html#cda">Categorical Data Analysis chapter</a>)</p>
<ul>
<li><p>R: <code>chisq.test</code></p></li>
<li><p>Ex: assess whether tapping depends on syntactic boundary in the <code>tapped</code> dataset (test whether <code>syntax</code> and <code>tapped</code> are independent)</p></li>
</ul></li>
</ul>
</div>
</div>
<div id="non-parametric-tests" class="section level2">
<h2><span class="header-section-number">2.5</span> Non-parametric tests</h2>
<p>In contrast, <em>non-parametric</em> hypothesis tests do not assume that the sample(s) being tested are drawn from a particular type of distribution (e.g.Â normal), and tend to have fewer other assumptions. There are non-parametric analogues to all commonly-used parametric hypothesis tests.</p>
<!-- Non-parametric tests can thus be applied without worrying about aspects of the data that could mess up parametric tests (outliers, non-normality)---at the expense of lower power (ex: more likely to **not** find a significant difference between groups which do in reality differ). -->
<div id="wilcoxson-tests" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Wilcoxson tests</h3>
<p>The most commonly-used non-parametric analogue to <span class="math inline">\(t\)</span>-tests are the <em>Wilcoxson signed-rank test</em> (for one sample, or paired data) and the <em>Wilcoxson rank-sum test</em> (for two samples, a.k.a. âMann-Whitney testâ).<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></p>
<p>It is easiest to think of all these as variants of âWilcoxson testsâ, analogous to â<span class="math inline">\(t\)</span>-testsâ, which have the same variants:</p>
<ul>
<li><p>One-sample</p></li>
<li><p>Two-sample</p></li>
<li><p>Paired</p></li>
</ul>
<p>all of which are executed in R using <code>wilcox.test</code>, which automatically selects the correct Wilcoxson test depending on the arguments it is given.</p>
<p>Unlike <span class="math inline">\(t\)</span>-tests, Wilcoxson tests do not assume sample(s) are drawn from a normal distributionâthough they still have the same independence assumptions as <span class="math inline">\(t\)</span>-tests.</p>
<!-- They can thus be applied to more or less non-normal samples (meaning: drawn from non-normal distributions).   -->
</div>
<div id="two-sample-wilcoxson-test" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Two-sample Wilcoxson test</h3>
<p>The most commonly-used Wilcoxson test is the two-sample <em>rank-sum test</em>, which checks whether two samples were drawn from populations with the same distributions. This test is often described as checking whether the samples have different <strong>medians</strong> (as opposed to means, in the two-sample <span class="math inline">\(t\)</span>-test), but it is is actually more general. For example, two samples with similar medians but different <strong>variances</strong> will be significantly different using a Wilcoxson rank-sum test.</p>
<p>The null hypothesis for this test is that the two samples are drawn from identical population distributions. The test statistic is related to the sum of <strong>ranks</strong> of the data in sample 1, when all observations from both samples are combined and put in order. (The <a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test">Wikipedia page</a> gives more detail and intuitive examples, if youâre interested.)</p>
<p>The consequence of this rank-based test statistic that is important to remember is that <strong>Wilcoxson tests are robust to outliers</strong> and skewed distributionsâlike medians are.</p>
<div id="example-auxiliaries" class="section level4 unnumbered">
<h4>Example: auxiliaries</h4>
<p>We know <a href="hypothesis-testing.html#shapiro-wilk-example">from above</a> that the frequencies of Dutch âhebbenâ verbs are not normally distributed (i.e. <span class="math inline">\(p &lt; 0.0001\)</span> in a Shapiro-Wilk test), so it was not actually appropriate to compare their frequencies with non-âhebbenâ verbs <a href="hypothesis-testing.html#welch-example">using a <span class="math inline">\(t\)</span>-test</a>.</p>
<p>Letâs compare the frequencies of these two verb groups again, using a Wilcoxson test:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">wilcox.test</span>(WrittenFrequency~type, regularity)</code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  WrittenFrequency by type
## W = 29315, p-value = 0.002444
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>suggesting that <em>hebben</em> and <em>non-hebben</em> verbs have significantly different frequencies (<span class="math inline">\(p=0.002\)</span>).<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a></p>
<p>To report this result in a paper, you would use the <strong>median</strong> as a descriptive statistic:</p>
<blockquote>
<p>Verb frequencies differ significantly by type (Wilcoxson rank-sum: <span class="math inline">\(W=29315\)</span>, <span class="math inline">\(p=0.0024\)</span>), with non-hebben verbs having higher frequency (median=6.9) than hebben verbs (median=6.4).</p>
</blockquote>
<p>You would also want to be sure to convey the amount of within-group variation, either by reporting a non-parametric measure of dispersion for each group (such as interquartile range) or including a visualization such as this density plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">regularity %&gt;%<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(WrittenFrequency, <span class="dt">fill=</span>type)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha=</span><span class="fl">0.5</span>)</code></pre></div>
<p><img src="02-hypothesis-testing_files/figure-html/unnamed-chunk-20-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="parametric-versus-non-parametric-tests" class="section level3">
<h3><span class="header-section-number">2.5.3</span> Parametric versus non-parametric tests</h3>
<p>For the verb frequency example, where the data is not normally distributed, the <span class="math inline">\(t\)</span>-test has a higher <span class="math inline">\(p\)</span>-value than the Wilcoxson test:</p>
<ul>
<li><p>Two-sample <span class="math inline">\(t\)</span>-test: <span class="math inline">\(p = 0.0083\)</span></p></li>
<li><p>Two-sample Wilcoxson test: <span class="math inline">\(p = 0.0024\)</span></p></li>
</ul>
<p>When the samples being compared <strong>are</strong> normally distributed, the <span class="math inline">\(t\)</span>-tests will have a (slightly) lower <span class="math inline">\(p\)</span>-value:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">981</span>)
x1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dt">mean=</span><span class="fl">1.5</span>, <span class="dt">sd=</span><span class="fl">0.5</span>)
x2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dt">mean=</span><span class="dv">1</span>, <span class="dt">sd=</span><span class="fl">1.0</span>)
<span class="kw">t.test</span>(x1, x2)</code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  x1 and x2
## t = 5.3111, df = 147.12, p-value = 3.948e-07
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.3312187 0.7237747
## sample estimates:
## mean of x mean of y 
##  1.489870  0.962373</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">wilcox.test</span>(x1, x2)</code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  x1 and x2
## W = 6936, p-value = 2.254e-06
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>because the <span class="math inline">\(t\)</span>-test is âmore powerfulâ (less likely to miss inter-group differences that do in reality exist) for data that obeys the <span class="math inline">\(t\)</span>-testâs assumptions.</p>
<p>In general, our advice is to use Wilcoxson tests rather than <span class="math inline">\(t\)</span>-tests, because:</p>
<ul>
<li><p>They are robust to outliers.</p></li>
<li><p>You donât need to check for normality.</p></li>
<li><p>If it âmattersâ for your result whether a Wilcoxson or <span class="math inline">\(t\)</span>-test is used (e.g. <span class="math inline">\(p\)</span>-value crosses the significance threshold <span class="math inline">\(\alpha\)</span>), the effect is unreliable anyway.</p></li>
</ul>
<!-- This doesn't mean that you should immediately be suspicious of any $t$-test results you read, though---see discussion [above](#t-test-assumptions). -->
<div id="type-i-and-type-ii-error" class="section level4">
<h4><span class="header-section-number">2.5.3.1</span> Type I and Type II error</h4>
<p>When should you use parametric versus non-parametric hypothesis tests, more generally? It depends on what your data looks like, and how much you weight the risk of missing a true effect ( <em>Type II error</em>, which is one minus <em>power</em>) versus the risk of detecting a spurious effect (<em>Type I error</em>).If you have a choice between a parametric and non-parametric test (such as <span class="math inline">\(t\)</span>-tests and Wilcoxson tests):</p>
<ul>
<li><p>The parametric test will (often) be more powerful if its assumptions are met (lower Type II error), and potentially invalid otherwiseâsusceptible to detecting spurious effects (higher Type I error)<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></p></li>
<li><p>The non-parametric test potentially has less power to detect effects that in fact exist (higher Type II error), but requires fewer assumptions about the data, thus minimizing the risk of detecting spurious effects (lower Type I error)</p></li>
</ul>
<!-- * In general: the more estimates/fewer assumptions go into a test statistic, the broader its distribution -->
<!--     * $\implies$ larger $p$-value -->
<!--     * less **power** to detect differences -->
<!--     * e.g. two-sample vs. paired $t$-test; parametric vs. non-parametric -->
<p>The convention in cognitive/behavioral sciences is to prioritize minimizing Type I error over Type II error, which would suggest using the non-parametric test unless you are sure the parametric test is appropriate. However, the choice to minimize Type I error at the expense of power is mostly just tradition, and depending on your research questions it may make more sense to prioritize higher power (assuming the assumptions of the parametric test are met!). This tension between Type I error and Type II error when choosing between possible statistical toolsâand to what extent currently standard methods are due to convention versus principled reasonsâwill come up repeatedly in this book.</p>
<p>In general the more estimates/fewer assumptions go into a test statisistic, the broader its distribution. This implies a larger <span class="math inline">\(p\)</span>-value, and (assuming the null hypothesis is in fact false) means the test has less power to detect differences.</p>
</div>
</div>
</div>
<div id="other-reading-1" class="section level2">
<h2><span class="header-section-number">2.6</span> Other reading</h2>
<p>Hypothesis testing is covered in detail with R examples in many textbooks, including those for general audiences, e.g.</p>
<ul>
<li><p><span class="citation">Dalgaard (<a href="#ref-dalgaard2008introductory">2008</a>)</span>: Ch. 3 &amp; 5</p></li>
<li><p><span class="citation">Crawley (<a href="#ref-crawley2015statistics">2015</a>)</span>: Ch. 5-6</p></li>
</ul>
<p>and sources for language scientists/psychologists, such as:</p>
<ul>
<li><p><span class="citation">Vasishth &amp; Nicenboim (<a href="#ref-vasishth2016statistical">2016</a>)</span></p></li>
<li><p><span class="citation">Navarro (<a href="#ref-NavarroOnline">2015</a>)</span>: Ch. 10-11.6</p></li>
<li><p><span class="citation">Vasishth (<a href="#ref-vasishth2014introduction">2014</a>)</span>: Ch. 2</p></li>
</ul>
<!-- * Parametric vs. non-parametric hypothesis tests, generally -->
<!--     * Parametric: (often) more powerful if assumptions are met; invalid otherwise -->
<!--     * Non-parametric: no/fewer assumptions; potentially less **power** to detect differences -->

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-dalgaard2008introductory">
<p>Dalgaard, P. (2008). <em>Introductory statistics with R</em> (2nd ed.). New York, NY: Springer.</p>
</div>
<div id="ref-crawley2015statistics">
<p>Crawley, M. J. (2015). <em>Statistics: an introduction using R</em> (Second edition). Wiley.</p>
</div>
<div id="ref-vasishth2016statistical">
<p>Vasishth, S., &amp; Nicenboim, B. (2016). Statistical methods for linguistic research: Foundational ideasâPart i. <em>Language and Linguistics Compass</em>, <em>10</em>(8), 349â369.</p>
</div>
<div id="ref-NavarroOnline">
<p>Navarro, D. (2015). <em>Learning Statistics with R</em>. School of Psychology, University of Adelaide, Adelaide, Australia.</p>
</div>
<div id="ref-vasishth2014introduction">
<p>Vasishth, S. (2014). An introduction to statistical data analysis. <em>Summer 2014 Version. Available at <a href="https://github.com/vasishth/Statistics-lecture-notes-Potsdam/">Https://Github.com/Vasishth/Statistics-Lecture-Notes-Potsdam/</a></em>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>More precisely, <em>null-hypothesis significance testing</em> (NHST).<a href="hypothesis-testing.html#fnref3">â©</a></p></li>
<li id="fn4"><p>Also: with null hypothesis <span class="math inline">\(\mu_1 = \mu_2\)</span>, not assuming equal variances in the two groups, not paired.<a href="hypothesis-testing.html#fnref4">â©</a></p></li>
<li id="fn5"><p>Note that we cannot technically conclude that <em>zijn</em> verbs have <strong>higher</strong> frequencies, because we carried out a two-sided test.<a href="hypothesis-testing.html#fnref5">â©</a></p></li>
<li id="fn6"><p>This division makes sense because the auxiliary âhebbenâ can be thought of as the default case. See <a href="http://www.dutchgrammar.com/en/?n=verbs.au04">here</a> (less technical) or âNon-finite formsâ <a href="https://en.wikipedia.org/wiki/Dutch_grammar">here</a> (more technical) if interested.<a href="hypothesis-testing.html#fnref6">â©</a></p></li>
<li id="fn7"><p>That is, the standard error in the denominator is larger than <span class="math inline">\(s/\sqrt{n_1}\)</span> or <span class="math inline">\(s/\sqrt{n_2}\)</span><a href="hypothesis-testing.html#fnref7">â©</a></p></li>
<li id="fn8"><p>Ex: do observations from participant Group A and participant Group B differ? In general participants are measured more than once, violating the independence assumption for <span class="math inline">\(t\)</span>-tests.<a href="hypothesis-testing.html#fnref8">â©</a></p></li>
<li id="fn9"><p>see e.g.Â discussion [here](<a href="https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl" class="uri">https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl</a> , or <span class="citation">Bland (<a href="#ref-bland2015introduction">2015</a>)</span> p.Â 168.<a href="hypothesis-testing.html#fnref9">â©</a></p></li>
<li id="fn10"><p>The terminology is not standardized: the one-sample test is sometimes called âWilcoxson <span class="math inline">\(T\)</span>-testâ (since it is analogous to a one-sample <span class="math inline">\(t\)</span>-test), and the two sample test is also called âMann-Whitney <span class="math inline">\(U\)</span>-testâ or âMann-Whitney-Wilcoxson testâ or a variation.<a href="hypothesis-testing.html#fnref10">â©</a></p></li>
<li id="fn11"><p>The alternative hypothesis refers to a âlocation shiftâ because the null hypothesis is actually âthe two samples are drawn from distributions differing only by shifting one over by <span class="math inline">\(\mu\)</span>â, where <span class="math inline">\(\mu\)</span> is a number specified in the test. By default <span class="math inline">\(\mu=0\)</span>, which makes the null hypothesis âthe two samples are drawn from identical distributionsâ.<a href="hypothesis-testing.html#fnref11">â©</a></p></li>
<li id="fn12"><p>Using an âinvalidâ test could also lead to Type II errors, but we emphasize Type I errors for this discussion.<a href="hypothesis-testing.html#fnref12">â©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inferential-statistics-introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
