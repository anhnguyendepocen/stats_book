<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Quantitative Methods for Linguistic Data</title>
  <meta name="description" content="Quantitative Methods for Linguistic Data">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Quantitative Methods for Linguistic Data" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Quantitative Methods for Linguistic Data" />
  
  
  

<meta name="author" content="Morgan Sonderegger, Michael Wagner, Francisco Torreira">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="hypothesis-testing.html">
<link rel="next" href="cda.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods for Linguistic Data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html"><i class="fa fa-check"></i><b>1</b> Inferential statistics: Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sample-population"><i class="fa fa-check"></i><b>1.1</b> Population vs.Â sample</a><ul>
<li class="chapter" data-level="1.1.1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sample-to-population-high-level"><i class="fa fa-check"></i><b>1.1.1</b> Sample <span class="math inline">\(\to\)</span> population: High level</a></li>
<li class="chapter" data-level="1.1.2" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sdsm"><i class="fa fa-check"></i><b>1.1.2</b> Sampling distribution of the sample mean</a></li>
<li class="chapter" data-level="1.1.3" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sampling-from-a-non-normal-distribution"><i class="fa fa-check"></i><b>1.1.3</b> Sampling from a non-normal distribution</a></li>
<li class="chapter" data-level="" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#confidence-intervals"><i class="fa fa-check"></i><b>1.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.3" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#t-distribution"><i class="fa fa-check"></i><b>1.3</b> <span class="math inline">\(t\)</span> distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#t-based-confidence-intervals"><i class="fa fa-check"></i><b>1.3.1</b> <span class="math inline">\(t\)</span>-based confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#other-reading"><i class="fa fa-check"></i><b>1.4</b> Other reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>2</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="2.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-testing-high-level"><i class="fa fa-check"></i><b>2.1</b> Hypothesis testing: High-level</a></li>
<li class="chapter" data-level="2.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#z-scores"><i class="fa fa-check"></i><b>2.2</b> <span class="math inline">\(z\)</span>-scores</a></li>
<li class="chapter" data-level="2.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#t-tests"><i class="fa fa-check"></i><b>2.3</b> <span class="math inline">\(t\)</span>-tests</a><ul>
<li class="chapter" data-level="2.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#single-sample-t-test-setup"><i class="fa fa-check"></i><b>2.3.1</b> Single-sample <span class="math inline">\(t\)</span>-test: Setup</a></li>
<li class="chapter" data-level="2.3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-testing-in-general"><i class="fa fa-check"></i><b>2.3.2</b> Hypothesis testing in general</a></li>
<li class="chapter" data-level="2.3.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#two-sample-t-test"><i class="fa fa-check"></i><b>2.3.3</b> Two-sample <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#welch-example"><i class="fa fa-check"></i><b>2.3.4</b> Unequal variances: Welch <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#t-test-assumptions"><i class="fa fa-check"></i><b>2.3.5</b> Assumptions behind <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#paired-t-test"><i class="fa fa-check"></i><b>2.3.6</b> Paired <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#reporting-a-hypothesis-test"><i class="fa fa-check"></i><b>2.3.7</b> Reporting a hypothesis test</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#checking-normality"><i class="fa fa-check"></i><b>2.4</b> Checking normality</a><ul>
<li class="chapter" data-level="2.4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#visual-methods"><i class="fa fa-check"></i><b>2.4.1</b> Visual methods</a></li>
<li class="chapter" data-level="2.4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#q-q-plots"><i class="fa fa-check"></i><b>2.4.2</b> Q-Q plots</a></li>
<li class="chapter" data-level="2.4.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#shapiro-wilk-example"><i class="fa fa-check"></i><b>2.4.3</b> Hypothesis test</a></li>
<li class="chapter" data-level="2.4.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#other-parametric-tests"><i class="fa fa-check"></i><b>2.4.4</b> Other parametric tests</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#non-parametric-tests"><i class="fa fa-check"></i><b>2.5</b> Non-parametric tests</a><ul>
<li class="chapter" data-level="2.5.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#wilcoxson-tests"><i class="fa fa-check"></i><b>2.5.1</b> Wilcoxson tests</a></li>
<li class="chapter" data-level="2.5.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#two-sample-wilcoxson-test"><i class="fa fa-check"></i><b>2.5.2</b> Two-sample Wilcoxson test</a></li>
<li class="chapter" data-level="2.5.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#parametric-versus-non-parametric-tests"><i class="fa fa-check"></i><b>2.5.3</b> Parametric versus non-parametric tests</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#other-reading-1"><i class="fa fa-check"></i><b>2.6</b> Other reading</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#regression-general-introduction"><i class="fa fa-check"></i><b>3.1</b> Regression: General introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="linear-regression.html"><a href="linear-regression.html#linear-models"><i class="fa fa-check"></i><b>3.1.1</b> Linear models</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear-regression.html"><a href="linear-regression.html#terminology"><i class="fa fa-check"></i><b>3.1.2</b> Terminology</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear-regression.html"><a href="linear-regression.html#steps-and-assumptions-of-regression-analysis"><i class="fa fa-check"></i><b>3.1.3</b> Steps and assumptions of regression analysis</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple linear regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#slr-continuous-predictor"><i class="fa fa-check"></i><b>3.2.1</b> SLR: Continuous predictor</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#slr-parameter-estimation"><i class="fa fa-check"></i><b>3.2.2</b> SLR: Parameter estimation</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.2.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.2.4" data-path="linear-regression.html"><a href="linear-regression.html#quality-of-fit"><i class="fa fa-check"></i><b>3.2.4</b> Quality of fit</a></li>
<li class="chapter" data-level="3.2.5" data-path="linear-regression.html"><a href="linear-regression.html#categorical-predictor"><i class="fa fa-check"></i><b>3.2.5</b> Categorical predictor</a></li>
<li class="chapter" data-level="3.2.6" data-path="linear-regression.html"><a href="linear-regression.html#slr-with-a-binary-categorical-predictor-vs.two-sample-t-test"><i class="fa fa-check"></i><b>3.2.6</b> SLR with a binary categorical predictor vs.Â two-sample <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression.html"><a href="linear-regression.html#goodness-of-fit-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Goodness of fit metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression.html"><a href="linear-regression.html#interactions-and-factors"><i class="fa fa-check"></i><b>3.3.2</b> Interactions and factors</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-regression.html"><a href="linear-regression.html#plotting-interactions"><i class="fa fa-check"></i><b>3.3.3</b> Plotting interactions</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-regression.html"><a href="linear-regression.html#categorical-factors-with-more-than-two-levels"><i class="fa fa-check"></i><b>3.3.4</b> Categorical factors with more than two levels</a></li>
<li class="chapter" data-level="3.3.5" data-path="linear-regression.html"><a href="linear-regression.html#releveling-factors"><i class="fa fa-check"></i><b>3.3.5</b> Releveling factors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-assumptions"><i class="fa fa-check"></i><b>3.4</b> Linear regression assumptions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="linear-regression.html"><a href="linear-regression.html#visual-methods-1"><i class="fa fa-check"></i><b>3.4.1</b> Visual methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-regression.html"><a href="linear-regression.html#assumption-1-linearity"><i class="fa fa-check"></i><b>3.4.2</b> Assumption 1: Linearity</a></li>
<li class="chapter" data-level="3.4.3" data-path="linear-regression.html"><a href="linear-regression.html#c2ioe"><i class="fa fa-check"></i><b>3.4.3</b> Assumption 2: Independence of errors</a></li>
<li class="chapter" data-level="3.4.4" data-path="linear-regression.html"><a href="linear-regression.html#assumption-3-normality-of-errors"><i class="fa fa-check"></i><b>3.4.4</b> Assumption 3: Normality of errors</a></li>
<li class="chapter" data-level="3.4.5" data-path="linear-regression.html"><a href="linear-regression.html#assumtion-4-constancy-of-variance"><i class="fa fa-check"></i><b>3.4.5</b> Assumtion 4: Constancy of variance</a></li>
<li class="chapter" data-level="3.4.6" data-path="linear-regression.html"><a href="linear-regression.html#interim-summary"><i class="fa fa-check"></i><b>3.4.6</b> Interim summary</a></li>
<li class="chapter" data-level="3.4.7" data-path="linear-regression.html"><a href="linear-regression.html#transforming-to-normality"><i class="fa fa-check"></i><b>3.4.7</b> Transforming to normality</a></li>
<li class="chapter" data-level="3.4.8" data-path="linear-regression.html"><a href="linear-regression.html#assumption-5-linear-independence-of-predictors"><i class="fa fa-check"></i><b>3.4.8</b> Assumption 5: Linear independence of predictors</a></li>
<li class="chapter" data-level="3.4.9" data-path="linear-regression.html"><a href="linear-regression.html#collinearity"><i class="fa fa-check"></i><b>3.4.9</b> Collinearity</a></li>
<li class="chapter" data-level="3.4.10" data-path="linear-regression.html"><a href="linear-regression.html#assumption-6-observations"><i class="fa fa-check"></i><b>3.4.10</b> Assumption 6: Observations</a></li>
<li class="chapter" data-level="3.4.11" data-path="linear-regression.html"><a href="linear-regression.html#lin-reg-measuring-influence"><i class="fa fa-check"></i><b>3.4.11</b> Measuring influence</a></li>
<li class="chapter" data-level="3.4.12" data-path="linear-regression.html"><a href="linear-regression.html#outliers"><i class="fa fa-check"></i><b>3.4.12</b> Outliers</a></li>
<li class="chapter" data-level="3.4.13" data-path="linear-regression.html"><a href="linear-regression.html#regression-assumptions-reassurance"><i class="fa fa-check"></i><b>3.4.13</b> Regression assumptions: Reassurance</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#lm-model-comparison"><i class="fa fa-check"></i><b>3.5</b> Model comparison</a><ul>
<li class="chapter" data-level="3.5.1" data-path="linear-regression.html"><a href="linear-regression.html#nested-model-comparison"><i class="fa fa-check"></i><b>3.5.1</b> Nested model comparison</a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-regression.html"><a href="linear-regression.html#non-nested-model-comparison"><i class="fa fa-check"></i><b>3.5.2</b> Non-nested model comparison</a></li>
<li class="chapter" data-level="3.5.3" data-path="linear-regression.html"><a href="linear-regression.html#c2varselect"><i class="fa fa-check"></i><b>3.5.3</b> Variable selection</a></li>
<li class="chapter" data-level="3.5.4" data-path="linear-regression.html"><a href="linear-regression.html#interpretability-issues"><i class="fa fa-check"></i><b>3.5.4</b> Interpretability issues</a></li>
<li class="chapter" data-level="3.5.5" data-path="linear-regression.html"><a href="linear-regression.html#interim-recipe-building-a-multiple-linear-regression-model"><i class="fa fa-check"></i><b>3.5.5</b> Interim recipe: Building a multiple linear regression model</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="linear-regression.html"><a href="linear-regression.html#c2solns"><i class="fa fa-check"></i><b>3.6</b> Solutions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-solutions"><i class="fa fa-check"></i><b>3.6.1</b> Multiple linear regression: Solutions</a></li>
<li class="chapter" data-level="3.6.2" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-assumptions-solutions"><i class="fa fa-check"></i><b>3.6.2</b> Linear regression assumptions: Solutions</a></li>
<li class="chapter" data-level="3.6.3" data-path="linear-regression.html"><a href="linear-regression.html#model-comparison-solutions"><i class="fa fa-check"></i><b>3.6.3</b> Model comparison: Solutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>4</b> Categorical data analysis: Preliminaries</a><ul>
<li class="chapter" data-level="4.1" data-path="cda.html"><a href="cda.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="cda.html"><a href="cda.html#x2-contingency-tables"><i class="fa fa-check"></i><b>4.1.1</b> 2x2 contingency tables</a></li>
<li class="chapter" data-level="4.1.2" data-path="cda.html"><a href="cda.html#the-chi-squared-test"><i class="fa fa-check"></i><b>4.1.2</b> The chi-squared test</a></li>
<li class="chapter" data-level="4.1.3" data-path="cda.html"><a href="cda.html#fishers-exact-test"><i class="fa fa-check"></i><b>4.1.3</b> Fisherâs exact test</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="cda.html"><a href="cda.html#towards-logistic-regression"><i class="fa fa-check"></i><b>4.2</b> Towards logistic regression</a><ul>
<li class="chapter" data-level="4.2.1" data-path="cda.html"><a href="cda.html#odds"><i class="fa fa-check"></i><b>4.2.1</b> Odds</a></li>
<li class="chapter" data-level="4.2.2" data-path="cda.html"><a href="cda.html#log-odds"><i class="fa fa-check"></i><b>4.2.2</b> Log-odds</a></li>
<li class="chapter" data-level="4.2.3" data-path="cda.html"><a href="cda.html#odds-ratios"><i class="fa fa-check"></i><b>4.2.3</b> Odds ratios</a></li>
<li class="chapter" data-level="4.2.4" data-path="cda.html"><a href="cda.html#log-odds-sample-and-population"><i class="fa fa-check"></i><b>4.2.4</b> Log odds: sample and population</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="cda.html"><a href="cda.html#cda-other-readings"><i class="fa fa-check"></i><b>4.3</b> Other readings</a></li>
<li class="chapter" data-level="4.4" data-path="cda.html"><a href="cda.html#c3solns"><i class="fa fa-check"></i><b>4.4</b> Solutions</a><ul>
<li class="chapter" data-level="4.4.1" data-path="cda.html"><a href="cda.html#solutions-to-exercise-1"><i class="fa fa-check"></i><b>4.4.1</b> Solutions to Exercise 1:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5</b> Logistic regression</a><ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>5.1</b> Simple logistic regression</a><ul>
<li class="chapter" data-level="5.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#log-reg-hyp-test"><i class="fa fa-check"></i><b>5.1.1</b> Hypothesis testing</a></li>
<li class="chapter" data-level="5.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#interpreting-the-coefficients-logit-odds-and-probability"><i class="fa fa-check"></i><b>5.1.2</b> Interpreting the coefficients: Logit, odds, and probability</a></li>
<li class="chapter" data-level="5.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-as-a-glm"><i class="fa fa-check"></i><b>5.1.3</b> Logistic regression as a GLM</a></li>
<li class="chapter" data-level="5.1.4" data-path="logistic-regression.html"><a href="logistic-regression.html#c4differences"><i class="fa fa-check"></i><b>5.1.4</b> Differences from linear regression: Fitting and interpretation</a></li>
<li class="chapter" data-level="5.1.5" data-path="logistic-regression.html"><a href="logistic-regression.html#fitting-a-logistic-regression-model"><i class="fa fa-check"></i><b>5.1.5</b> Fitting a logistic regression model</a></li>
<li class="chapter" data-level="5.1.6" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation"><i class="fa fa-check"></i><b>5.1.6</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression.html"><a href="logistic-regression.html#evaluating-logistic-regression-models"><i class="fa fa-check"></i><b>5.2</b> Evaluating logistic regression models</a><ul>
<li class="chapter" data-level="5.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#c4lrt"><i class="fa fa-check"></i><b>5.2.1</b> Likelihood ratio test</a></li>
<li class="chapter" data-level="5.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#classification-accuracy"><i class="fa fa-check"></i><b>5.2.2</b> Classification accuracy</a></li>
<li class="chapter" data-level="5.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-pseudo-r2"><i class="fa fa-check"></i><b>5.2.3</b> Pseudo-<span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression.html"><a href="logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>5.3</b> Multiple logistic regression</a><ul>
<li class="chapter" data-level="5.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#likelihood-ratio-test-general-case"><i class="fa fa-check"></i><b>5.3.1</b> Likelihood ratio test: General case</a></li>
<li class="chapter" data-level="5.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#log-reg-worked-example"><i class="fa fa-check"></i><b>5.3.2</b> Worked example</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression.html"><a href="logistic-regression.html#model-criticism-logistic-regression"><i class="fa fa-check"></i><b>5.4</b> Model criticism for logistic regression</a><ul>
<li class="chapter" data-level="5.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#residual-plots"><i class="fa fa-check"></i><b>5.4.1</b> Residual plots</a></li>
<li class="chapter" data-level="5.4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-cooks-distance"><i class="fa fa-check"></i><b>5.4.2</b> Cookâs distance</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="logistic-regression.html"><a href="logistic-regression.html#other-readings"><i class="fa fa-check"></i><b>5.5</b> Other readings</a></li>
<li class="chapter" data-level="5.6" data-path="logistic-regression.html"><a href="logistic-regression.html#c4solns"><i class="fa fa-check"></i><b>5.6</b> Solutions</a></li>
<li class="chapter" data-level="5.7" data-path="logistic-regression.html"><a href="logistic-regression.html#c4appendix2"><i class="fa fa-check"></i><b>5.7</b> Appendix: Other Generalized Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><i class="fa fa-check"></i><b>6</b> Practical Regression Topics 1: Multi-level factors, contrast coding, interactions</a><ul>
<li class="chapter" data-level="6.1" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#multi-level-factors-introduction"><i class="fa fa-check"></i><b>6.1</b> Multi-level factors: Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#contrast-coding"><i class="fa fa-check"></i><b>6.2</b> Contrast coding</a><ul>
<li class="chapter" data-level="6.2.1" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#first-examples"><i class="fa fa-check"></i><b>6.2.1</b> First examples</a></li>
<li class="chapter" data-level="6.2.2" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#basic-interpretation-of-contrasts"><i class="fa fa-check"></i><b>6.2.2</b> Basic interpretation of contrasts</a></li>
<li class="chapter" data-level="6.2.3" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#contrast-coding-schemes"><i class="fa fa-check"></i><b>6.2.3</b> Contrast coding schemes</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#c5mlf"><i class="fa fa-check"></i><b>6.3</b> Assessing a multi-level factorâs contribution</a></li>
<li class="chapter" data-level="6.4" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#practice-with-interactions"><i class="fa fa-check"></i><b>6.4</b> Practice with interactions</a></li>
<li class="chapter" data-level="6.5" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#c5solns"><i class="fa fa-check"></i><b>6.5</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lmem.html"><a href="lmem.html"><i class="fa fa-check"></i><b>7</b> Linear mixed models</a><ul>
<li class="chapter" data-level="7.1" data-path="lmem.html"><a href="lmem.html#mixed-effects-models-motivation"><i class="fa fa-check"></i><b>7.1</b> Mixed-effects models: Motivation</a><ul>
<li class="chapter" data-level="7.1.1" data-path="lmem.html"><a href="lmem.html#simpsons-paradox"><i class="fa fa-check"></i><b>7.1.1</b> Simpsonâs paradox</a></li>
<li class="chapter" data-level="7.1.2" data-path="lmem.html"><a href="lmem.html#repeated-measure-anovas"><i class="fa fa-check"></i><b>7.1.2</b> Repeated-measure ANOVAs</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="lmem.html"><a href="lmem.html#linear-mixed-models-1-one-grouping-factor-random-intercepts"><i class="fa fa-check"></i><b>7.2</b> Linear mixed models 1: One grouping factor, random intercepts</a><ul>
<li class="chapter" data-level="7.2.1" data-path="lmem.html"><a href="lmem.html#c6model1A"><i class="fa fa-check"></i><b>7.2.1</b> Model 1A: Simple linear regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="lmem.html"><a href="lmem.html#c6model1b"><i class="fa fa-check"></i><b>7.2.2</b> Model 1B: Random intercept only</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="lmem.html"><a href="lmem.html#c6lmm2"><i class="fa fa-check"></i><b>7.3</b> Linear mixed models 2: One grouping factor, random intercepts and slopes</a><ul>
<li class="chapter" data-level="7.3.1" data-path="lmem.html"><a href="lmem.html#c6model1c"><i class="fa fa-check"></i><b>7.3.1</b> Model 1C</a></li>
<li class="chapter" data-level="7.3.2" data-path="lmem.html"><a href="lmem.html#fitting-model-1c"><i class="fa fa-check"></i><b>7.3.2</b> Fitting Model 1C</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="lmem.html"><a href="lmem.html#linear-mixed-models-3-two-grouping-factors"><i class="fa fa-check"></i><b>7.4</b> Linear mixed models 3: Two grouping factors</a><ul>
<li class="chapter" data-level="7.4.1" data-path="lmem.html"><a href="lmem.html#c6model2A"><i class="fa fa-check"></i><b>7.4.1</b> Model 2A: By-participant and by-item random intercepts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="lmem.html"><a href="lmem.html#evaluating-lmms"><i class="fa fa-check"></i><b>7.5</b> Evaluating LMMs</a><ul>
<li class="chapter" data-level="7.5.1" data-path="lmem.html"><a href="lmem.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>7.5.1</b> Hypothesis testing</a></li>
<li class="chapter" data-level="7.5.2" data-path="lmem.html"><a href="lmem.html#significance-of-a-random-effect-term"><i class="fa fa-check"></i><b>7.5.2</b> Significance of a random effect term</a></li>
<li class="chapter" data-level="7.5.3" data-path="lmem.html"><a href="lmem.html#c6fixedp"><i class="fa fa-check"></i><b>7.5.3</b> Significance of fixed effects</a></li>
<li class="chapter" data-level="7.5.4" data-path="lmem.html"><a href="lmem.html#evaluating-goodness-of-fit"><i class="fa fa-check"></i><b>7.5.4</b> Evaluating goodness of fit</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="lmem.html"><a href="lmem.html#linear-mixed-models-4-multiple-predictors"><i class="fa fa-check"></i><b>7.6</b> Linear mixed models 4: Multiple predictors</a><ul>
<li class="chapter" data-level="7.6.1" data-path="lmem.html"><a href="lmem.html#types-of-predictors"><i class="fa fa-check"></i><b>7.6.1</b> Types of predictors</a></li>
<li class="chapter" data-level="7.6.2" data-path="lmem.html"><a href="lmem.html#c6model3A"><i class="fa fa-check"></i><b>7.6.2</b> Model 3A: Random intercepts only</a></li>
<li class="chapter" data-level="7.6.3" data-path="lmem.html"><a href="lmem.html#c6model3B"><i class="fa fa-check"></i><b>7.6.3</b> Model 3B: Random intercepts and all possible random slopes</a></li>
<li class="chapter" data-level="7.6.4" data-path="lmem.html"><a href="lmem.html#assessing-variability"><i class="fa fa-check"></i><b>7.6.4</b> Assessing variability</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="lmem.html"><a href="lmem.html#more-on-random-slopes"><i class="fa fa-check"></i><b>7.7</b> More on random slopes</a><ul>
<li class="chapter" data-level="7.7.1" data-path="lmem.html"><a href="lmem.html#what-does-adding-a-random-slope-term-do"><i class="fa fa-check"></i><b>7.7.1</b> What does adding a random slope term do?</a></li>
<li class="chapter" data-level="7.7.2" data-path="lmem.html"><a href="lmem.html#adding-a-random-slope"><i class="fa fa-check"></i><b>7.7.2</b> Discussion: Adding a random slope</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="lmem.html"><a href="lmem.html#random-effect-correlations"><i class="fa fa-check"></i><b>7.8</b> Random effect correlations</a><ul>
<li class="chapter" data-level="7.8.1" data-path="lmem.html"><a href="lmem.html#model-1e-correlated-random-slope-intercept"><i class="fa fa-check"></i><b>7.8.1</b> Model 1E: <strong>Correlated</strong> random slope &amp; intercept</a></li>
<li class="chapter" data-level="7.8.2" data-path="lmem.html"><a href="lmem.html#c6discuss"><i class="fa fa-check"></i><b>7.8.2</b> Dicussion: Adding a correlation</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="lmem.html"><a href="lmem.html#model-criticism-for-linear-mixed-models"><i class="fa fa-check"></i><b>7.9</b> Model criticism for linear mixed models</a><ul>
<li class="chapter" data-level="7.9.1" data-path="lmem.html"><a href="lmem.html#model-3b-residual-plots"><i class="fa fa-check"></i><b>7.9.1</b> Model 3B: Residual plots</a></li>
<li class="chapter" data-level="7.9.2" data-path="lmem.html"><a href="lmem.html#model-3b-random-effect-distribution"><i class="fa fa-check"></i><b>7.9.2</b> Model 3B: Random effect distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="lmem.html"><a href="lmem.html#c6factorsissue"><i class="fa fa-check"></i><b>7.10</b> Random slopes for factors</a><ul>
<li class="chapter" data-level="7.10.1" data-path="lmem.html"><a href="lmem.html#model-with-random-effect-correlations"><i class="fa fa-check"></i><b>7.10.1</b> Model with random-effect correlations</a></li>
<li class="chapter" data-level="7.10.2" data-path="lmem.html"><a href="lmem.html#lmem-mwrec"><i class="fa fa-check"></i><b>7.10.2</b> Models without random-effect correlations</a></li>
</ul></li>
<li class="chapter" data-level="7.11" data-path="lmem.html"><a href="lmem.html#other-readings-1"><i class="fa fa-check"></i><b>7.11</b> Other readings</a></li>
<li class="chapter" data-level="7.12" data-path="lmem.html"><a href="lmem.html#c6extraexamples"><i class="fa fa-check"></i><b>7.12</b> Appendix: Extra examples</a><ul>
<li class="chapter" data-level="7.12.1" data-path="lmem.html"><a href="lmem.html#lmm-simulation-confint"><i class="fa fa-check"></i><b>7.12.1</b> Predicting confidence intervals by simulation</a></li>
<li class="chapter" data-level="7.12.2" data-path="lmem.html"><a href="lmem.html#random-intercept-and-slope-model-for-givenness-data"><i class="fa fa-check"></i><b>7.12.2</b> Random intercept and slope model for <code>givenness</code> data</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="lmem.html"><a href="lmem.html#c6extendedexercise"><i class="fa fa-check"></i><b>7.13</b> Appendix: Extended exercise</a></li>
<li class="chapter" data-level="7.14" data-path="lmem.html"><a href="lmem.html#c6solns"><i class="fa fa-check"></i><b>7.14</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Mixed-effects logistic regression</a><ul>
<li class="chapter" data-level="8.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#preliminaries"><i class="fa fa-check"></i><b>8.1</b> Preliminaries</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#motivation"><i class="fa fa-check"></i><b>8.1.1</b> Motivation</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#basics"><i class="fa fa-check"></i><b>8.2</b> Basics</a><ul>
<li class="chapter" data-level="8.2.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7m1"><i class="fa fa-check"></i><b>8.2.1</b> Model 1: <code>givenness</code> data, crossed random effects (intercepts + slopes)</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>8.3</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="8.3.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#fixed-effects"><i class="fa fa-check"></i><b>8.3.1</b> Fixed effects</a></li>
<li class="chapter" data-level="8.3.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#random-effects"><i class="fa fa-check"></i><b>8.3.2</b> Random effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#fixed-and-random-effects"><i class="fa fa-check"></i><b>8.4</b> Fixed and random effects</a></li>
<li class="chapter" data-level="8.5" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#melr-practice"><i class="fa fa-check"></i><b>8.5</b> MELR Practice</a><ul>
<li class="chapter" data-level="8.5.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7ex1"><i class="fa fa-check"></i><b>8.5.1</b> Exercise 1: tapping</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#model-criticism-for-mixed-effects-logistic-regression"><i class="fa fa-check"></i><b>8.6</b> Model criticism for mixed-effects logistic regression</a><ul>
<li class="chapter" data-level="8.6.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#random-effect-distributions"><i class="fa fa-check"></i><b>8.6.1</b> Random-effect distributions</a></li>
<li class="chapter" data-level="8.6.2" data-path="logistic-regression.html"><a href="logistic-regression.html#residual-plots"><i class="fa fa-check"></i><b>8.6.2</b> Residual plots</a></li>
<li class="chapter" data-level="8.6.3" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#influence"><i class="fa fa-check"></i><b>8.6.3</b> Influence</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#evaluation-measures"><i class="fa fa-check"></i><b>8.7</b> Evaluation measures</a><ul>
<li class="chapter" data-level="8.7.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#evaluation-measure-1-likelihood-ratio-test"><i class="fa fa-check"></i><b>8.7.1</b> Evaluation measure 1: Likelihood ratio test</a></li>
<li class="chapter" data-level="8.7.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#evaluation-measure-2-classification-accuracy"><i class="fa fa-check"></i><b>8.7.2</b> Evaluation measure 2: Classification accuracy</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#miscellaneous-mixed-effects-regression-topics"><i class="fa fa-check"></i><b>8.8</b> Miscellaneous mixed-effects regression topics</a><ul>
<li class="chapter" data-level="8.8.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7m2"><i class="fa fa-check"></i><b>8.8.1</b> Random-effect correlation issues</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#other-readings-2"><i class="fa fa-check"></i><b>8.9</b> Other readings</a></li>
<li class="chapter" data-level="8.10" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#appendices"><i class="fa fa-check"></i><b>8.10</b> Appendices</a><ul>
<li class="chapter" data-level="8.10.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#melr-random-slopes-for-factors"><i class="fa fa-check"></i><b>8.10.1</b> Appendix: Random slopes for factors</a></li>
<li class="chapter" data-level="8.10.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7appendix2"><i class="fa fa-check"></i><b>8.10.2</b> Appendix: Multi-level factors and uncorrelated random effects</a></li>
<li class="chapter" data-level="8.10.3" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#appendix-what-can-happen-if-a-random-slope-isnt-included"><i class="fa fa-check"></i><b>8.10.3</b> Appendix: What can happen if a random slope isnât included?</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7solns"><i class="fa fa-check"></i><b>8.11</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><i class="fa fa-check"></i><b>9</b> Practical regression topics 2: Ordered factors, nonlinear effects, model predictions, post-hoc tests</a><ul>
<li class="chapter" data-level="9.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#ordered-factors"><i class="fa fa-check"></i><b>9.2</b> Ordered factors</a><ul>
<li class="chapter" data-level="9.2.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#orthogonal-polynomial-contrasts"><i class="fa fa-check"></i><b>9.2.1</b> Orthogonal polynomial contrasts</a></li>
<li class="chapter" data-level="9.2.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#using-an-ordered-factor-as-a-predictor"><i class="fa fa-check"></i><b>9.2.2</b> Using an ordered factor as a predictor</a></li>
<li class="chapter" data-level="9.2.3" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#further-points"><i class="fa fa-check"></i><b>9.2.3</b> Further points</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#nonlinear-effects"><i class="fa fa-check"></i><b>9.3</b> Nonlinear effects</a><ul>
<li class="chapter" data-level="9.3.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#splines-definition-and-benefits"><i class="fa fa-check"></i><b>9.3.1</b> Splines: Definition and benefits</a></li>
<li class="chapter" data-level="9.3.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#restricted-cubic-splines"><i class="fa fa-check"></i><b>9.3.2</b> Restricted cubic splines</a></li>
<li class="chapter" data-level="9.3.3" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#choosing-spline-complexity"><i class="fa fa-check"></i><b>9.3.3</b> Choosing spline complexity</a></li>
<li class="chapter" data-level="9.3.4" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#rcs-components"><i class="fa fa-check"></i><b>9.3.4</b> RCS components</a></li>
<li class="chapter" data-level="9.3.5" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#using-rcs-in-a-mixed-model"><i class="fa fa-check"></i><b>9.3.5</b> Using RCS in a mixed model</a></li>
<li class="chapter" data-level="9.3.6" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#random-slopes-for-rcs-terms"><i class="fa fa-check"></i><b>9.3.6</b> Random slopes for RCS terms</a></li>
<li class="chapter" data-level="9.3.7" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#nonlinear-effects-summary"><i class="fa fa-check"></i><b>9.3.7</b> Nonlinear effects: Summary</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#predictions-from-mixed-models"><i class="fa fa-check"></i><b>9.4</b> Predictions from mixed models</a><ul>
<li class="chapter" data-level="9.4.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#making-model-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Making Model Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#simulation-based-predictions"><i class="fa fa-check"></i><b>9.4.2</b> Simulation-based predictions</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#post-hoc-mult-comp"><i class="fa fa-check"></i><b>9.5</b> Post-hoc tests and multiple comparisons</a></li>
<li class="chapter" data-level="9.6" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#c8indivpreds"><i class="fa fa-check"></i><b>9.6</b> Appendix: Model predictions for indiviudal participants</a><ul>
<li class="chapter" data-level="9.6.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#predictions-incorporating-offsets-for-individual-speakers"><i class="fa fa-check"></i><b>9.6.1</b> Predictions incorporating offsets for individual speakers</a></li>
<li class="chapter" data-level="9.6.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#predicted-williams-effect-for-each-speaker"><i class="fa fa-check"></i><b>9.6.2</b> Predicted Williams effect for each speaker</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#c8slopesForFactors"><i class="fa fa-check"></i><b>9.7</b> Appendix: Random slopes for factors</a></li>
<li class="chapter" data-level="9.8" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#c8solns"><i class="fa fa-check"></i><b>9.8</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="datasets-appendix.html"><a href="datasets-appendix.html"><i class="fa fa-check"></i><b>10</b> Appendix: Datasets and packages</a><ul>
<li class="chapter" data-level="10.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#engdata"><i class="fa fa-check"></i><b>10.1</b> <code>english</code> lexical decision and naming latencies</a></li>
<li class="chapter" data-level="10.2" data-path="datasets-appendix.html"><a href="datasets-appendix.html#dutch-regularity"><i class="fa fa-check"></i><b>10.2</b> Dutch <code id="dregdata">regularity</code></a></li>
<li class="chapter" data-level="10.3" data-path="datasets-appendix.html"><a href="datasets-appendix.html#european-french-phrase-medial-vowel-devoicing"><i class="fa fa-check"></i><b>10.3</b> European French phrase-medial vowel <code id="devdata">devoicing</code></a><ul>
<li class="chapter" data-level="10.3.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background"><i class="fa fa-check"></i><b>10.3.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="datasets-appendix.html"><a href="datasets-appendix.html#north-american-english-tapping"><i class="fa fa-check"></i><b>10.4</b> North American English <code id="tapdata">tapping</code></a><ul>
<li class="chapter" data-level="10.4.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-1"><i class="fa fa-check"></i><b>10.4.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="datasets-appendix.html"><a href="datasets-appendix.html#halfdata"><i class="fa fa-check"></i><b>10.5</b> <code>halfrhyme</code>: English half-rhymes</a></li>
<li class="chapter" data-level="10.6" data-path="datasets-appendix.html"><a href="datasets-appendix.html#givedata"><i class="fa fa-check"></i><b>10.6</b> <code>givenness</code> data: the Williams Effect</a><ul>
<li class="chapter" data-level="10.6.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-2"><i class="fa fa-check"></i><b>10.6.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="datasets-appendix.html"><a href="datasets-appendix.html#alternatives"><i class="fa fa-check"></i><b>10.7</b> <code id="altdata">alternatives</code></a><ul>
<li class="chapter" data-level="10.7.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-3"><i class="fa fa-check"></i><b>10.7.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="datasets-appendix.html"><a href="datasets-appendix.html#votdata"><i class="fa fa-check"></i><b>10.8</b> VOT</a><ul>
<li class="chapter" data-level="10.8.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-4"><i class="fa fa-check"></i><b>10.8.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="datasets-appendix.html"><a href="datasets-appendix.html#transitionsdata"><i class="fa fa-check"></i><b>10.9</b> Transitions</a></li>
<li class="chapter" data-level="10.10" data-path="datasets-appendix.html"><a href="datasets-appendix.html#packages"><i class="fa fa-check"></i><b>10.10</b> Packages</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Methods for Linguistic Data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Linear regression</h1>
<p><strong>Preliminary code</strong></p>
<p>This code is needed to make other code below work:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gridExtra) <span class="co"># for grid.arrange() to print plots side-by-side</span>
<span class="kw">require</span>(languageR)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(arm)

## loads alternativesMcGillLing620.csv from OSF project for Wagner (2016) data
alt &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">url</span>(<span class="st">&quot;https://osf.io/6qctp/download&quot;</span>))

## loads french_medial_vowel_devoicing.txt from OSF project for Torreira &amp; Ernestus (2010) data
df &lt;-<span class="st"> </span><span class="kw">read.delim</span>(<span class="kw">url</span>(<span class="st">&quot;https://osf.io/uncd8/download&quot;</span>))

## loads halfrhymeMcGillLing620.csv from OSF project for Harder (2013) data
halfrhyme &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">url</span>(<span class="st">&quot;https://osf.io/37uqt/download&quot;</span>))</code></pre></div>
<!-- TODO later: just wrap this into actual text -->
<!-- Old links: -->
<!-- alt <- read.delim('datasets/alternatives.txt') -->
<!-- df <- read.delim('datasets/french_medial_vowel_devoicing.txt') -->
<script src="js/hideOutput.js"></script>
<p><strong>Note</strong>: Most answers to questions/exercises not listed in text are in <a href="linear-regression.html#c2solns">Solutions</a>.</p>
<p>This chapter introduces linear regression. The broad topics we will cover are:</p>
<ol style="list-style-type: decimal">
<li><p>Regression: general introduction</p></li>
<li><p>Simple linear regression</p></li>
<li><p>Multiple linear regression</p></li>
<li><p>Linear regression assumptions, model criticism, and interpretation</p></li>
<li><p>Model comparison</p></li>
</ol>
<div id="regression-general-introduction" class="section level2">
<h2><span class="header-section-number">3.1</span> Regression: General introduction</h2>
<p>First, what is âregressionâ? <span class="citation">Chatterjee &amp; Hadi (<a href="#ref-chatterjee2012regression">2012</a>)</span> define it as âa conceptually simple method for investigating functional relationships among variablesâ:</p>
<ul>
<li><p>The variable to be explained is called the <em>response</em>, often written <span class="math inline">\(Y\)</span></p></li>
<li><p>The explanatory variables are called <em>predictors</em>, often written <span class="math inline">\(X_1, X_2, \ldots\)</span>.</p></li>
</ul>
<p>Here is an example of a study that uses regression analysis:</p>
<center>
<img src="images/cow_milk.png" />
</center>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li><p>What is the response variable in this study?</p></li>
<li><p>What could be a predictor variable?</p></li>
</ul>
</blockquote>
<div id="linear-models" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Linear models</h3>
The relationship between variables is captured by a regression <em>model</em>:
<span class="math display">\[\begin{equation*}
  Y = f(X_1, X_2, ..., X_p) + \epsilon
\end{equation*}\]</span>
<p>In this model, <span class="math inline">\(Y\)</span> is approximated by a function of the predictors, and the difference between the model and reality is called the <em>error</em> (<span class="math inline">\(\epsilon\)</span>).</p>
<p>Throughout this book we will be dealing exclusively with <em>linear models</em> (including âgeneralized linear modelsâ, like logistic regression), where the model takes the form:</p>
<span class="math display" id="eq:linreg1">\[\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon
    \tag{3.1}
\end{equation}\]</span>
<p>The <span class="math inline">\(\beta\)</span>âs are called <em>regression coefficients</em>.</p>
<p>This turns out to be an extremely general class of model, which can be applied to a wide range of phenomena. Some important kinds of models that donât appear at first glance to fit Equation <a href="linear-regression.html#eq:linreg1">(3.1)</a> can be linearized into this form.</p>
</div>
<div id="terminology" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Terminology</h3>
<p>We will consider two broad types of regression in this book:</p>
<ol style="list-style-type: decimal">
<li><p><em>linear regression</em>, where the response (<span class="math inline">\(Y\)</span>) is a continuous variable. An example would be modeling reaction time (<code>RTlexdec</code>) as a function of word frequency (<code>WrittenFrequency</code>) for <a href="datasets-appendix.html#engdata">the <code>english</code> dataset</a>.</p></li>
<li><p>Later we will consider <em>logistic regression</em>, where the response (<span class="math inline">\(Y\)</span>) is binary: 0 or 1. An example would be modeling whether tapping occurs or not (<code>tapping</code>) as a function of <code>vowelDuration</code> and <code>speakingRate</code> in <a href="datasets-appendix.html#tapdata">the <code>tapping</code> dataset</a>.</p></li>
</ol>
<p>Regression with just one predictor is called <em>simple</em>, while regression with multiple predictors is called <em>multiple</em>. The two examples just given would be âsimple linear regressionâ and âmultiple logistic regressionâ.</p>
<p>Predictors can be <em>continuous</em>, such as milk consumption, or <em>categorical</em>âalso called âfactorsââsuch as participant gender, or word type.</p>
<p>Certain special cases of linear models that are common go by names such as:</p>
<ul>
<li><p><em>Analysis of variance</em>: continuous <span class="math inline">\(Y\)</span>, categorical predictors. (Models variability between groups)</p></li>
<li><p><em>Analysis of covariance</em>: continuous <span class="math inline">\(Y\)</span>, mix of categorical and continuous predictors.</p></li>
</ul>
<p>We are not covering these cases in this book, but ANOVAs (and ANCOVAs) are widely used in language research, and you may have seen them before. ANOVAs can be usefully thought of as just a <strong>special case</strong> of regression, as discussed in <span class="citation">Vasishth &amp; Broe (<a href="#ref-vasishth2011foundations">2011</a>)</span>, <span class="citation">R. Levy (<a href="#ref-levy2012probabilistic">2012</a>)</span>, <span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2007data">2007</a>)</span>. Once you understand linear regression well, understanding ANOVA analyses is relatively straightforward.</p>
</div>
<div id="steps-and-assumptions-of-regression-analysis" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Steps and assumptions of regression analysis</h3>
<p>Regression analyses have five broad steps, as usefully discussed by <span class="citation">Chatterjee &amp; Hadi (<a href="#ref-chatterjee2012regression">2012</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li>Statement of the problem
<ul>
<li>Ex: Does milk consumption affect height gain?</li>
</ul></li>
<li>Selection of potentially relevant variables
<ul>
<li>Ex: Height gain, daily milk consumption, age, and sex.</li>
</ul></li>
<li>Data collection
<ul>
<li>Ex: data from an existing database.</li>
</ul></li>
<li>Model specification &amp; fitting
<ul>
<li>Ex: height gain = <span class="math inline">\(\beta_0 + \beta_1 \cdot\)</span> milk consumption <span class="math inline">\(+ \beta_2 \cdot\)</span> sex <span class="math inline">\(+\)</span> error</li>
</ul></li>
<li>Model validation and criticism</li>
</ol>
<p>It is important to remember that the <strong>validity of a regression analysis depends on the assumptions of the data and model</strong>.</p>
<p>For example, if you are modeling data where <span class="math inline">\(Y\)</span> has a maximum value, fitting a simple linear regression (= a line) to this data doesnât make conceptual sense, a priori. Hereâs an example using the <code>english</code> dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(languageR) ## makes the &#39;english&#39; dataset available

<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Familiarity, <span class="dt">y =</span> CorrectLexdec), <span class="dt">data =</span> english) +<span class="st"> </span><span class="kw">geom_point</span>(, <span class="dt">alpha=</span><span class="fl">0.1</span>) +<span class="st"> </span><span class="kw">geom_smooth</span>( <span class="dt">method=</span><span class="st">&#39;lm&#39;</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-2-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Because <code>CorrectLexdec</code> has a maximum value of 30, fitting a line doesnât make senseâthe predicted value when <code>Familiarity</code>=6 is above 30, but this is impossible given the definition of <code>CorrectLexdec</code>.</p>
<p>For now, we will not go into what the assumptions of linear regression are, and just assume that they are met. After introducing simple and multiple linear regressions, weâll come back to this issue in Section <a href="linear-regression.html#linear-regression-assumptions">3.4</a>:</p>
<ul>
<li><p>What are the assumptions (of linear regression)?</p></li>
<li><p>For each assumption, how do we determine whether itâs valid?</p></li>
<li><p>How much of a problem is it, and what can be done, if the assumption is not met?</p></li>
</ul>
<p>Note that R almost never checks whether your data and model meet regression analysis assumptions, unlike other software (e.g.Â SPSS, sometimes).</p>
</div>
</div>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">3.2</span> Simple linear regression</h2>
<p>The simplest application of simple linear regression (SLR) is to model an association between two continuous variables.</p>
<div id="example-english-data-young-participants-only" class="section level4 unnumbered">
<h4>Example: <code>english</code> data, young participants only</h4>
<ul>
<li><p><span class="math inline">\(X\)</span>: <code>WrittenFrequency</code> (= predictor)</p></li>
<li><p><span class="math inline">\(Y\)</span>: <code>RTlexdec</code> (= response)</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">young &lt;-<span class="st"> </span><span class="kw">filter</span>(english, AgeSubject==<span class="st">&#39;young&#39;</span>)

<span class="kw">ggplot</span>(young, <span class="kw">aes</span>(WrittenFrequency, RTlexdec)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.5</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-3-1.png" width="480" style="display: block; margin: auto;" /></p>
<!-- TODO later: when "previous chapters" put into book, add this back in: -->
<!-- In previous chapters, we have described this kind of association using a _correlation -->
<p>One way to describe this kind of association which you may be familiar with is a <em>correlation</em> coefficient which gives us two types of information about the relationship:</p>
<ol style="list-style-type: decimal">
<li><em>direction</em>: <span class="math inline">\(r = -0.434\)</span>
<ul>
<li><span class="math inline">\(\implies\)</span> negative relationship</li>
</ul></li>
<li><em>strength</em>: <span class="math inline">\(r^2 = 0.189\)</span>
<ul>
<li><span class="math inline">\(\implies\)</span> weak relationship (<span class="math inline">\(0 \le r^2 \le 1\)</span>)</li>
</ul></li>
</ol>
<p>A simple linear regression gives a <em>line of best fit</em>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(young, <span class="kw">aes</span>(WrittenFrequency, RTlexdec)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.5</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span>F)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-4-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>which gives some information not captured by a correlation coefficient:</p>
<ul>
<li><p>Prediction of <span class="math inline">\(Y\)</span> for a given <span class="math inline">\(X\)</span></p></li>
<li><p>Numerical description of relationship</p></li>
</ul>
<p>Regression gives both types of information, and more.</p>
</div>
<div id="slr-continuous-predictor" class="section level3">
<h3><span class="header-section-number">3.2.1</span> SLR: Continuous predictor</h3>
<p>The formula for simple linear regression, written for the <span class="math inline">\(i^{\text{th}}\)</span> observation, is:</p>
<span class="math display" id="eq:linreg2">\[\begin{equation}
  y_i = \underbrace{\beta_0}_{\text{intercept}} + \underbrace{\beta_1}_{\text{slope}} x_i + \epsilon_i
  \tag{3.2}
\end{equation}\]</span>
<p>In this expression:</p>
<ul>
<li><p><span class="math inline">\(\beta_0, \beta_1\)</span> are <em>coefficients</em></p></li>
<li>For the <span class="math inline">\(i^{\text{th}}\)</span> observation
<ul>
<li><p><span class="math inline">\(x_i\)</span> is the value of the <em>predictor</em></p></li>
<li><p><span class="math inline">\(y_i\)</span> is the value of the <em>response</em></p></li>
<li><p><span class="math inline">\(\epsilon_i\)</span> is the value of the <em>error</em> (or <em>residual</em>)</p></li>
</ul></li>
</ul>
<p>This is our first linear model of a random variable (<span class="math inline">\(Y\)</span>) as a function of a predictor variable (<span class="math inline">\(X\)</span>). The actual model, written not for individual observations, is written:</p>
<p><span class="math display">\[
  Y = \beta_0 + \beta_1 X + \epsilon
\]</span></p>
<p>That is, we use notation like <span class="math inline">\(X\)</span> for a variable, and notation like <span class="math inline">\(x_5\)</span> for actual values that it takes on.</p>
<p>For example, for <a href="datasets-appendix.html#engdata">the <code>english</code> dataset</a>, with <span class="math inline">\(Y\)</span> = <code>RTlexdec</code> and <span class="math inline">\(X\)</span> = <code>WrittenFrequency</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(dplyr::<span class="kw">select</span>(english, WrittenFrequency, RTlexdec))</code></pre></div>
<pre><code>##   WrittenFrequency RTlexdec
## 1         3.912023 6.543754
## 2         4.521789 6.397596
## 3         6.505784 6.304942
## 4         5.017280 6.424221
## 5         4.890349 6.450597
## 6         4.770685 6.531970</code></pre>
<ul>
<li><p><span class="math inline">\(x_2\)</span>=4.5217886 (predictor value for second observation)</p></li>
<li><p><span class="math inline">\(y_1\)</span>=6.5437536 (response value for first observation)</p></li>
</ul>
<blockquote>
<p><strong>Question</strong></p>
<ul>
<li>What is <span class="math inline">\(y_5\)</span>?</li>
</ul>
</blockquote>
</div>
<div id="slr-parameter-estimation" class="section level3">
<h3><span class="header-section-number">3.2.2</span> SLR: Parameter estimation</h3>
<p>To get a line of best fit, we want: <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the <em>population values</em>. Recall that we canât actually observe these (Sec.<a href="inferential-statistics-introduction.html#sample-population">1.1</a>), so we obtain <em>sample estimates</em>, written <span class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>.</p>
<p>Once sample estimates are specified, Equation <a href="linear-regression.html#eq:linreg2">(3.2)</a> gives <em>fitted values</em> for each observation, written <span class="math inline">\(\hat{y}_i\)</span>: <span class="math display">\[
  \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
\]</span></p>
<p>Note that there are no residuals in this equationâ<span class="math inline">\(\epsilon_i\)</span> are again population values, which we canât observe. Our estimates of the residuals, given an estimated line of best it, are written <span class="math inline">\(e_i\)</span> (<em>error</em>): <span class="math display">\[
  e_i = y_i - \hat{y}_i
\]</span></p>
<p>This diagram shows the relationship between some of these quantities, for a single observation:</p>
<center>
<img src="images/slr_continuous_model.png" width="600" />
</center>
<p>Our goal is to find coefficient values that minimize the difference between observed and expected valuesâthe magnitudes of error (<span class="math inline">\(|e_i|\)</span>), which are minimized by minimizing the squared errors (<span class="math inline">\(e_i^2\)</span>).</p>
<p>We choose <span class="math inline">\(\hat{\beta}_0\)</span>, <span class="math inline">\(\hat{\beta}_1\)</span> that minimize the sum of the <span class="math inline">\(e_i^2\)</span>; these are called the <em>least-squares estimates</em>.</p>
<p>One useful property of the resulting regression line is that it always passes through the point (mean(<span class="math inline">\(X\)</span>), mean(<span class="math inline">\(Y\)</span>)). A consequence is that SLR is easily thrown off by observations which are outliers in <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> (why?).</p>
<div id="example-5" class="section level4 unnumbered">
<h4>Example</h4>
<p>Here is an example of fitting an SLR model in R, of reaction time vs.Â frequency for young speakers in the <code>english</code> dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">young &lt;-<span class="st"> </span><span class="kw">filter</span>(english, AgeSubject ==<span class="st"> &quot;young&quot;</span>)
m &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>WrittenFrequency, young)</code></pre></div>
<p>The model output is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ WrittenFrequency, data = young)
## 
## Coefficients:
##      (Intercept)  WrittenFrequency  
##          6.62556          -0.03711</code></pre>
<p>The interpretation of the two coefficients is:</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span>: the predicted <span class="math inline">\(Y\)</span> value when <span class="math inline">\(X = 0\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m$coefficients[<span class="dv">1</span>]</code></pre></div>
<pre><code>## (Intercept) 
##    6.625556</code></pre></li>
<li><p><span class="math inline">\(\beta_1\)</span>: predicted change in <span class="math inline">\(Y\)</span> for every unit change in <span class="math inline">\(X\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m$coefficients[<span class="dv">2</span>]</code></pre></div>
<pre><code>## WrittenFrequency 
##      -0.03710692</code></pre></li>
</ul>
The regression line is:
<span class="math display">\[\begin{equation}
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
\end{equation}\]</span>
In terms of the variables used in this example:
<span class="math display">\[\begin{equation}
  \text{Predicted RTlexdec}_i = 6.625 - 0.037 \cdot \text{WrittenFrequency}_i
\end{equation}\]</span>
<blockquote>
<p><strong>Questions</strong>:</p>
<p>What is the predicted <code>RTlexdec</code> when:</p>
<ul>
<li><p><code>WrittenFrequency</code> = 5?</p></li>
<li><p><code>WrittenFrequency</code> = 10?</p></li>
</ul>
</blockquote>
<!-- 6.44, 6.25 -->
</div>
</div>
<div id="hypothesis-testing-1" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Hypothesis testing</h3>
The least-squared estimators are normally-distributed. These are sample estimates, so we can also approximate the standard errors of the estimators:
<span class="math display">\[\begin{equation*}
  SE(\hat{\beta}_0) \quad SE(\hat{\beta}_1)
\end{equation*}\]</span>
<p>and apply <span class="math inline">\(t\)</span>-tests to test for significance and obtain confidence intervals (CI) for the coefficients.</p>
<p>In particular, we are testing the null hypotheses of no relationship:</p>
<ul>
<li><span class="math inline">\(H_0~:~\beta_1 = 0\)</span></li>
</ul>
<p>(and similarly for <span class="math inline">\(\beta_0\)</span>).</p>
We then apply a <span class="math inline">\(t\)</span>-test, using test statistic:
<span class="math display">\[\begin{align}
  t_1 &amp;= \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} \\
  df &amp;= n - 2 \nonumber
\end{align}\]</span>
<p>(where <span class="math inline">\(n\)</span> = number of observations).</p>
<p>The resulting <span class="math inline">\(p\)</span>-value tells us how surprised are we to get a slope this far from zero (<span class="math inline">\(\beta_1\)</span>) under <span class="math inline">\(H_0\)</span>. (With this high a standard error, given this much data.)</p>
<p>To see the results of these hypothesis tests in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">lm</span>(RTlexdec~WrittenFrequency, young))</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ WrittenFrequency, data = young)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.34664 -0.05523 -0.00546  0.05167  0.34877 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       6.6255559  0.0049432 1340.34   &lt;2e-16 ***
## WrittenFrequency -0.0371069  0.0009242  -40.15   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.08142 on 2282 degrees of freedom
## Multiple R-squared:  0.414,  Adjusted R-squared:  0.4137 
## F-statistic:  1612 on 1 and 2282 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The <span class="math inline">\(t\)</span>-values and associated <span class="math inline">\(p\)</span>-values are in the <code>Coefficients:</code> table, where the first row is for <span class="math inline">\(\hat{\beta}_0\)</span> and the second row is for <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>Why does the column for the <span class="math inline">\(p\)</span>-value say <code>Pr(&gt;|t|)</code>?</li>
</ul>
</blockquote>
<p>Having the SEs of the coefficients also lets us compute 95% confidence intervals for the least-squared estimators. Going from the null hypothesis (<span class="math inline">\(H_0\)</span>) above: if the 95% CI of the slope (<span class="math inline">\(\beta_1\)</span>) does not include 0, we can reject <span class="math inline">\(H_0\)</span> with <span class="math inline">\(\alpha = 0.05\)</span>.</p>
<p>In R, you can get confidence intervals for a fitted model as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(<span class="kw">lm</span>(RTlexdec~WrittenFrequency, young))</code></pre></div>
<pre><code>##                        2.5 %      97.5 %
## (Intercept)       6.61586227  6.63524948
## WrittenFrequency -0.03891921 -0.03529463</code></pre>
<p>Neither CI includes zero, consistent with the very low <span class="math inline">\(p\)</span>-values in the model table above.</p>
<div id="example-small-subset" class="section level4 unnumbered">
<h4>Example: Small subset</h4>
<p>For the full dataset of <code>english</code> young speakers, itâs a little silly to do hypothesis testing given how much data there is and the clarity of the patternâthe line of best fit has a tiny confidence interval. Just for exposition, letâs look at the line of best fit for a subset of just <span class="math inline">\(n=100\)</span> points:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2903</span>) <span class="co"># This makes the following &quot;random&quot; sampling step always give the same resuult</span>
young_sample &lt;-<span class="st"> </span>young %&gt;%<span class="st"> </span><span class="kw">sample_n</span>(<span class="dv">100</span>)

<span class="kw">ggplot</span>(young_sample, <span class="kw">aes</span>(WrittenFrequency, RTlexdec)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-12-1.png" width="480" style="display: block; margin: auto;" /></p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>What does <code>geom_smooth(method=&quot;lm&quot;)</code> do?</li>
</ul>
</blockquote>
<p>In this plot, the shading around the line is the 95% confidence interval. âCan we reject <span class="math inline">\(H_0\)</span>?â is equivalent to asking, âCan a line with 0 slope cross the shaded area through the range of <span class="math inline">\(x\)</span> (and going through (mean(<span class="math inline">\(X\)</span>), mean(<span class="math inline">\(Y\)</span>))?</p>
</div>
</div>
<div id="quality-of-fit" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Quality of fit</h3>
<p>Here is the model we have been discussing, plotted on top of the empirical data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">young &lt;-<span class="st"> </span><span class="kw">filter</span>(english, AgeSubject ==<span class="st"> &quot;young&quot;</span>)

<span class="kw">ggplot</span>(young, <span class="kw">aes</span>(WrittenFrequency, RTlexdec)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.25</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-13-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>We often want a metric quantifying how well a model fits the dataâthe <em>goodness of fit</em>.</p>
<p>For simple linear regression, we can derive such a metric by first defining three quantities:</p>
<ul>
<li><p><em>SST</em>: Total sum of squares</p></li>
<li><p><span style="color:blue"><em>SSR</em></span>: Sum of squares due to regression</p></li>
<li><p><span style="color:orange"><em>SSE</em></span>: Sum of squares due to error</p></li>
</ul>
<center>
<img src="images/slr_quality_of_fit.png" />
</center>
<p>(Source: slides from <em>Business Statistics: A First Course (Third edition)</em>)</p>
<!-- TODO FUTURE: make own figure or get public domain figure -->
<p>The fundamental equality is: <em>SST</em> = <span style="color:blue"><em>SSR</em></span> + <span style="color:orange"><em>SSE</em></span></p>
Intuitively we want a measure of how much of SST is accounted for by SSR. This is <span class="math inline">\(R^2\)</span>: the proportion of total variability in <span class="math inline">\(Y\)</span> accounted for by <span class="math inline">\(X\)</span>:
<span class="math display">\[\begin{equation}
  R^2 = \frac{SS_{\text{fit}}}{SS_{\text{total}}}
\end{equation}\]</span>
<p><span class="math inline">\(R^2\)</span> always lies between 0 and 1, which can conceptually be thought of as:</p>
<ul>
<li>0: none of the variance in <span class="math inline">\(Y\)</span> is accounted for by <span class="math inline">\(X\)</span></li>
<li>1: all of the variance `` ``</li>
</ul>
<p>It turns out <span class="math inline">\(R^2\)</span> is also the square of (Pearsonâs) correlation, <span class="math inline">\(r\)</span>.</p>
<div id="example-6" class="section level4 unnumbered">
<h4>Example</h4>
<p>Here is the SLR model fitted to a subset of 100 data points, repeated for convenience:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2903</span>)

d &lt;-<span class="st"> </span>english %&gt;%<span class="st"> </span><span class="kw">filter</span>(AgeSubject==<span class="st">&quot;young&quot;</span>) %&gt;%<span class="st"> </span><span class="kw">sample_n</span>(<span class="dv">100</span>)
<span class="kw">summary</span>(<span class="kw">lm</span>(RTlexdec~WrittenFrequency, d))</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ WrittenFrequency, data = d)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.127857 -0.045072 -0.005826  0.042917  0.235034 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       6.61733    0.02190 302.236  &lt; 2e-16 ***
## WrittenFrequency -0.03501    0.00419  -8.354 4.43e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.07121 on 98 degrees of freedom
## Multiple R-squared:  0.4159, Adjusted R-squared:   0.41 
## F-statistic: 69.79 on 1 and 98 DF,  p-value: 4.43e-13</code></pre>
<p>In the model table, note the values of the sample statistic under <code>t value</code> and its significance in the <code>Pr(&gt;|t|)</code> column in the <code>WrittenFrequency</code> row, as well as of the correlation statistic <code>Multiple R-squared</code>.</p>
<p>This is a hypothesis test for Pearsonâs <span class="math inline">\(r\)</span> for the same data, checking whether it is significantly different from 0:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor.test</span>(d$WrittenFrequency, d$RTlexdec)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  d$WrittenFrequency and d$RTlexdec
## t = -8.354, df = 98, p-value = 4.43e-13
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.7467533 -0.5135699
## sample estimates:
##       cor 
## -0.644931</code></pre>
<p>Note that <span class="math inline">\(t\)</span>, <span class="math inline">\(p\)</span>, and <span class="math inline">\(r\)</span> (the square root of <code>Multiple R-squared</code>) are exactly the same. Thus, fitting a simple linear regression and conducting a correlation test give us two ways of finding the same information.</p>
</div>
</div>
<div id="categorical-predictor" class="section level3">
<h3><span class="header-section-number">3.2.5</span> Categorical predictor</h3>
<p>Simple linear regression easily extends to the case of a binary <span class="math inline">\(X\)</span> (a <em>factor</em>).</p>
<div id="example-7" class="section level4 unnumbered">
<h4>Example</h4>
<ul>
<li><p><code>english</code> data</p></li>
<li><p>Predictor: <code>AgeSubject</code></p></li>
<li><p>Response: <code>RTlexdec</code></p></li>
</ul>
<p>Everything is the same as for the case where <span class="math inline">\(X\)</span> is continuous, except now we have:</p>
<ul>
<li><p><span class="math inline">\(x_i = 0\)</span>: if <code>AgeSubject == &quot;old&quot;</code></p></li>
<li><p><span class="math inline">\(x_i = 1\)</span>: if <code>AgeSubject == &quot;young&quot;</code></p></li>
</ul>
<p>The regression equation is exactly the same as Equation <a href="linear-regression.html#eq:linreg2">(3.2)</a>:</p>
<p><span class="math display">\[
  y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]</span></p>
<p>Only the interpretation of the coefficients differs:</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span>: <strong>mean</strong> <code>RTlexdec</code> when <code>AgeSubject == &quot;old&quot;</code> (since <span class="math inline">\(x_i = 0 \iff\)</span> <code>AgeSubject == &quot;old&quot;</code>)</p></li>
<li><p><span class="math inline">\(\beta_1\)</span>: <strong>difference in mean</strong> <code>RTlexdec</code> between <code>young</code> and <code>old</code></p></li>
</ul>
<p>Hypothesis tests, <span class="math inline">\(p\)</span>-values, CIs, and goodness of fit work exactly the same as for a continuous predictor.</p>
</div>
<div id="example-8" class="section level4 unnumbered">
<h4>Example</h4>
<p>Suppose we want to test whether the difference in group means is statistically significantly different from 0:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">english %&gt;%<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(AgeSubject, RTlexdec)) +
<span class="st">  </span><span class="kw">geom_boxplot</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.y=</span><span class="st">&quot;mean&quot;</span>, <span class="dt">geom=</span><span class="st">&quot;point&quot;</span>, <span class="dt">color=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-16-1.png" width="288" style="display: block; margin: auto;" /></p>
<blockquote>
<p><strong>Question</strong>:</p>
<ul>
<li>What goes in the blanks?</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(_____ ~<span class="st"> </span>_____, english)
<span class="kw">summary</span>(m1)</code></pre></div>
<!-- ```{r} -->
<!-- m1 <- lm(RTlexdec ~ AgeSubject, english) -->
<!-- summary(m1) -->
<!-- ``` -->
<p>Note the <span class="math inline">\(t\)</span> and <span class="math inline">\(p\)</span>-values for <code>AgeSubjectyoung</code>, weâll need them in a second.</p>
</div>
</div>
<div id="slr-with-a-binary-categorical-predictor-vs.two-sample-t-test" class="section level3">
<h3><span class="header-section-number">3.2.6</span> SLR with a binary categorical predictor vs.Â two-sample <span class="math inline">\(t\)</span>-test</h3>
<p>Conceptually, we just did the same thing as a two-sample <span class="math inline">\(t\)</span>-testâtested the difference between two groups in the value of a continuous variable. Letâs see what the equivalent <span class="math inline">\(t\)</span>-test gives us:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(RTlexdec ~<span class="st"> </span>AgeSubject, english, <span class="dt">var.equal=</span>T)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  RTlexdec by AgeSubject
## t = 67.468, df = 4566, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.2152787 0.2281642
## sample estimates:
##   mean in group old mean in group young 
##            6.660958            6.439237</code></pre>
<p>(The <code>var.equal</code> option forces the <span class="math inline">\(t\)</span>-test to assume equal variances in both groups, which is one assumption of linear regression.)</p>
<p>You should find that both tests give identical <span class="math inline">\(t\)</span> and <span class="math inline">\(p\)</span>-values. So, a <span class="math inline">\(t\)</span>-test can be thought of as a special case of simple linear regression.</p>
<div id="bonus-linear-vs.smooth-regression-lines" class="section level4">
<h4><span class="header-section-number">3.2.6.1</span> Bonus: Linear vs.Â smooth regression lines</h4>
<p>We have forced an SLR fit in the plots above using the <code>method='lm'</code> flag, but by default <code>geom_smooth</code> uses a <em>nonparametric smoother</em> (such as LOESS, the <code>geom_smooth</code> default for small samples):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">young &lt;-<span class="st"> </span><span class="kw">filter</span>(english, AgeSubject==<span class="st">&#39;young&#39;</span>)

<span class="kw">set.seed</span>(<span class="dv">2903</span>)
young_sample &lt;-<span class="st"> </span>young %&gt;%<span class="st"> </span><span class="kw">sample_n</span>(<span class="dv">100</span>)

day7_plt1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(young_sample, <span class="kw">aes</span>(WrittenFrequency, RTlexdec)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.5</span>) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Linear regression line and 95% CI&quot;</span>)

day7_plt2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(young_sample, <span class="kw">aes</span>(WrittenFrequency, RTlexdec)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.5</span>) +
<span class="st">  </span><span class="kw">geom_smooth</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Smooth regression line and 95% CI&quot;</span>)

<span class="kw">grid.arrange</span>(day7_plt1, day7_plt2, <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Note the differences in the two plots:</p>
<ul>
<li><p>Linear/nonlinear</p></li>
<li><p>CI widths related to distance from mean, versus <strong>amount of data nearby</strong></p></li>
</ul>
<p>(Hence â<strong>L</strong>ocalâ in LOESS.)</p>
</div>
</div>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2><span class="header-section-number">3.3</span> Multiple linear regression</h2>
<p>In <em>multiple linear regression</em>, we use a linear model to predict a continuous response with <span class="math inline">\(p\)</span> predictors (<span class="math inline">\(p&gt;1\)</span>):</p>
<p><span class="math display">\[
  Y = \beta_0 + \beta_1 X_i + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon
\]</span></p>
<p>Each predictor <span class="math inline">\(X_i\)</span> can be continuous or categorical.</p>
<div id="ex1" class="section level4 unnumbered">
<h4>Example: RT ~ frequency + age</h4>
<p>For <a href="datasets-appendix.html#engdata">the <code>english</code> dataset</a>, letâs model reaction time as a function of word frequency and participant age. Recall that in addition to the word frequency effect, older speakers react more slowly than younger speakers:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">english %&gt;%<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(AgeSubject, RTlexdec)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_boxplot</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.y=</span><span class="st">&quot;mean&quot;</span>, <span class="dt">geom=</span><span class="st">&quot;point&quot;</span>, <span class="dt">color=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-20-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>The response and predictors are:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span>: <code>RTlexdec</code></p></li>
<li><p><span class="math inline">\(X_1\)</span>: <code>WrittenFrequency</code> (continuous)</p></li>
<li><p><span class="math inline">\(X_2\)</span>: <code>AgeSubject</code> (categorical) (0: <code>old</code>, 1: <code>young</code>)</p></li>
</ul>
<p>Because <span class="math inline">\(p=2\)</span>, the regression equation for observation <span class="math inline">\(i\)</span> is<br />
<span class="math display">\[
  y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i
\]</span> where <span class="math inline">\(x_{ij}\)</span> means the value of the <span class="math inline">\(j^{\text{th}}\)</span> predictor for the <span class="math inline">\(i^{\text{th}}\)</span> observation.</p>
<p>To fit this model in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec~WrittenFrequency+AgeSubject, english)</code></pre></div>
<p>Summary of the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ WrittenFrequency + AgeSubject, data = english)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.34622 -0.06029 -0.00722  0.05178  0.44999 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       6.8467921  0.0039792 1720.64   &lt;2e-16 ***
## WrittenFrequency -0.0370103  0.0007033  -52.62   &lt;2e-16 ***
## AgeSubjectyoung  -0.2217215  0.0025930  -85.51   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.08763 on 4565 degrees of freedom
## Multiple R-squared:  0.6883, Adjusted R-squared:  0.6882 
## F-statistic:  5040 on 2 and 4565 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>This model tells us that the least-squares solution for the regression line is: <span class="math display">\[
  \texttt{RTlexdec} = \underbrace{6.846}_{\beta_0} + \underbrace{(- 0.037)}_{\beta_1} \cdot \texttt{WrittenFrequency} + \underbrace{(- 0.221)}_{\beta_2} \cdot \texttt{AgeSubject} + \text{error}
\]</span></p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>What RT does the model predict for an observation with <code>WrittenFrequency</code>=5 and <code>AgeSubject</code>=âoldâ?</li>
</ul>
</blockquote>
<div class="fold s o">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">6.846</span> +<span class="st"> </span>(-<span class="fl">0.037</span> *<span class="st"> </span><span class="dv">5</span>) +<span class="st"> </span>(-<span class="fl">0.221</span> *<span class="st"> </span><span class="dv">0</span>)</code></pre></div>
<pre><code>## [1] 6.661</code></pre>
</div>
<p>Note that in this MLR model, the interpretation of each coefficient is:</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span>: predicted value when all predictors = 0</p></li>
<li><p><span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>: change in a predictor <strong>when others are held constant</strong></p></li>
</ul>
<p>For example, the difference between old and young speakers in RT is 0.221, when word frequency is held constant.</p>
</div>
<div id="goodness-of-fit-metrics" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Goodness of fit metrics</h3>
<p>With <span class="math inline">\(R^2\)</span> defined as for simple linear regression, in terms of sums of squares, the exact same measure works to quantify goodness of fit of a multiple linear regression: <span class="math display">\[
  R^2 = \frac{SS_{\text{fit}}}{SS_{\text{total}}}
\]</span> sometimes called <em>multiple <span class="math inline">\(R^2\)</span></em>.</p>
<p>An alternative to <span class="math inline">\(R^2\)</span> when thereâs more than one predictor (MLR) is <em>adjusted <span class="math inline">\(R^2\)</span></em>, defined as: <span class="math display">\[
  R^2 = 1 - \frac{SS_{\text{fit}}/(n - p - 1)}{ SS_{\text{total}}/(n - 1) }
\]</span> where <span class="math inline">\(p\)</span> is the number of predictors and <span class="math inline">\(n\)</span> is the number of observations.</p>
<p>In this expression, the sum-of-squares term can be thought of as a ratio comparing the amount of variance explained by two models: the âfullâ model (the one with <span class="math inline">\(p\)</span> predictors) and the âbaselineâ model (the one with just the intercept). Each modelâs sum of squares is scaled by its degrees of freedom; intuitively, this gives a measure of how much variance is explained <strong>given the number of predictors</strong>. (We expect that if you throw more predictors in a model, more variance can be explained, just by chance.)</p>
<p>The adjusted <span class="math inline">\(R^2\)</span> measure only increases if the <span class="math inline">\(p\)</span> additional predictors improve the model more than would be expected by chance.</p>
<ul>
<li><p><strong>Pro</strong>: Adjusted <span class="math inline">\(R^2\)</span> is more appropriate as a metric for comparing different possible modelâunlike âmultiple <span class="math inline">\(R^2\)</span>â, adjusted <span class="math inline">\(R^2\)</span> doesnât automatically increase whenever new predictors are added.</p></li>
<li><p><strong>Con</strong>: Multiple <span class="math inline">\(R^2\)</span> is no longer interpretable as âfraction of the variation accounted for by the modelâ.</p></li>
</ul>
<p>R reports both adjusted and non-adjusted versions, as seen in the model summary above.</p>
</div>
<div id="interactions-and-factors" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Interactions and factors</h3>
<p>The models we have considered so far assume that each predictor affects the response <strong>independently</strong>. For example, in <a href="linear-regression.html#c2ex1">the example above</a> (<code>RT ~ frequency + age</code>), our model assumes that the slope of the frequency effect on RT is the same for old speakers as for young speakers. This looks like it might be approximately true:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(english, <span class="kw">aes</span>(WrittenFrequency, RTlexdec)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.5</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span>F) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~AgeSubject)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-24-1.png" width="480" style="display: block; margin: auto;" /> in that there seems to be a similarly negative slope for both groups. That is, a model of this form seems approximately correct: <span class="math display">\[
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
\]</span> (<span class="math inline">\(\epsilon\)</span> = error).</p>
<p>Here is a (fake) example where the independence assumption is definitely not true:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span>: Job performance (continuous)</p></li>
<li><p><span class="math inline">\(X1\)</span>: Training (categorical)</p></li>
<li><p><span class="math inline">\(X2\)</span>: Autonomy (categorical)</p></li>
</ul>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-25-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The effect of training on job performance is larger for high-autonomy participants. In this case, we say there is an <em>interaction</em> between training and autonomy: the value of one predictor modulates the effect of the other. This interaction is modeled by adding an extra term to the regression equation: <span class="math display">\[
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon
\]</span> which is the <em>product</em> of the two terms. Note that <span class="math display">\[
\beta_2 X_2 + \beta_3 X_1 X_2 = (\beta_2 + \beta_3 X_1) X_2
\]</span> so the interaction coefficient <span class="math inline">\(\beta_3\)</span> modulates the slope of <span class="math inline">\(X_2\)</span>: depending on the value of <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> has a different effect on <span class="math inline">\(Y\)</span>.</p>
<div id="example-9" class="section level4 unnumbered">
<h4>Example</h4>
<p>Returning to <a href="linear-regression.html#c2ex1">the example above</a>, suppose weâd like to know how much the slope of <code>WrittenFrequency</code> does actually differ between old and young speakers, and whether the difference is statistically significant (<span class="math inline">\(\alpha = 0.05\)</span>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">english %&gt;%<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(WrittenFrequency, RTnaming)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.5</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">group=</span>AgeSubject, <span class="dt">color=</span>AgeSubject), <span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span>F)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-26-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><code>X1:X2</code> means âinteraction between <code>X1</code> and <code>X2</code>, and the notation used in R for interactions is <code>X1*X2</code>, which expands automatically to <code>X1 + X2 + X1:X2</code>. (The non-interaction terms are sometimes called <em>main effects</em>.) Note that in R these are equivalent:</p>
<pre><code>lm(RTnaming ~ WrittenFrequency * AgeSubject, english)
lm(RTnaming ~ WrittenFrequency + AgeSubject + WrittenFrequency:AgeSubject, english)</code></pre>
<p>To fit a model including an interaction between frequency and age:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m3 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTnaming ~<span class="st"> </span>WrittenFrequency *<span class="st"> </span>AgeSubject, english)</code></pre></div>
<p>In the summary of this model, of interest is the <code>WrittenFrequency:AgeSubjectyoung</code> row, which is the interaction effect:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m3)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTnaming ~ WrittenFrequency * AgeSubject, data = english)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.160510 -0.033425 -0.002963  0.030855  0.181032 
## 
## Coefficients:
##                                    Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                       6.5517608  0.0029118 2250.09  &lt; 2e-16
## WrittenFrequency                 -0.0116031  0.0005444  -21.31  &lt; 2e-16
## AgeSubjectyoung                  -0.3651823  0.0041179  -88.68  &lt; 2e-16
## WrittenFrequency:AgeSubjectyoung  0.0046191  0.0007699    6.00 2.13e-09
##                                     
## (Intercept)                      ***
## WrittenFrequency                 ***
## AgeSubjectyoung                  ***
## WrittenFrequency:AgeSubjectyoung ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.04796 on 4564 degrees of freedom
## Multiple R-squared:  0.9278, Adjusted R-squared:  0.9278 
## F-statistic: 1.956e+04 on 3 and 4564 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We see that there is indeed a significant interaction between <code>WrittenFrequency</code> and <code>AgeSubject</code>.</p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li><p>What does it mean that this coefficient is positive?</p></li>
<li><p>For this regression model including an interaction, what is the model for an observation with <code>WrittenFrequency=3</code> and <code>AgeSubject=='old'</code>? <a href="linear-regression.html#c2sol1">Solution</a>.</p></li>
<li><p>What is the model for an observation with <code>WrittenFrequency=3</code> and <code>AgeSubject=='young'</code>? <a href="linear-regression.html#c2sol2">Solution</a>.</p></li>
</ul>
</blockquote>
</div>
</div>
<div id="plotting-interactions" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Plotting interactions</h3>
<p>In general, making plots is indispensable for interpreting interactions. It is possible, with practice, to interpret interactions from the regression table, but examining a good plot is usually also necessary and much faster.</p>
<p>In later chapters we will cover how to actually visualize model predictionsâexactly what the model predicts for different combinations of predictor values. You can usually get a reasonable approximation of this by making the relevant empirical plot, such as:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">english %&gt;%<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(WrittenFrequency, RTnaming)) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&#39;lm&#39;</span>, <span class="kw">aes</span>(<span class="dt">color=</span>AgeSubject)) +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&#39;Log written frequency&#39;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&#39;Naming RT in log(s)&#39;</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-29-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>It is often better to use empirical plots to visualize interactionsâeven though, strictly speaking, you are not plotting the modelâs predictions.</p>
<ul>
<li><p><strong>Pros</strong>: Empirical plots are more intuitive, and if you have a robust effect it should probably show up in an empirical plot.</p></li>
<li><p><strong>Cons</strong>: Empirical plots donât show actual model predictions, and in particular donât control for the effects of other predictors.</p></li>
</ul>
</div>
<div id="categorical-factors-with-more-than-two-levels" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Categorical factors with more than two levels</h3>
<p>We are often interested in categorical predictors with more than two levels. For example, for the <a href="datasets-appendix.html#dregdata">Dutch <code>regularity</code> data</a>, we might wonder whether the size of a verbâs morphological family size is affected by what auxiliary it takes in the past tense.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">regularity %&gt;%<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(Auxiliary, FamilySize)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_boxplot</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.y=</span><span class="st">&quot;mean&quot;</span>, <span class="dt">geom=</span><span class="st">&quot;point&quot;</span>, <span class="dt">color=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-30-1.png" width="480" style="display: block; margin: auto;" /> The relevant variable, <code>Auxiliary</code>, has three levels. Letâs see how this kind of variable is dealt with in a regression model.</p>
<div id="exercise" class="section level4 unnumbered">
<h4>Exercise</h4>
<ol style="list-style-type: decimal">
<li><p>Fit a regression model predicting <code>FamilySize</code> from <code>Auxiliary</code>.</p></li>
<li><p>What does the intercept (<span class="math inline">\(\beta_0\)</span>) represent?</p></li>
<li><p>What do the two coefficients for <code>Auxiliary</code> (<span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>) represent?</p></li>
</ol>
<p>Hint: Compare <span class="math inline">\(\beta\)</span> coefficients with group means, which you can check using <code>summarise()</code> from <code>dplyr</code>.</p>
Solution to (1):
<div class="fold s o">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m4 &lt;-<span class="st"> </span><span class="kw">lm</span>(FamilySize ~<span class="st"> </span>Auxiliary, regularity)
<span class="kw">summary</span>(m4)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = FamilySize ~ Auxiliary, data = regularity)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.9133 -0.7982 -0.0250  0.7442  3.5983 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       2.59000    0.04902  52.839   &lt;2e-16 ***
## Auxiliaryzijn     0.39274    0.26780   1.467   0.1430    
## Auxiliaryzijnheb  0.32329    0.12594   2.567   0.0105 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.177 on 697 degrees of freedom
## Multiple R-squared:  0.01169,    Adjusted R-squared:  0.008855 
## F-statistic: 4.123 on 2 and 697 DF,  p-value: 0.0166</code></pre>
</div>
<p>Solution to (2): The value of <code>FamilySize</code> when <code>Auxiliary</code>=âhebbenâ.</p>
<p>Solution to (3): The predicted difference in <code>FamilySize</code> between <code>Auxiliary</code>=âzijnâ and âhebbenâ, and between <code>Auxiliary</code>=âzijnhebâ and âhebbenâ.</p>
</div>
</div>
<div id="releveling-factors" class="section level3">
<h3><span class="header-section-number">3.3.5</span> Releveling factors</h3>
<p>Itâs often useful for conceptual understanding to change the ordering of a factorâs levels. For the <code>regularity</code> example, we could make <code>zijn</code> the base level of the <code>Auxiliary</code> factor:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">regularity$Auxiliary &lt;-<span class="kw">relevel</span>(regularity$Auxiliary, <span class="st">&quot;zijn&quot;</span>)

m5 &lt;-<span class="st"> </span><span class="kw">lm</span>(FamilySize ~<span class="st"> </span>Auxiliary, regularity)
<span class="kw">summary</span>(m5)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = FamilySize ~ Auxiliary, data = regularity)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.9133 -0.7982 -0.0250  0.7442  3.5983 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       2.98274    0.26328  11.329   &lt;2e-16 ***
## Auxiliaryhebben  -0.39274    0.26780  -1.467    0.143    
## Auxiliaryzijnheb -0.06946    0.28771  -0.241    0.809    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.177 on 697 degrees of freedom
## Multiple R-squared:  0.01169,    Adjusted R-squared:  0.008855 
## F-statistic: 4.123 on 2 and 697 DF,  p-value: 0.0166</code></pre>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>What is the interpretation of the intercept and the two <code>Auxiliary</code> coefficients in this new model?</li>
</ul>
</blockquote>
</div>
</div>
<div id="linear-regression-assumptions" class="section level2">
<h2><span class="header-section-number">3.4</span> Linear regression assumptions</h2>
<p>Up to now, we have discussed regression models without worrying about the assumptions that are made by linear regression, about your data and the model. We will cover six main assumptions, the first four have to do with the form of the model and errors:</p>
<ol style="list-style-type: decimal">
<li><p>Linearity</p></li>
<li><p>Independence of errors</p></li>
<li><p>Normality of errors</p></li>
<li><p>Constancy of errors (<em>homoscediasticity</em>)</p></li>
</ol>
<p>followed by two assumptions about the predictors and observations:</p>
<ol start="5" style="list-style-type: decimal">
<li><p>Linear independence of predictors</p></li>
<li><p>Observations have roughly equal influence on the model</p></li>
</ol>
<p>Weâll discuss each in turn.</p>
<p>Our presentation of regression assumptions and diagnostics is indebted to Chapters 4, 6 and 9 of <span class="citation">Chatterjee &amp; Hadi (<a href="#ref-chatterjee2012regression">2012</a>)</span>, where you can find more detail.</p>
<div id="visual-methods-1" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Visual methods</h3>
<p>Visualization is crucial for checking model assumptions, and for data analysis in general. A famous example illustrating this is <em>Anscombeâs quartet</em>: a set of four small datasets of <span class="math inline">\((x,y)\)</span> pairs with:</p>
<ul>
<li><p>The same mean and variance for <span class="math inline">\(x\)</span></p></li>
<li><p>The same mean and variance for <span class="math inline">\(y\)</span></p></li>
<li><p>A correlation(<span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>) = 0.816</p></li>
<li><p>The same regression line (<span class="math inline">\(y = 3 + 0.5\cdot x\)</span>)</p></li>
</ul>
in each caseâand yet the datasets show qualitatively different patterns, as can be seen by plotting <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span>:
<center>
<img src="images/anscombe.png" />
</center>
<p>(Source: unknown, but definitely taken from somewhere)</p>
<!-- TODO FUTURE: find or fix -->
<p>With more than one predictor it becomes difficult to check regression assumptions by just plotting the data, and visual methods such as <em>residual plots</em> (presented below) are crucial.</p>
</div>
<div id="assumption-1-linearity" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Assumption 1: Linearity</h3>
<p>The first assumption of a linear regression model is that the relationship between the response (<span class="math inline">\(Y\)</span>) and predictors (<span class="math inline">\(X_i\)</span>) isâ¦ linear.</p>
<p>While obvious, this assumption is very important: if it is violated, the modelâs predictions can be in serious error.</p>
<p>The linearity assumption can be partially checked by making a scatterplot of <span class="math inline">\(Y\)</span> as a function of each predictor <span class="math inline">\(X_i\)</span>. It is hard to exhaustively check linearity for MLR, because nonlinearity might only become apparent when <span class="math inline">\(Y\)</span> is plotted as a function of several predictors.</p>
<div id="example-10" class="section level4 unnumbered">
<h4>Example</h4>
<p>Consider relative pitch, intensity, and duration in <a href="datasets-appendix.html#altdata">the <code>alternatives</code> dataset</a>.</p>
<p>We can make <em>pairwise plots</em> of these variables using the <code>pairscor.fnc()</code> function <code>languageR</code>, to see if any of these variables might be a function of the other two.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pairscor.fnc</span>(<span class="kw">with</span>(alt, <span class="kw">cbind</span>(rpitch, rintensity, rduration)))</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-33-1.png" width="480" style="display: block; margin: auto;" /></p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>Is this the case?</li>
</ul>
</blockquote>
<p>Letâs examine the relationship between realtive duration and relative intensity more closely:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alt %&gt;%<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(rduration, rintensity)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-34-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>We can try to fit a line to this data, but if we compare to using a nonlinear smoother, it seems clear that the relationship is not linear:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alt %&gt;%<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(rduration, rintensity)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">col=</span><span class="st">&#39;red&#39;</span>, <span class="dt">se=</span>F) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">se=</span>F)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-35-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>In particular, it looks like there is a quadratic trend. This means that we can in fact fit a linear regression, we just need to include coefficients for both <code>rduration</code> and its square, like so:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mq &lt;-<span class="st"> </span><span class="kw">lm</span>(rintensity ~<span class="st"> </span>rduration +<span class="st"> </span><span class="kw">I</span>(rduration^<span class="dv">2</span>), alt)
<span class="kw">summary</span>(mq)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = rintensity ~ rduration + I(rduration^2), data = alt)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -16.3199  -3.5197  -0.2448   2.9515  19.1480 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      5.8280     0.2622  22.228   &lt;2e-16 ***
## rduration        3.8841     0.3263  11.904   &lt;2e-16 ***
## I(rduration^2)  -3.1381     0.3429  -9.151   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.05 on 627 degrees of freedom
## Multiple R-squared:  0.358,  Adjusted R-squared:  0.3559 
## F-statistic: 174.8 on 2 and 627 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We will cover more nonlinear functions of predictors in <a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#nonlinear-effects">a later chapter</a>. The important point here is that a model with just <code>rduration</code> as a predictor would have violated the linearity assumption, but a model with both <code>rduration</code> and <code>rduration^2</code> as predictors doesnât (arguably).</p>
</div>
</div>
<div id="c2ioe" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Assumption 2: Independence of errors</h3>
<p>All regression equations we have considered contain an <em>error</em> or <em>residual</em> term: <span class="math inline">\(\epsilon_i\)</span> for the <span class="math inline">\(i^{\text{th}}\)</span> observation. A crucial assumption is that these <span class="math inline">\(\epsilon_i\)</span> are <strong>independent</strong>: knowing the error for one observation shouldnât tell you anything about the error for another observation.</p>
<p>Unfortunately, violations of this assumption are endemic in realistic data. The simplest example is in time series, or longitudinal dataâsuch as pitch measurements taken every 10 msec in a speech signal.</p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>Can you think of why this might be the case?</li>
</ul>
</blockquote>
<p>In linguistics and psycholinguistics, violations of the independence assuption are common because most datasets include multiple observations per participant or per item (or per word, etc.). Crucially, violations of the independence assumption are often <em>anti-conservative</em>: CIs will be too narrow and <span class="math inline">\(p\)</span>-values too small if the lack of independence of errors is not taken into account by the model.</p>
<p>Some solutions to these issues:</p>
<ul>
<li><p><a href="hypothesis-testing.html#paired-t-test"><strong>Paired-t-tests</strong></a>, where applicable (binary predictor; two measures per participant)</p></li>
<li><p><strong>Mixed-effects regression</strong> (more general solution, major focus later this term)</p></li>
</ul>
<p>Until we cover mixed-effects regression, we will be getting around the fact that the independence-of-errors assumption usually doesnât hold for linguistic data, in one of two ways:</p>
<ol style="list-style-type: decimal">
<li><p>Selectively using datasets where this assumption <strong>does</strong> hold, such as <code>regularity</code>.</p></li>
<li><p>Analyzing datasets where this assumption does <strong>not</strong> hold, such as <code>tapping</code> or <code>english</code>, using analysis methods that do assume indepedence of errors (such as linear regression), with the understanding that our regression models are probably giving results that are âwrongâ in some sense.</p></li>
</ol>
</div>
<div id="assumption-3-normality-of-errors" class="section level3">
<h3><span class="header-section-number">3.4.4</span> Assumption 3: Normality of errors</h3>
<p>The next major assumption is that the errors <span class="math inline">\(\epsilon_i\)</span> are normally distributed, with mean 0 and a fixed variance. This assumption is impossible to check directly, because we never observe the true <em>errors</em> <span class="math inline">\(\epsilon_i\)</span>, only the <em>residuals</em> <span class="math inline">\(e_i\)</span>. The residuals are no longer normally distributed (even if the errors are), because some observations will be more influential than others in determining the fitted responses <span class="math inline">\(\hat{y}_i\)</span> when fitting the least-squares estimates of the regression coefficients.</p>
<div id="example-11" class="section level4 unnumbered">
<h4>Example</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">young &lt;-<span class="st"> </span><span class="kw">filter</span>(english, AgeSubject==<span class="st">&#39;young&#39;</span>)

<span class="kw">set.seed</span>(<span class="dv">2903</span>)
young_sample &lt;-<span class="st"> </span>young %&gt;%<span class="st"> </span><span class="kw">sample_n</span>(<span class="dv">100</span>)

<span class="kw">ggplot</span>(young_sample, <span class="kw">aes</span>(WrittenFrequency, RTlexdec)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.5</span>) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Linear regression line and 95% CI&quot;</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-37-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The width of the confidence interval increases for points further from (average of <code>WrittenFrequency</code>, average of <code>RTlexdec</code>), because these points are more influential, causing the variance of the residuals to increaseâthus, the variance is not constant.</p>
<p>In order to correct for non-normality, the residuals are transformed in a way which accounts for the different influence of different observations (see e.g. <span class="citation">Chatterjee &amp; Hadi (<a href="#ref-chatterjee2012regression">2012</a>)</span> 4.3), to <em>studentized</em> or <em>standardized residuals</em>.<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a> (In R, by applying <code>rstudent</code> or <code>rstandard</code> to a fitted model.)</p>
<p>In general, we <strong>check assumptions about errors by examining the distribution of standardized residuals</strong>. This is because <strong>if</strong> the normality of errors assumption holds, <strong>then</strong> the standardized residuals will be normally distributed with mean 0 and fixed variance. So if they are not, we know the normality of errors assumption does not hold. (If they are, itâs not a guarantee that the normality of errors assumption holds, but we hope for the best.)</p>
</div>
<div id="c2ex2" class="section level4 unnumbered">
<h4>Example</h4>
<p>This exercises uses the <code>halfrhyme</code> data, briefly described <a href="datasets-appendix.html#halfdata">here</a>. Letâs abstract away from what the variables actually mean, and just think of them as <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>cohortSize, <span class="dt">y=</span>rhymeRating),  <span class="dt">data=</span><span class="kw">filter</span>(halfrhyme, conditionLabel==<span class="st">&#39;bad&#39;</span>)) +
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&#39;lm&#39;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_jitter</span>() +<span class="st"> </span><span class="kw">xlab</span>(<span class="st">&quot;X&quot;</span>) +<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Y&quot;</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>The distribution of the standarized residuals for the regression of <span class="math inline">\(Y\)</span> as a function of <span class="math inline">\(X\)</span> is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">halfrhyme.sub &lt;-<span class="st"> </span><span class="kw">filter</span>(halfrhyme, conditionLabel==<span class="st">&#39;bad&#39;</span> &amp;<span class="st"> </span>!<span class="kw">is.na</span>(cohortSize))
mod &lt;-<span class="st"> </span><span class="kw">lm</span>(rhymeRating ~<span class="st"> </span>cohortSize, <span class="dt">data=</span>halfrhyme.sub)
halfrhyme.sub$resid &lt;-<span class="st"> </span><span class="kw">rstandard</span>(mod)
<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>resid), <span class="dt">data=</span>halfrhyme.sub) +<span class="st"> </span><span class="kw">geom_histogram</span>() +<span class="st"> </span><span class="kw">xlab</span>(<span class="st">&quot;Residual (standardized)&quot;</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>Why do the residuals have this distribution?</li>
</ul>
</blockquote>
<p>This example illustrates probably the most common source of non-normal residuals: a highly non-normal distirbution of the predictor or response.</p>
</div>
<div id="effect-and-solution" class="section level4">
<h4><span class="header-section-number">3.4.4.1</span> Effect and solution</h4>
<p>Non-normality of residuals is a pretty common violation of regression assumptions. How much does it actually matter? <span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2007data">2007</a>)</span> (p.Â 46) argue ânot muchâ, at least in terms of the least-squares estimates of the regression line (i.e., the regression coefficient values), which is often what you are interested in.</p>
<p>However, non-normality of residuals, especially when severe, can signal other issues with the data, such as the presence of outliers, or the predictor or response being on the same scale. (Example: using non-log-transformed word frequency as a predictor.) Non-normality of residuals can often be dealt with by <strong>transforming the predictor or response</strong> to have a more normal distribution (see Sec. <a href="linear-regression.html#transforming-to-normality">3.4.7</a>).</p>
<p>Non-normality of residuals can also signal other errors, such as an important predictor missing.</p>
</div>
<div id="exercise-1" class="section level4 unnumbered">
<h4>Exercise</h4>
<ol style="list-style-type: decimal">
<li>Using the <code>english</code> data, plot <code>RTlexdec</code> as a function of <code>WrittenFrequency</code>, and add a linear regression line.</li>
</ol>
<div class="fold s o">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(english, <span class="kw">aes</span>(<span class="dt">x =</span> WrittenFrequency, <span class="dt">y =</span> RTlexdec)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="fl">0.5</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span>F)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-40-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<ol start="2" style="list-style-type: decimal">
<li><p>Do you think the residuals of this model are normally distributed? Why/why not?</p></li>
<li><p>Now plot a histogram of the standardized residuals of the mode. Does the plot confirm your first impressions?</p></li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m8 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>_______, english)
m8.resid.std &lt;-<span class="st"> </span><span class="kw">rstandard</span>(______)
<span class="kw">hist</span>(______, <span class="dt">breaks =</span> <span class="dv">50</span>)</code></pre></div>
<div class="fold s o">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">day9_plt1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(english, <span class="kw">aes</span>(WrittenFrequency, RTlexdec)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.5</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span>F)

m8 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec~WrittenFrequency, english)
m8.resid.std &lt;-<span class="st"> </span><span class="kw">rstandard</span>(m8)

day9_plt2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(m8.resid.std), <span class="kw">aes</span>(<span class="dt">x =</span> m8.resid.std)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">50</span>)

<span class="kw">grid.arrange</span>(day9_plt2, day9_plt1, <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-42-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<ol start="4" style="list-style-type: decimal">
<li>Now add <code>AgeSubject</code> to the model, and plot a histogram of its standardized residuals. What has changed? Why so?</li>
</ol>
<div class="fold s o">
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-43-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<p>This exercise shows one reason that examining the residual distribution is useful. If we didnât already know what the missing predictor was, the non-normality of the residual distribution gives us a way to look for an explanatory variable. (Look at observations in each mode of the distribution, see what they have in common.)</p>
</div>
</div>
<div id="assumtion-4-constancy-of-variance" class="section level3">
<h3><span class="header-section-number">3.4.5</span> Assumtion 4: Constancy of variance</h3>
<p><em>Homoscedasticity</em> is one of the trickier regression assumptions to think about: the assumption that <span class="math inline">\(\epsilon_i\)</span> is normally distributed with the same variance, across all values of the predictors.</p>
<p>For example, in our example modeling reaction time as a function of subject age and word frequency, it is assumed that the amount of variability in reaction time is similar for old speakers and young speakers, for high frequency words and young speakers, for observations of high frequency words for old speakers, and so on.</p>
<p>In <a href="linear-regression.html#c2ex2">the example above</a> from the <code>halfrhyme</code> data, the homoscedasticity assumption is violated: lower values of <span class="math inline">\(\hat{y}\)</span> show higher variance in the residuals.</p>
<p>In this case, the model shows <em>heteroscedasticity</em>.</p>
<p><strong>If</strong> homoscedasticity holds, <strong>then</strong> the standardized residuals are uncorrelated with the predictor values, and with the fitted values, and there should be a constant spread (variance) of residual values (y-axis) for each fitted or predictor value (x-axis). Thus, it is common to plot (standardized) residuals versus fitted values and versus predictors. (The fitted values-residuals plot is one of the diagnostic plots that shows up if you <code>plot(mod)</code> in R, where <code>mod</code> is a fitted model.) The desired pattern is a flat line, with the same variance for different x-axis values.</p>
<p>For the <code>halfrhyme</code> data, the fitted value-residuals plot looks like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">halfrhyme.sub &lt;-<span class="st"> </span><span class="kw">filter</span>(halfrhyme, conditionLabel==<span class="st">&#39;bad&#39;</span> &amp;<span class="st"> </span>!<span class="kw">is.na</span>(cohortSize))
mod &lt;-<span class="st"> </span><span class="kw">lm</span>(rhymeRating ~<span class="st"> </span>cohortSize, <span class="dt">data=</span>halfrhyme.sub)
halfrhyme.sub$resid &lt;-<span class="st"> </span><span class="kw">rstandard</span>(mod)
halfrhyme.sub$fitted &lt;-<span class="st"> </span><span class="kw">fitted</span>(mod)
<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>fitted, <span class="dt">y=</span>resid), <span class="dt">data=</span>halfrhyme.sub) +<span class="st"> </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_smooth</span>() +<span class="st"> </span><span class="kw">xlab</span>(<span class="st">&quot;Fitted values&quot;</span>) +<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Standardized residuals&quot;</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>There is greater variance for higher fitted values, indicating heteroscedacticity.</p>
<div id="effect-and-solution-1" class="section level4">
<h4><span class="header-section-number">3.4.5.1</span> Effect and solution</h4>
<p>In general, estimates of least-squares coefficients in the presence of heteroscedasticity are unbiased, but standard errors will be under- or over-estimated. This means that confidence intervals will be too narrow/wide and <span class="math inline">\(p\)</span>-values too low/high.</p>
<p>Heteroscedasticity is endemic in some types of data, such as from lexical statistics (<span class="citation">R. Baayen (<a href="#ref-baayen2008analyzing">2008</a>)</span>, p.Â 35). In other types of data, such as economic data, heteroscedasticity is so common that dealing with it is a primary concern in statistical analysis. Heteroscedasticity is discussed less frequently than other regression assumptions for linguistic data, but it is unclear whether this is because heteroscedasticity is less common than in other types of data or just has not been focused on by language scientists.</p>
<!-- I don't know (MS) whether heteroscedasticity is actually less common in linguistic data than in other kinds of data, or whether it's just not something that language scientists have focused on.  Certainly it is discussed less frequently than other regression assumptions. -->
<!-- Morgan: Do you want to leave this part (^^^) in here ??? -->
<p>One can often correct for heteroscedasticity by using various transformations of the response and predictors to get better estimates (<span class="citation">Chatterjee &amp; Hadi (<a href="#ref-chatterjee2012regression">2012</a>)</span>, Ch. 4). For example, in the <code>halfrhyme</code> example, it turns out that a stronger effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> (lower <span class="math inline">\(p\)</span>-value) can be detected once variance is stabilized.</p>
</div>
</div>
<div id="interim-summary" class="section level3">
<h3><span class="header-section-number">3.4.6</span> Interim summary</h3>
<ul>
<li><p><strong>Linearity</strong></p>
<ul>
<li><p>Serious violation if not met.</p></li>
<li><p>Fit data with non-linear trend (e.g.Â quadratic)</p></li>
<li><p>Transformed predictor/response to normality (e.g.Â log-transform)</p></li>
</ul></li>
<li><p><strong>Independence of error</strong>:</p>
<ul>
<li>In linguistic data: use mixed-effects regression<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a></li>
</ul></li>
<li><p><strong>Normality of errors</strong>:</p>
<ul>
<li><p>Not too serious violation if not met, but may signal issues with model/data</p></li>
<li><p>Remove outliers; transform <span class="math inline">\(X\)</span>/<span class="math inline">\(Y\)</span> to normality</p></li>
</ul></li>
<li><p><strong>Constancy of variance</strong>:</p>
<ul>
<li><p>Not commonly checked in linguistic data</p></li>
<li><p>Leads to uncertain regression estimates</p></li>
<li><p>Transform predictor/response to normality</p></li>
</ul></li>
</ul>
</div>
<div id="transforming-to-normality" class="section level3">
<h3><span class="header-section-number">3.4.7</span> Transforming to normality</h3>
<p>Normality of the distribution of the response and predictors (<span class="math inline">\(Y\)</span> and <span class="math inline">\(X_i\)</span>) is <strong>not</strong> an assumption of linear regression. This is a common misconception, perhaps because normality is an assumption of other basic statistical inference tools, such as <span class="math inline">\(t\)</span>-tests.</p>
<p>However, there is still good reason to be circumspect if <span class="math inline">\(Y\)</span> or <span class="math inline">\(X_i\)</span> are not normally distributed, because this can often lead to violations of regression assumptions. This is why it is recommended to transform the predictors and response to normality to fix violations of the linearity, normality of errors, and homoscedasticity assumptions. Because non-normality of <span class="math inline">\(Y\)</span> or <span class="math inline">\(X_i\)</span> can easily lead to violations of regression assumptions, it is sometimes recommended to transform them to normality just to be safe. This makes it less likely that a regression assumption will be violated, but also changes the interpretation of the transformed variable, which may make it harder to interpret the modelâs results.</p>
<p>For linguistic data, <em>logarithmic</em> transformations are often useful when working with skewed distributions, because many kinds of linguistic data are roughly <em>log-normally</em> distributed, meaning the log-transformed variable is normally distributed. Some examples:</p>
<ul>
<li><p>Lexical statistics (e.g.Â lexical frequency, probability)</p></li>
<li><p>Reaction times (e.g.Â naming latencies)</p></li>
<li><p>Duration measures in phonetics (syllable, phrase durations)</p></li>
</ul>
<p>Other transformations besides log are also used: reaction times are sometimes inverse or inverse-log-transformed (1/RT, log(1/RT)), and durations are sometimes square-root-transformed.</p>
<div id="example-distribution-of-raw-vs.log-lexical-frequency" class="section level4 unnumbered">
<h4>Example: Distribution of raw vs.Â log lexical frequency</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">english$WrittenFrequency_raw &lt;-<span class="st"> </span><span class="kw">exp</span>(english$WrittenFrequency)
english$WrittenFrequency_log &lt;-<span class="st"> </span>english$WrittenFrequency

day10_plt1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(english, <span class="kw">aes</span>(<span class="dt">x =</span> WrittenFrequency_raw)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Raw written Frequency&quot;</span>)
day10_plt2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(english, <span class="kw">aes</span>(<span class="dt">x =</span> WrittenFrequency_log)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Log written Frequency&quot;</span>)

<span class="kw">grid.arrange</span>(day10_plt1, day10_plt2, <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-45-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>R note: ggplot functions such as <code>scale_x_log10()</code> can be used to plot data in its raw units on a log scale, which is often more interpretable. Ex:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"> <span class="kw">ggplot</span>(english, <span class="kw">aes</span>(<span class="dt">x =</span> WrittenFrequency_raw)) +<span class="st"> </span><span class="kw">geom_histogram</span>() +<span class="st"> </span><span class="kw">scale_x_log10</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Written Frequency (per million words)&quot;</span>) +<span class="st"> </span><span class="kw">annotation_logticks</span>(<span class="dt">sides=</span><span class="st">&#39;b&#39;</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
</div>
<div id="c2ex3" class="section level4 unnumbered">
<h4>Exercise</h4>
<p>Consider the raw and log frequency measures for the <code>english</code> dataset, for young speakers:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">english$WrittenFrequency_raw &lt;-<span class="st"> </span><span class="kw">exp</span>(english$WrittenFrequency)
english$WrittenFrequency_log &lt;-<span class="st"> </span>english$WrittenFrequency

young &lt;-<span class="st"> </span><span class="kw">filter</span>(english, AgeSubject==<span class="st">&quot;young&quot;</span>)</code></pre></div>
<ol style="list-style-type: decimal">
<li>Plot <code>RTnaming</code> as a function of raw written frequency, and of log written frequency, with a green LOESS (smooth) regression line added to each plot (using <code>geom_smooth(color='green')</code>).</li>
</ol>
<div class="fold s o">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plt5 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(young, <span class="kw">aes</span>(WrittenFrequency_raw, RTnaming)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.5</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&#39;Raw written Frequency&#39;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se=</span>F, <span class="dt">color=</span><span class="st">&quot;green&quot;</span>)

plt6 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(young, <span class="kw">aes</span>(WrittenFrequency_log, RTnaming)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.5</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&#39;Log written Frequency&#39;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se=</span>F, <span class="dt">color=</span><span class="st">&quot;green&quot;</span>)
<span class="kw">grid.arrange</span>(plt5, plt6, <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-48-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>Is the linearity assumption met in each case?</li>
</ol>
<!-- The point here is that the linearity assumption isn't met when $Y$ is untransformed word frequency, or at least it's unclear, whereas the assumption looks safe for log-transformed $Y$. -->
</div>
</div>
<div id="assumption-5-linear-independence-of-predictors" class="section level3">
<h3><span class="header-section-number">3.4.8</span> Assumption 5: Linear independence of predictors</h3>
<p>A crucial assumption of linear regression is that the predictors are <em>linearly independent</em>, meaning it isnât possible to write one predictor as a linear function of the others. If you can do so, itâs impossible to disentangle the effect of different predictors on the response. For example, temperatures in farenheit and centigrade are related by a linear function (F = <span class="math inline">\(9/5\)</span>C + 32), so F and C are linearly dependent.</p>
<p>Another way of thinking of linear dependence is that in a linear regression where you predict one predictor as a function of the others, the <span class="math inline">\(R^2\)</span> would be 1. Linear dependence of predictors will either give a model error or a weird model output if you fit a linear regression in R, because the math to find least-squares estimates of coefficients doesnât work out if there is linear dependence: there is no longer a unique optimal solution. (For example, if the slope of F and C in a model were <span class="math inline">\(\beta_1 = 2\)</span> and <span class="math inline">\(\beta_2 = 0\)</span>, then a different model using slopes of 0 and <span class="math inline">\(10/9\)</span> would also work.)<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a></p>
<div id="exercise-2" class="section level4 unnumbered">
<h4>Exercise</h4>
<p>Define a new variable in the <code>english</code> dataset that is the average of <code>WrittenFrequency</code> and <code>Familiarity</code>, then fit a linear regression of <code>RTlexdec</code> as a function of this new variable, <code>WrittenFrequency</code>, and <code>Familiarity</code>. What looks odd in the model output?</p>
</div>
</div>
<div id="collinearity" class="section level3">
<h3><span class="header-section-number">3.4.9</span> Collinearity</h3>
<p>Full linear dependence of predictors is usually a sign that something is conceptually wrong with your data, or your model structure. In the farenheit/celsius example, it doesnât make conceptual sense to have both as predictors of anything.</p>
<p>However, it is very common for there to be <strong>partial</strong> dependence between predictorsâthat is, <span class="math inline">\(0 &lt; R^2 &lt; 1\)</span> when you regress one predictor on the others. This is called <em>multicollinearity</em>, or just <em>collinearity</em>.<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a> Collinearity is ubiquitous in lingusitic data, and can significantly affect the estimates and interpretations of regression coefficientsâparticularly when collinearity is âhighâ (say, <span class="math inline">\(|R|&gt;0.8\)</span>) However, (high) <strong>collinearity is not a violation of the assumptions of linear regression</strong>!</p>
<p>This figure may be useful to get an intuitive sense of what collinearity is, if you think of X1-X4 as four predictors which affect Y, and may be highly interrelated (right figure) or independent (left figure).</p>
<div class="figure">
<img src="images/collinearity.png" />

</div>
<p>(Source: <a href="http://www.creative-wisdom.com/computer/sas/collinear_stepwise.html" class="uri">http://www.creative-wisdom.com/computer/sas/collinear_stepwise.html</a>)</p>
<p>There can be collinearity among several predictorsâhence the term âmulticollinearityââwhere one predictor is a linear function of several others.</p>
<div id="exercise-3" class="section level4 unnumbered">
<h4>Exercise</h4>
<p>Suppose we are trying to predict the duration of the first vowel of every word in a dataset of conversational speech, using these four predictors:</p>
<ol style="list-style-type: decimal">
<li><p>log(speaking rate)</p></li>
<li><p>log(# of syllables in the word)</p></li>
<li><p>log(duration of word)</p></li>
<li><p>log(first syllable duration)</p></li>
</ol>
<p>where âspeaking rateâ is defined as âsyllables per secondâ.</p>
<p>Why are these predictors linearly dependent?</p>
</div>
<div id="exercise-4" class="section level4 unnumbered">
<h4>Exercise</h4>
<!-- > **Questions**:  -->
<!-- > -->
<!-- > * Is `RTlexdec` affected by a word's familiarity (`Familiarity`) for young speakers? -->
<p>Let us simulate a small-scale experiment (<span class="math inline">\(n=25\)</span>), using data from young speakers in the <code>english</code> dataset. We measure <code>RT</code>, take <code>Familiarity</code> norms from a previous study, and would like to control for the following variables:</p>
<ul>
<li><p><code>WrittenFrequency</code></p></li>
<li><p><code>LengthInLetters</code></p></li>
</ul>
<p>Because both are expected to correlate with <code>Familiarity</code>, and to affect <code>RT</code>.</p>
<p>We first examine whether there is collinearity among predictors, then assess whether familiarity affects lexical decision RT.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">young &lt;-<span class="st"> </span><span class="kw">filter</span>(english, AgeSubject ==<span class="st"> &quot;young&quot;</span>)
<span class="kw">set.seed</span>(<span class="dv">2903</span>)
d &lt;-<span class="st"> </span><span class="kw">sample_n</span>(young, <span class="dv">25</span>)

<span class="kw">pairscor.fnc</span>(dplyr::<span class="kw">select</span>(d, RTlexdec, Familiarity, WrittenFrequency, LengthInLetters))</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-49-1.png" width="672" style="display: block; margin: auto;" /></p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>What correlations are present in the data? Is there collinearity among predictors?</li>
</ul>
</blockquote>
<p>Now, fit a model of just familiarity on RT:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.fam &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>Familiarity, d)
<span class="kw">summary</span>(m.fam)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ Familiarity, data = d)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.094956 -0.035753  0.002142  0.050448  0.093090 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  6.645471   0.038231  173.83  &lt; 2e-16 ***
## Familiarity -0.047697   0.009502   -5.02 4.44e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.05699 on 23 degrees of freedom
## Multiple R-squared:  0.5228, Adjusted R-squared:  0.502 
## F-statistic:  25.2 on 1 and 23 DF,  p-value: 4.442e-05</code></pre>
<p>This model finds a highly significant effect of word familiarity on RT.</p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>What is the interpretation of this modelâwhat relationship is it capturing from the grid of plots above?</li>
</ul>
</blockquote>
<p>Now,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.fam_freq &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>Familiarity +<span class="st"> </span>WrittenFrequency +<span class="st"> </span>LengthInLetters, d)
<span class="kw">summary</span>(m.fam_freq)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ Familiarity + WrittenFrequency + LengthInLetters, 
##     data = d)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.090416 -0.040823 -0.008442  0.042555  0.094739 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       6.66953    0.07094  94.013   &lt;2e-16 ***
## Familiarity      -0.03333    0.01821  -1.831   0.0814 .  
## WrittenFrequency -0.01017    0.01117  -0.911   0.3728    
## LengthInLetters  -0.00643    0.01410  -0.456   0.6530    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.05838 on 21 degrees of freedom
## Multiple R-squared:  0.5429, Adjusted R-squared:  0.4776 
## F-statistic: 8.313 on 3 and 21 DF,  p-value: 0.0007782</code></pre>
<p>This model finds that <strong>none</strong> of the three variables significantly affect RT</p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li><p>Why has the <code>Familiarity</code> effect changed between the two models? (Examine both the coefficient estimates and SEs.)</p></li>
<li><p>How is it possible that none of the three variables significantly affect RT, but together they do predict half the variation in RT (<span class="math inline">\(R^2 = 0.54\)</span>)?</p></li>
</ul>
</blockquote>
<p>This kind of situation is called a <em>credit assignment problem</em>: we can tell that some combination of predictors <strong>together</strong> affects the response, but not whether an <strong>individual</strong> predictor does, after controlling for other predictors.</p>
</div>
<div id="effects-of-collinearity" class="section level4">
<h4><span class="header-section-number">3.4.9.1</span> Effects of collinearity</h4>
<p>In general, collinearity increases Type II error: the probability of concluding a predictor has no effect on the response, when it actually does. Collinearity does not in general affect the actual values of coefficient estimates, just their standard errors.<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a></p>
<p>Importantly, <strong>collinearity is a property of the data</strong>ânot a violation of model assumptions. To the extent that an effect is âmissedâ (a Type II error) in the presence of collinearity, this is because of the structure of the data. In the example above, it makes intuitive sense that it is harder to detect a ârealâ effect of <code>Familiarity</code> when <code>frequency</code> is added as a predictor, because of the correlation between <code>Familiarity</code> and <code>frequency</code>.</p>
<p>Because collinearity isnât a violation of model assumptions, high collinearity cannot be diagnosed using diagnostic plots, such as those considered for Assumptions 1-4 above.</p>
</div>
<div id="diagnosing-collinearity" class="section level4">
<h4><span class="header-section-number">3.4.9.2</span> Diagnosing collinearity</h4>
<p>How can one then detect collinearity, and decide if it could be affecting (the standard errors of) regression coefficients?</p>
<p>Some warning signs that there may be substantial collinearity in your data (<span class="citation">Chatterjee &amp; Hadi (<a href="#ref-chatterjee2012regression">2012</a>)</span>, 9.4):</p>
<ul>
<li><p><strong>Unstable coefficients</strong>: large changes in values of the <span class="math inline">\(\hat{\beta}_i\)</span> when predictors/data points are added/dropped.</p></li>
<li><p><strong>Nonsensical coefficients</strong>: signs of <span class="math inline">\(\hat{\beta}_i\)</span> donât conform to prior expectations.</p></li>
<li><p><strong>Unexpected non-significance</strong>: values of <span class="math inline">\(\hat{\beta}_i\)</span> for predictors expected to be important (e.g.Â from EDA) have large SEs, low <span class="math inline">\(t\)</span>-values, and high <span class="math inline">\(p\)</span>-values.</p></li>
</ul>
<p>These diagnostics all follow from the fact that when data is highly collinear, a linear relationship between predictors <strong>almost</strong> holds, so there are many regression coefficient estimates that give models <strong>almost</strong> as good as the least-squared estimates.</p>
<p>The degree of collinearity can be quantified using the <em>condition number</em> (<code>collin.fnc</code> in <code>languageR</code>), applied to the design matrix (where each column = values of one predictor, across the dataset). For example, for <code>frequency</code> and <code>AoA</code> in the example above, the condition number is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(languageR)
<span class="kw">collin.fnc</span>(dplyr::<span class="kw">select</span>(d, Familiarity, WrittenFrequency, LengthInLetters))$cnumber</code></pre></div>
<pre><code>## [1] 16.0162</code></pre>
<p>As a rule of thumb, condition numbers can be interpreted as (<span class="citation">R. Baayen (<a href="#ref-baayen2008analyzing">2008</a>)</span>, p.Â 200, citing <span class="citation">Belsley &amp; Welsch (<a href="#ref-belsley1980regression">1980</a>)</span>):</p>
<ul>
<li><p>CN &lt; 6: âno collinearityâ</p></li>
<li><p>CN &lt; 15: âacceptable collinearityâ</p></li>
<li><p>CN &gt; 30: âpotential harmful collinearityâ</p></li>
</ul>
</div>
<div id="is-collinearity-a-problem" class="section level4">
<h4><span class="header-section-number">3.4.9.3</span> Is collinearity a problem?</h4>
<p>There are two philosophies here.</p>
<p>The first is represented by <span class="citation">Chatterjee &amp; Hadi (<a href="#ref-chatterjee2012regression">2012</a>)</span> and <span class="citation">R. Baayen (<a href="#ref-baayen2008analyzing">2008</a>)</span>: (high) collinearity <strong>is</strong> a problemâit causes unstable coefficient estimates, increases Type II error, and can slow down or foil model fitting. Therefore, it should be somehow dealt with, for example by changing the predictors included in the model (âresidualizingâ or dimensionality reduction strategies such as âprincipal componentsâ).</p>
<p>The second is represented by <span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2007data">2007</a>)</span> and <span class="citation">R. Levy (<a href="#ref-levy2012probabilistic">2012</a>)</span>: collinearity <strong>is not</strong> a problemâthe issues above reflect a lack of information in your data, and how hard it is to detect effects of these variables based on this dataset. Therefore, you should either fit the model with your original predictors (which acknowledges the lack of information), or collect more data.</p>
<p>While we think there is real merit to the second view, there are certainly (very common) circumstances in which it is appropriate to take steps to decrease collinearity. Most common: centering all predictor variables decreases collinearity, without any loss of information. âResidualizingâ one predictor on another is generally not a good idea, because it complicates the interpretation of the regression coefficients in unintended ways <span class="citation">(Wurm &amp; Fisicaro, <a href="#ref-wurm2014residualizing">2014</a>)</span>.</p>
</div>
</div>
<div id="assumption-6-observations" class="section level3">
<h3><span class="header-section-number">3.4.10</span> Assumption 6: Observations</h3>
<p>Two linear regression assumptions about <strong>observations</strong> are that they are:</p>
<ol style="list-style-type: decimal">
<li><p>Equally <strong>reliable</strong></p></li>
<li><p>Roughly equally <strong>influential</strong></p></li>
</ol>
<p>The first assumption is hard to check in practice, and we wonât consider it further. The second assumption is important: observations should have âa roughly equal role in determining regression results and in influencing conclusionsâ (<span class="citation">Chatterjee &amp; Hadi (<a href="#ref-chatterjee2012regression">2012</a>)</span>, p.Â 88). This is because our goal in statistical analysis is usually to make inferences about <strong>population</strong> values of parameters (like the slope of a line of best fit), abstracting away from our finite sample. If certain observations are much more influential than others, they skew the regression results to reflect not the population, but the particular sample we happened to draw.</p>
<p>For example, consider the relationship between raw and log-transformed word frequency, and naming latency, in <a href="datasets-appendix.html#engdata">the <code>english</code> dataset</a>. (The same data as in <a href="linear-regression.html#c2ex3">this example above</a>, but now fitting a simple line of best fit in each case.)</p>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-53-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>When raw frequency is used as the predictor (left plot), the handful of âextremeâ observations with frequency above 25000 have a much greater effect on the slope of the line than other observations, resulting in larger confidence intervals.</p>
<p>Often, a non-normally distributed response or predictors leads to some points influencing the model much more than others, and the problem can be fixed by transforming to normalityâas for the word frequency example (right plot). After log-transforming frequency, removing the seven most extreme X values hardly affects the fitted line.</p>
<p>The presence of highly-influential observations can lead to either Type I or Type II errors: the influential observations might be responsible for a spurious result, or they might obscure a pattern that would be clear if the influential observations were excluded.</p>
<p>This isnât always the case, thoughâoften, some points are inherently more influential than others (say, a couple participants with behavior very different from others), and we need to decide how to proceed in fitting and interpreting the model.</p>
</div>
<div id="lin-reg-measuring-influence" class="section level3">
<h3><span class="header-section-number">3.4.11</span> Measuring influence</h3>
<p><em>Cookâs distance</em> is a metric of how influential an observation is in a given model, defined as the product of two terms:</p>
<ol style="list-style-type: decimal">
<li><p>The (squared) standardized residual of the observation</p></li>
<li><p>A measure of the <em>leverage</em> of this observationâhow much it affects the fitted values (<span class="math inline">\(\hat{y}_i\)</span>)</p></li>
</ol>
<p>Cookâs distance (CD) is roughly <strong>how much the regression coefficients change</strong> when the model is fitted with all data except this observation. Data points with significantly higher CD than other points are highly influential, and should be flagged and examined as potential outliers.</p>
<div id="example-12" class="section level4 unnumbered">
<h4>Example</h4>
<p>Letâs take a sample of the young-speaker subset of <a href="#c2engdata"><code>english</code></a> and tweak it, to get an intuition of what it means for observations to have high influence.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2903</span>)
young_subset &lt;-<span class="st"> </span><span class="kw">filter</span>(english, AgeSubject==<span class="st">&quot;young&quot;</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">100</span>)

<span class="co"># Tweak data to illustrate point </span>
young_subset[young_subset$Familiarity&gt;<span class="dv">6</span>,]$RTnaming &lt;-<span class="st"> </span><span class="fl">6.35</span>

<span class="kw">ggplot</span>(young_subset, <span class="kw">aes</span>(Familiarity, RTnaming)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span>F)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-54-1.png" width="480" style="display: block; margin: auto;" /></p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>Which points do you think have high influence on the model relating <code>Familiarity</code> and <code>RTnaming</code>?</li>
</ul>
</blockquote>
Letâs check our intuition by coloring the points according to their CD values:
<div class="fold o">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="kw">lm</span>(RTnaming ~<span class="st"> </span>Familiarity, young_subset)
young_subset$CD &lt;-<span class="st"> </span><span class="kw">cooks.distance</span>(m)

p3 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(young_subset, <span class="kw">aes</span>(Familiarity, RTnaming)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="dv">3</span>, <span class="kw">aes</span>(<span class="dt">color=</span>CD)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>)

p4 &lt;-<span class="st"> </span><span class="kw">qplot</span>(young_subset$CD)

<span class="kw">grid.arrange</span>(p3, p4, <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-55-1.png" width="864" style="display: block; margin: auto;" /></p>
</div>
<p>Indeed, two points are outliers in terms of Cookâs distance. We expect to get a very different regression line by removing these two points (why?):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">young_subset2 &lt;-<span class="st"> </span><span class="kw">filter</span>(young_subset, CD &lt;<span class="st"> </span><span class="fl">0.4</span>)

p5 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(young_subset2, <span class="kw">aes</span>(Familiarity, RTnaming)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>)

<span class="kw">grid.arrange</span>(p5, p3, <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="03-linear-regression_files/figure-html/unnamed-chunk-56-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>After removal of these two outliers, the regression line has negative slope instead of flat slope.</p>
</div>
</div>
<div id="outliers" class="section level3">
<h3><span class="header-section-number">3.4.12</span> Outliers</h3>
<p>Highly influential points are one example of <em>outliers</em>. It is also possible for points to have extreme values of the predictor or response, and yet not be highly influential. There are different philosophies on how to detect outliers (visual inspection versus numeric measures), and what to do with them. At a minimum:</p>
<ul>
<li><p>During data analysis, potential outliers should be flagged, examined, and considered for exclusion.</p></li>
<li><p>Note what effect excluding outliers <strong>would</strong> have on the analysis.</p></li>
<li><p>Report how outliers were dealt with, and ideally what effect they have on the analysis, in any write-up.</p></li>
</ul>
<!-- We may return later to different strategies for detecting and dealing with outliers.   -->
<p>In many cases inspection of gross outliers will reveal data points that should be clearly excluded, such as a participant who always gave the same response, or data coding errors that cannot be corrected. In other cases the decision is more subjective.</p>
</div>
<div id="regression-assumptions-reassurance" class="section level3">
<h3><span class="header-section-number">3.4.13</span> Regression assumptions: Reassurance</h3>
<!-- We have just spent quite a while discussing assumptions that are made by linear regression models (and one assumption that isn't made, but is important anyway---multicollinearity).   -->
<p>It may seem a bit daunting that there are so many ways for a regression model to <strong>not</strong> be appropriate. <!-- it is risky to  to actually fit a model to your data.   --> It is important to take a step back, and remember: <!-- Some reassurance:  --> when doing data analysis, you should just satisfy the assumptions of the statistical tool being used as best you can, and then <strong>fit and report your model anyway</strong>. You should not fit regression models blindly, and itâs important to be aware of the assumptions being made by these models, and their limitations. But the more you learn about regression (or any statistical technique), the easier it is to become paralyzed with fear by everything that could be wrong with your data. Keep in mind the dictum of George Box: âEssentially, all models are wrong, but some are useful.â That is, every model is just that: a <strong>model</strong> of reality, that is only as useful as the insight it can offer about the questions of interest. Often, minor violations of model assumptions donât affect the qualitative conclusions you can draw from the model.</p>
</div>
</div>
<div id="lm-model-comparison" class="section level2">
<h2><span class="header-section-number">3.5</span> Model comparison</h2>
<p>So far in our discussion of regression, we have assumed that the form of the model is given: we already know what predictors will be used, and what terms will be included in the model. In practice, with real data this is usually not clearâit is necessary to choose among several possible models of the same data, that differ in which terms they include. Choosing between models is called <em>variable selection</em> (or <em>model selection</em>). In order to perform variable selection, we need a way to perform <em>model comparison</em>: comparing two or more candidate models to assess which one is âbetterâ, in the sense of how well it fits the data relative to its expressive power (e.g.Â number of predictors).</p>
<p>We wish to compare models of the same dataset, with:</p>
<ul>
<li><p>Same response (<span class="math inline">\(Y\)</span>)</p></li>
<li><p>Different sets of predictors (<span class="math inline">\(X_i\)</span>)</p></li>
</ul>
<p>Model comparison techniques differ on whether they can compare ânestedâ models only, or both nested and ânon-nestedâ models.</p>
<div id="nested-model-comparison" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Nested model comparison</h3>
<p>Two models are <em>nested</em> if one is a subset of the other, in terms of the set of predictors included.</p>
<p>For example, these two models of <code>RT</code> (in the <code>english</code> dataset) are nested:</p>
<span class="math display">\[\begin{align*}
   \quad M_1 ~:~ Y &amp;= \beta_0 + \beta_1 \cdot \texttt{WrittenFrequency} + \epsilon\\
   \quad M_2 ~:~ Y &amp;= \beta_0 + \beta_1 \cdot \texttt{WrittenFrequency} + \beta_2 \cdot \texttt{Familiarity} + \epsilon
\end{align*}\]</span>
<p><span class="math inline">\(M_2\)</span> is called the <em>full</em> or <em>superset</em> model, and <span class="math inline">\(M_1\)</span> the <em>reduced</em> or <em>subset</em> model.</p>
<p>To compare these two models, we wish to perform a hypothesis test of: <span class="math display">\[
H_0 : \beta_2 = 0
\]</span></p>
<p>More generally, we wish to compare two nested models:</p>
<ul>
<li><p><span class="math inline">\(M_1\)</span>: predictors <span class="math inline">\(X_1, ..., X_q\)</span></p></li>
<li><p><span class="math inline">\(M_2\)</span>: predictors <span class="math inline">\(X_1, ..., X_p\)</span> (where <span class="math inline">\(q &lt; p\)</span>)</p></li>
</ul>
<p>By performing this hypothesis test:</p>
<p><span class="math display">\[
H_0~:~\beta_{p + 1} = \beta_{p + 2} = \cdots = \beta_q = 0
\]</span></p>
<p>We fit both models, and obtain a sum of squared residuals for each: <span class="math inline">\(RSS_1\)</span>, <span class="math inline">\(RSS_2\)</span>, which serves as a measure of how well the model fits the data. The RSS of a regression model can be scaled by <span class="math inline">\(n-p-1\)</span>, where <span class="math inline">\(p\)</span> is the number of predictors in the model, to give a measure of how well the model fits the data <strong>given the sample size and number of predictors</strong>: <span class="math display">\[
RSS/(n-p-1)
\]</span> In addition, the difference in RSS values between two nested models, with <span class="math inline">\(q\)</span> and <span class="math inline">\(p\)</span> predictors (<span class="math inline">\(p&gt;q\)</span>), can be scaled to give a measure of how much RSS has gone down, <strong>relative to whatâs expected</strong> given the additional predictors: <span class="math display">\[
(RSS_1 - RSS_2)/(p-q)
\]</span></p>
<p>Thus, this test statistic: <span class="math display">\[
F = \frac{(RSS_1 - RSS_2)/(p - q)}{RSS_2 / (n - p - 1)}
\]</span> gives a measure of how much the unexplained variance is reduced in the full model, with respect to the reduced model. Intuitively, <span class="math inline">\(R\)</span> divides how much effect âdroppingâ the <span class="math inline">\(q\)</span> extra predictors has on explained variance. Note that <span class="math inline">\(RSS_2\)</span> will always be smaller than <span class="math inline">\(RSS_1\)</span>, since adding predictors to a model canât give a <strong>worse</strong> fit to the data. What our hypothesis test checks is: does the superset model fit the data <strong>significantly</strong> better than the subset model, given the added complexity?</p>
<p>It turns out that this test statistic follows an <span class="math inline">\(F\)</span> distribution (under the null hypothesis above) with <span class="math inline">\(p-q\)</span>, <span class="math inline">\(n-p\)</span> degrees of freedom, written <span class="math inline">\(F_{p-q,n-p}\)</span>. So, we can perform hypothesis testing using an <a href="https://en.wikipedia.org/wiki/F-test"><span class="math inline">\(F\)</span>-test</a>, which may be familiar if you have seen ANOVAs.</p>
<div id="c2ex1" class="section level4 unnumbered">
<h4>Example</h4>
<p>This model comparison addresses the question: does adding <code>AgeSubject</code> and <code>LengthInLetters</code> to <code>m1</code> significantly reduce its unexplained variance?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>WrittenFrequency, english)
m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>WrittenFrequency +<span class="st"> </span>AgeSubject +<span class="st"> </span>LengthInLetters, english) 
<span class="kw">anova</span>(m1, m2)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: RTlexdec ~ WrittenFrequency
## Model 2: RTlexdec ~ WrittenFrequency + AgeSubject + LengthInLetters
##   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1   4566 91.194                                  
## 2   4564 35.004  2     56.19 3663.1 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The highly significant <span class="math inline">\(F\)</span>-test means that adding these two variables does significantly improve the model.</p>
<p><strong>Practical notes</strong>:</p>
<ul>
<li><p>In experimental literature (at least in linguistics), âmodel comparisonâ is often used as shorthand for ânested model comparison via an F testâ when comparing two linear regressions, because this is the most common way to do so. We will sometimes use this shorthand as well.</p></li>
<li><p>The results of such a hypothesis test are usually reported in parentheses, like âSubject age and the wordâs length in letters together affect RT beyond word frequency <span class="math inline">\((F(2, 4564) = 3663, p&lt;0.0001)\)</span>â.</p></li>
</ul>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>What are the reduced model and full model in the <span class="math inline">\(F\)</span> test reported at the bottom of every linear regressionâs output?</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ WrittenFrequency + AgeSubject + LengthInLetters, 
##     data = english)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.34438 -0.06041 -0.00695  0.05241  0.45157 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       6.8293072  0.0079946 854.245   &lt;2e-16 ***
## WrittenFrequency -0.0368919  0.0007045 -52.366   &lt;2e-16 ***
## AgeSubjectyoung  -0.2217215  0.0025915 -85.556   &lt;2e-16 ***
## LengthInLetters   0.0038897  0.0015428   2.521   0.0117 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.08758 on 4564 degrees of freedom
## Multiple R-squared:  0.6887,   Adjusted R-squared:  0.6885 
## F-statistic:  3366 on 3 and 4564 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Hint: fit <code>m0 &lt;- lm(RTlexdec ~ 1, english)</code> and compare to <code>m2</code>.</p>
</blockquote>
</div>
</div>
<div id="non-nested-model-comparison" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Non-nested model comparison</h3>
<p>In non-nested model comparison, one model isnât a subset of the other. For example, the two models <code>RT ~ Frequency</code> and <code>RT ~ Familiarity</code> would be non-nested.</p>
<p>Non-nested models can no longer be compared using sums-of-squares (using an <span class="math inline">\(F\)</span> statistic). Instead, a very different approach is used: <em>information criteria</em>. Instead of testing the hypothesis that certain model coefficients are zero, information criteria compare models based on two general goals:</p>
<ol style="list-style-type: decimal">
<li><p>Fit the data as well as possible</p></li>
<li><p>Have as few predictors as possible (âparsimonyâ)</p></li>
</ol>
<p>Different information criteria measures combine model likelihood (<span class="math inline">\(L\)</span>) and number of predictors (<span class="math inline">\(p\)</span>; the fewer the better) into a single value, of which the most common are:</p>
<ul>
<li>Akaike information criterion (AIC):</li>
</ul>
<p><span class="math display">\[ AIC = 2p - 2 \log(L)\]</span></p>
<ul>
<li>Bayesian information criterion (BIC)</li>
</ul>
<p><span class="math display">\[ BIC = p\log(n)  - 2 \log(L) \]</span></p>
<p>To apply either criterion, you calculate its value for each model in a set of candidate models, and pick the model with the lowest value.</p>
<p>Note that information criteria can be used to compare models regardless of whether they are nested or non-nested.</p>
<div id="c2ex4" class="section level4 unnumbered">
<h4>Example</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>WrittenFrequency, english) 
m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>WrittenFrequency +<span class="st"> </span>AgeSubject, english)
m3 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>WrittenFrequency +<span class="st"> </span>AgeSubject +<span class="st"> </span>LengthInLetters, english)
m4 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>WrittenFrequency +<span class="st"> </span>LengthInLetters, english)</code></pre></div>
<p>Using AIC and BIC to compare the models:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">AIC</span>(m1, m2, m3, m4)</code></pre></div>
<pre><code>##    df       AIC
## m1  3 -4908.994
## m2  4 -9274.590
## m3  5 -9278.948
## m4  4 -4909.437</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">BIC</span>(m1, m2, m3, m4)</code></pre></div>
<pre><code>##    df       BIC
## m1  3 -4889.713
## m2  4 -9248.883
## m3  5 -9246.814
## m4  4 -4883.729</code></pre>
<p>By AIC, we would choose <code>m3</code>, while by BIC we would choose <code>m2</code>. This illustrates a couple of points:</p>
<ul>
<li><p>Different model selection criteria donât necessarily give the same answer.</p></li>
<li><p>BIC tends to choose more parsimonious models than AIC.</p></li>
</ul>
</div>
</div>
<div id="c2varselect" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Variable selection</h3>
<p>Now that we have seen some methods to compare models with different sets of predictors, we can turn to variable selection: how to decide what predictors to keep in a given model.</p>
<p>There is no single best way to do variable selection, out of contextâeach method has pros and cons, and what method to use depends on the goals of your study. Your goal could be:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Prediction</strong> (estimating <span class="math inline">\(Y\)</span> for unseen data as accurately as possible)</p></li>
<li><p><strong>Explanation</strong> (choosing the right model from one of several possible pre-specified choices)</p></li>
<li><p><strong>Exploratory</strong> study</p></li>
</ol>
<p>In #1 and #2, our higher-level goal is generalization about âthe worldâ (unseen data). In #3, we are interested primarily in the data we have, and donât claim that our results generalize to the world. Weâll assume going forward that our goal is #1 or #2âmany studies in language sciences are in fact exploratory, but at present this isnât a goal that leads to publication.</p>
<!-- My (MS) impression is that inference of coefficient values is usually the goal in language sciences---we are usually interested in accurately determining the values of some parameters of interest, of which null-siginficance hypothesis testing is one part ("is the parameter zero?").  But, the choice of #2 over #1 may just be convention.   -->
<!-- TODO future: uncomment and flesh this out -->
<!-- In other fields, prediction is seen as more important, and indeed there is a debate within the stats-for-language-sciences literature on which of #1 and #2 to prioritize, by default (REFS).  -->
<!-- The choice of whether to prioritize (1) or (2)  has consequences for which variable selection method to use, but the issues are complex.  For example: roughly speaking, AIC tends to perform better for prediction, while BIC performs better for inference in the sense of choosing  -->
<!-- TODO Morgan: references for the last couple sentences -->
<p>It is important to know that different variable selection methods result in different final models, appropriate for different goals, because most papers that report variable selection simply say what they did, without justification. It is up to the reader to think, what might the final model be if a different variable selection method were used?</p>
<p>There are three components of any variable selection method:</p>
<ol style="list-style-type: decimal">
<li><p>How models are compared (e.g. <span class="math inline">\(F\)</span> test on nested models, AIC)</p></li>
<li><p>How the quality of a single model is evaluated</p></li>
<li><p>How itâs decided which of a set of models to choose, based on #1 and #2.</p></li>
</ol>
<p>Weâll consider a couple methods here, and cover others in later chapters.</p>
<div id="method-1-nested-model-comparison" class="section level4">
<h4><span class="header-section-number">3.5.3.1</span> Method 1: Nested model comparison</h4>
<p>As noted above, âmodel comparisonâ is often used as shorthand for âcomparison of nested models using F testsâ, when referring to linear regressions.</p>
</div>
<div id="exercise-5" class="section level4 unnumbered">
<h4>Exercise</h4>
<p>Which model of <code>m1</code>, <code>m2</code>, <code>m3</code> (from <a href="linear-regression.html#c2ex4">the example above</a>) would you choose based on nested model comparison (<span class="math inline">\(F\)</span> test)?</p>
<p>Note: <code>anova(x,y,z)</code> can be used as shorthand when <code>x</code>, <code>y</code>, <code>z</code> are nested models, and so on (<code>anova(w,x,y,z)</code>â¦).</p>
</div>
<div id="method-2-stepwise-variable-selection" class="section level4">
<h4><span class="header-section-number">3.5.3.2</span> Method 2: Stepwise variable selection</h4>
<p>A common type of variable selection uses âchoose automaticallyâ for Component 3 above (how do we decide which model to prefer). In <em>stepwise</em> variable selection, we decide whether to add or drop terms based on any model comparison procedure (Component 1): AIC, BIC, <span class="math inline">\(F\)</span> test with <span class="math inline">\(p&lt;0.05\)</span>, etc.</p>
<p>There are various flavors of stepwise model selection, such as <em>forwards</em>, where you start with an empty model (intercept only) and add terms, and <em>backwards</em>, where you start with a full model and drop terms.</p>
</div>
<div id="example-stepwise-backwards-selection-using-aic" class="section level4 unnumbered">
<h4>Example: Stepwise backwards selection using AIC</h4>
<p>This is what the <code>step</code> function in R does by default.</p>
<ol style="list-style-type: decimal">
<li><p>Start with the complete regression model (all possible predictors) and obtain AIC</p></li>
<li><p>At each âstepâ we try all possible ways of removing one of the predictors, and whichever yields the lowest AIC value is kept.</p></li>
<li><p>Repeat steps 1-2 until we end up with a model with lower AIC value than any of the other possible models that you could produce by deleting one of its predictors.</p></li>
</ol>
<p>Stepwise model selection is very popular, probably because it is easy to do using statistical software. It can be helpful to select among a huge set of predictors as a first pass, but has serious drawbacks.</p>
</div>
<div id="c2tfpp" class="section level4 unnumbered">
<h4>Exercise: 25 potential predictors</h4>
<p>Fit the same model with 25 potential predictors to two random halves of <a href="#c2engdata">the <code>english</code> data</a>, then perform stepwise backwards selection:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## split the English data in half, randomly:
<span class="kw">set.seed</span>(<span class="dv">2903</span>)
english<span class="fl">.1</span> &lt;-<span class="st"> </span>english %&gt;%<span class="st"> </span><span class="kw">sample_n</span>(<span class="kw">round</span>(<span class="kw">nrow</span>(english)/<span class="dv">2</span>))
english<span class="fl">.2</span> &lt;-<span class="st"> </span>english[!(<span class="kw">row.names</span>(english) %in%<span class="st"> </span><span class="kw">row.names</span>(english<span class="fl">.1</span>)),]

## fit full model to each half-dataset
m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>(WrittenFrequency +<span class="st"> </span>Familiarity +<span class="st"> </span>AgeSubject +<span class="st"> </span>LengthInLetters +<span class="st"> </span>FamilySize)^<span class="dv">3</span>, english<span class="fl">.1</span>)
m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>(WrittenFrequency +<span class="st"> </span>Familiarity +<span class="st"> </span>AgeSubject +<span class="st"> </span>LengthInLetters +<span class="st"> </span>FamilySize)^<span class="dv">3</span>, english<span class="fl">.2</span>)

## trace=0 suppresses ouput
m1.stepped &lt;-<span class="st"> </span><span class="kw">step</span>(m1, <span class="dt">trace=</span><span class="dv">0</span>)
m2.stepped &lt;-<span class="st"> </span><span class="kw">step</span>(m2, <span class="dt">trace=</span><span class="dv">0</span>)

## the two resulting models
<span class="kw">summary</span>(m1.stepped)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ WrittenFrequency + Familiarity + AgeSubject + 
##     LengthInLetters + FamilySize + WrittenFrequency:Familiarity + 
##     WrittenFrequency:AgeSubject + WrittenFrequency:LengthInLetters + 
##     WrittenFrequency:FamilySize + Familiarity:AgeSubject + Familiarity:LengthInLetters + 
##     Familiarity:FamilySize + AgeSubject:LengthInLetters + LengthInLetters:FamilySize + 
##     WrittenFrequency:Familiarity:AgeSubject + WrittenFrequency:AgeSubject:LengthInLetters, 
##     data = english.1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.42008 -0.05175 -0.00326  0.04572  0.37444 
## 
## Coefficients:
##                                                   Estimate Std. Error
## (Intercept)                                       7.189260   0.045538
## WrittenFrequency                                 -0.084197   0.011610
## Familiarity                                      -0.058426   0.014137
## AgeSubjectyoung                                  -0.413999   0.058362
## LengthInLetters                                  -0.004709   0.008692
## FamilySize                                       -0.112072   0.018225
## WrittenFrequency:Familiarity                      0.008855   0.001203
## WrittenFrequency:AgeSubjectyoung                  0.045885   0.011738
## WrittenFrequency:LengthInLetters                  0.004581   0.002131
## WrittenFrequency:FamilySize                       0.005662   0.001863
## Familiarity:AgeSubjectyoung                       0.030455   0.008445
## Familiarity:LengthInLetters                      -0.006493   0.002830
## Familiarity:FamilySize                            0.007705   0.003262
## AgeSubjectyoung:LengthInLetters                   0.013042   0.011006
## LengthInLetters:FamilySize                        0.006179   0.003240
## WrittenFrequency:Familiarity:AgeSubjectyoung     -0.006894   0.001396
## WrittenFrequency:AgeSubjectyoung:LengthInLetters -0.003181   0.002059
##                                                  t value Pr(&gt;|t|)    
## (Intercept)                                      157.875  &lt; 2e-16 ***
## WrittenFrequency                                  -7.252 5.61e-13 ***
## Familiarity                                       -4.133 3.71e-05 ***
## AgeSubjectyoung                                   -7.094 1.74e-12 ***
## LengthInLetters                                   -0.542 0.588063    
## FamilySize                                        -6.149 9.17e-10 ***
## WrittenFrequency:Familiarity                       7.360 2.56e-13 ***
## WrittenFrequency:AgeSubjectyoung                   3.909 9.53e-05 ***
## WrittenFrequency:LengthInLetters                   2.150 0.031699 *  
## WrittenFrequency:FamilySize                        3.039 0.002405 ** 
## Familiarity:AgeSubjectyoung                        3.606 0.000317 ***
## Familiarity:LengthInLetters                       -2.295 0.021842 *  
## Familiarity:FamilySize                             2.362 0.018245 *  
## AgeSubjectyoung:LengthInLetters                    1.185 0.236150    
## LengthInLetters:FamilySize                         1.907 0.056609 .  
## WrittenFrequency:Familiarity:AgeSubjectyoung      -4.940 8.39e-07 ***
## WrittenFrequency:AgeSubjectyoung:LengthInLetters  -1.545 0.122548    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.07727 on 2267 degrees of freedom
## Multiple R-squared:  0.755,  Adjusted R-squared:  0.7533 
## F-statistic: 436.6 on 16 and 2267 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m2.stepped)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ WrittenFrequency + Familiarity + AgeSubject + 
##     LengthInLetters + FamilySize + WrittenFrequency:Familiarity + 
##     WrittenFrequency:LengthInLetters + WrittenFrequency:FamilySize + 
##     Familiarity:AgeSubject + Familiarity:LengthInLetters + Familiarity:FamilySize + 
##     LengthInLetters:FamilySize + WrittenFrequency:LengthInLetters:FamilySize, 
##     data = english.2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.30263 -0.05327 -0.00526  0.04654  0.50932 
## 
## Coefficients:
##                                               Estimate Std. Error t value
## (Intercept)                                  7.0214673  0.0616493 113.894
## WrittenFrequency                            -0.0338445  0.0137689  -2.458
## Familiarity                                 -0.0768169  0.0139772  -5.496
## AgeSubjectyoung                             -0.2047469  0.0113464 -18.045
## LengthInLetters                              0.0228065  0.0137176   1.663
## FamilySize                                  -0.0362279  0.0365663  -0.991
## WrittenFrequency:Familiarity                 0.0072404  0.0009183   7.884
## WrittenFrequency:LengthInLetters            -0.0024614  0.0029950  -0.822
## WrittenFrequency:FamilySize                 -0.0096833  0.0055442  -1.747
## Familiarity:AgeSubjectyoung                 -0.0042687  0.0028607  -1.492
## Familiarity:LengthInLetters                 -0.0044449  0.0028657  -1.551
## Familiarity:FamilySize                       0.0137811  0.0031320   4.400
## LengthInLetters:FamilySize                  -0.0059273  0.0082370  -0.720
## WrittenFrequency:LengthInLetters:FamilySize  0.0020024  0.0012937   1.548
##                                             Pr(&gt;|t|)    
## (Intercept)                                  &lt; 2e-16 ***
## WrittenFrequency                              0.0140 *  
## Familiarity                                 4.32e-08 ***
## AgeSubjectyoung                              &lt; 2e-16 ***
## LengthInLetters                               0.0965 .  
## FamilySize                                    0.3219    
## WrittenFrequency:Familiarity                4.87e-15 ***
## WrittenFrequency:LengthInLetters              0.4113    
## WrittenFrequency:FamilySize                   0.0808 .  
## Familiarity:AgeSubjectyoung                   0.1358    
## Familiarity:LengthInLetters                   0.1210    
## Familiarity:FamilySize                      1.13e-05 ***
## LengthInLetters:FamilySize                    0.4718    
## WrittenFrequency:LengthInLetters:FamilySize   0.1218    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.08024 on 2270 degrees of freedom
## Multiple R-squared:  0.7444, Adjusted R-squared:  0.7429 
## F-statistic: 508.4 on 13 and 2270 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Compare the two resulting models:</p>
<ul>
<li><p>How similar or different are the resulting terms in the model, and coefficient values?</p></li>
<li><p>Why is the degree of (non-)similarity a problem?</p></li>
</ul>
<hr />
<p>In general, fully automatic model selection procedures are <strong>dangerous</strong>: they can easily find spurious effects, because automatically dropping terms with high <span class="math inline">\(p\)</span>-values leads to inflation (= lower values) of significances for remaining terms. Stepwise procedures can also (to a lesser extent) miss true effects. Some statisticians recommend that automatic model selection should never be used at all, as it is simply too easy and too dangerous. At a minimum, automatic model selection procedures should never be used <strong>alone</strong> for variable selectionâthat is, in the absence of another method, such as careful exploratory data analysis. Despite this, stepwise procedures are widely used in language research, and you should critically examine any model arrived at in this way.<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a></p>
</div>
<div id="method-3-gelman2007data" class="section level4">
<h4><span class="header-section-number">3.5.3.3</span> Method 3: <span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2007data">2007</a>)</span></h4>
<p><span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2007data">2007</a>)</span> (p.Â 69) suggest a holistic approach to variable selection, that takes common sense into account in evaluating model quality (Component 2 above), in addition to quantitative model comparison (Component 1):</p>
<ol style="list-style-type: decimal">
<li><p>Include all predictors that, <strong>for substantive reasons</strong>, are expected to be important (e.g.Â speech rate when modeling vowel duration).</p></li>
<li><p>For predictors with large effects, consider interactions as well.</p></li>
<li><p>Now decide whether to exclude predictors, one at a time:</p>
<ul>
<li><p>Not significant, coefficient has expected sign: Consider leaving in.</p></li>
<li><p>Not significant, wrong sign: Remove.</p></li>
<li><p>Significant, wrong sign: Think hard if somethingâs wrong.</p></li>
<li><p>Significant, right sign: Keep in.</p></li>
</ul></li>
</ol>
<p>This approach requires you to have a sense of what the ârightâ and âwrongâ sign of different coefficients are, either for substantive reasons (previous work suggests an effect direction), or from exploratory data analysis. If your sense of ârightâ and âwrongâ for a given model coefficient is based <strong>purely</strong> on EDA, itâs important to remember that you may be simply modeling this dataset well (goal = exploratory analysis) rather than obtaining results that generalize to unseen data (goal = prediction or explanation).</p>
</div>
<div id="c2ex5" class="section level4 unnumbered">
<h4>Exercise: Phrase medial devoicing in European French</h4>
<p>This dataset is described in <a href="datasets-appendix.html#devdata">the appendix</a>. We are interested in whether function words are shorter than content wordsâconsidering just words which are a consonant-vowel sequenceâafter controlling for the consonant and vowel identities (since different Cs/Vs are intrinsically longer than others) and how fast the speaker is speaking.</p>
<p>In terms of the variables in this dataset:</p>
<ul>
<li><p>Is there is an effect of <code>func</code> on <code>syldur</code>â¦</p></li>
<li><p>â¦ after controlling for <code>c1</code>, <code>v</code>, and <code>speechrate</code>?</p></li>
</ul>
<p><strong>Part 1</strong>: Model comparison</p>
<p>Use (nested) model comparison to compare these three models:</p>
<ol style="list-style-type: decimal">
<li><p><code>syldur~(speechrate+c1+v)^2</code></p></li>
<li><p><code>syldur~(speechrate+c1+v)^2+func</code></p></li>
<li><p><code>syldur~(speechrate+c1+v)^2*func</code></p></li>
</ol>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>Which model ends up being chosen, and what do you conclude about the research question?</li>
</ul>
</blockquote>
<div class="fold s o">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#model comparison</span>
m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(syldur~(speechrate+c1+v)^<span class="dv">2</span>, df)
m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(syldur~(speechrate+c1+v)^<span class="dv">2</span>+func, df)
m3 &lt;-<span class="st"> </span><span class="kw">lm</span>(syldur~(speechrate+c1+v)^<span class="dv">2</span>*func, df)
<span class="kw">anova</span>(m1, m2, m3)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: syldur ~ (speechrate + c1 + v)^2
## Model 2: syldur ~ (speechrate + c1 + v)^2 + func
## Model 3: syldur ~ (speechrate + c1 + v)^2 * func
##   Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   
## 1    529 528092                                
## 2    528 518990  1    9102.4 9.3121 0.002392 **
## 3    523 511219  5    7770.7 1.5900 0.161123   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = syldur ~ (speechrate + c1 + v)^2 + func, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -130.682  -21.233    0.304   20.544  103.960 
## 
## Coefficients: (1 not defined because of singularities)
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    247.5718    23.8023  10.401  &lt; 2e-16 ***
## speechrate     -15.6166     3.2983  -4.735 2.82e-06 ***
## c1k            -89.2652    26.6617  -3.348 0.000872 ***
## c1p            -67.4682    39.4741  -1.709 0.088006 .  
## c1s            -27.7955    26.1708  -1.062 0.288683    
## c1t            -68.6392    27.3605  -2.509 0.012416 *  
## vu               4.6582    31.5829   0.147 0.882802    
## vy              16.2882    20.2343   0.805 0.421194    
## funcf          -13.8005     4.5350  -3.043 0.002458 ** 
## speechrate:c1k   9.5259     3.7368   2.549 0.011077 *  
## speechrate:c1p   9.2280     5.8037   1.590 0.112428    
## speechrate:c1s   4.4121     3.5902   1.229 0.219637    
## speechrate:c1t   7.5638     3.7154   2.036 0.042268 *  
## speechrate:vu   -0.6849     3.9631  -0.173 0.862851    
## speechrate:vy   -1.6412     2.6474  -0.620 0.535579    
## c1k:vu          19.8107    22.1307   0.895 0.371104    
## c1p:vu          -1.3369    25.6737  -0.052 0.958489    
## c1s:vu           0.9348    22.3409   0.042 0.966639    
## c1t:vu               NA         NA      NA       NA    
## c1k:vy          -3.1728    13.8966  -0.228 0.819491    
## c1p:vy          -9.7528    15.2713  -0.639 0.523338    
## c1s:vy         -18.2987    11.6317  -1.573 0.116279    
## c1t:vy          -5.4756    11.9582  -0.458 0.647219    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 31.35 on 528 degrees of freedom
## Multiple R-squared:  0.3012, Adjusted R-squared:  0.2734 
## F-statistic: 10.84 on 21 and 528 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<p><strong>Part 2</strong>: Stepwise backwards selection</p>
<p>Apply stepwise backwards model selection to this model:</p>
<ul>
<li><code>syldur~(speechrate+c1+v+func)^2</code></li>
</ul>
<p>using <code>step</code>.</p>
<div class="fold s o">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Backwards selection</span>
m =<span class="st"> </span><span class="kw">lm</span>(syldur~(speechrate+c1+v+func)^<span class="dv">2</span>, df)
<span class="kw">summary</span>(<span class="kw">step</span>(m, <span class="dt">trace=</span><span class="dv">0</span>))</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = syldur ~ speechrate + c1 + func + speechrate:c1 + 
##     c1:func, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -129.449  -19.780    0.762   21.343  104.048 
## 
## Coefficients: (2 not defined because of singularities)
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     255.813     20.798  12.300  &lt; 2e-16 ***
## speechrate      -16.508      2.991  -5.520  5.3e-08 ***
## c1k             -87.803     25.813  -3.401 0.000720 ***
## c1p             -71.475     37.858  -1.888 0.059566 .  
## c1s             -41.730     23.963  -1.741 0.082180 .  
## c1t             -64.292     24.776  -2.595 0.009719 ** 
## funcf           -21.398      5.558  -3.850 0.000132 ***
## speechrate:c1k   10.350      3.559   2.908 0.003785 ** 
## speechrate:c1p    9.105      5.564   1.637 0.102310    
## speechrate:c1s    5.058      3.422   1.478 0.139894    
## speechrate:c1t    7.322      3.498   2.093 0.036832 *  
## c1k:funcf        -1.833      8.565  -0.214 0.830655    
## c1p:funcf            NA         NA      NA       NA    
## c1s:funcf        17.373      7.218   2.407 0.016427 *  
## c1t:funcf            NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 31.14 on 537 degrees of freedom
## Multiple R-squared:  0.2987, Adjusted R-squared:  0.283 
## F-statistic: 19.06 on 12 and 537 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li><p>Which model ends up being chosen, and what do you conclude about the research question?</p></li>
<li><p>What would be different about your answer to the research question using stepwise backwards selection versus nested model comparison? Why?</p></li>
</ul>
</blockquote>
<p><strong>Part 3</strong>: Gelman &amp; Hillâs method</p>
<p>This method requires more domain specific knowledge, so weâll walk through it together.</p>
<p>First, all four possible predictors are included as main effectsâthey are all expected to affect vowel duration for substantive reasons:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># G&amp;H</span>
m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(syldur~speechrate+c1+v+func, df)
<span class="kw">summary</span>(m1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = syldur ~ speechrate + c1 + v + func, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -131.151  -21.337    0.863   20.419   98.228 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 208.8600     7.9734  26.195  &lt; 2e-16 ***
## speechrate   -9.5866     0.9546 -10.043  &lt; 2e-16 ***
## c1k         -26.6525     5.8330  -4.569 6.07e-06 ***
## c1p         -11.0561     6.9893  -1.582  0.11426    
## c1s          -2.9871     5.3464  -0.559  0.57659    
## c1t         -18.3920     5.8075  -3.167  0.00163 ** 
## vu           12.3446     6.1420   2.010  0.04494 *  
## vy           -2.0698     3.4933  -0.592  0.55377    
## funcf       -13.3345     3.2684  -4.080 5.19e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 31.47 on 541 degrees of freedom
## Multiple R-squared:  0.2787, Adjusted R-squared:  0.2681 
## F-statistic: 26.13 on 8 and 541 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Every predictor has a significant effect. Letâs suppose that <code>speechrate</code> in particular has a large effect (weâll discuss soon how to assess effect size). We thus consider interactions with speech rate:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(syldur~speechrate*(func+c1+v), df)
<span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = syldur ~ speechrate * (func + c1 + v), data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -130.275  -20.390    0.522   20.501  103.274 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       255.785     22.952  11.144  &lt; 2e-16 ***
## speechrate        -16.386      3.274  -5.005 7.58e-07 ***
## funcf             -29.289     17.059  -1.717  0.08658 .  
## c1k               -86.729     28.237  -3.071  0.00224 ** 
## c1p               -66.594     38.304  -1.739  0.08269 .  
## c1s               -29.059     26.392  -1.101  0.27137    
## c1t               -60.538     27.829  -2.175  0.03004 *  
## vu                 -6.158     27.997  -0.220  0.82599    
## vy                  3.596     18.342   0.196  0.84463    
## speechrate:funcf    2.364      2.384   0.992  0.32172    
## speechrate:c1k      8.949      4.009   2.232  0.02604 *  
## speechrate:c1p      8.235      5.645   1.459  0.14522    
## speechrate:c1s      3.813      3.775   1.010  0.31291    
## speechrate:c1t      6.203      3.965   1.564  0.11829    
## speechrate:vu       2.217      3.953   0.561  0.57512    
## speechrate:vy      -1.013      2.585  -0.392  0.69529    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 31.32 on 534 degrees of freedom
## Multiple R-squared:  0.2945, Adjusted R-squared:  0.2747 
## F-statistic: 14.86 on 15 and 534 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>There seems to be a significant interaction with preceding consonant, in particular preceding /k/ means less duration decrease with increasing rate. There isnât an immediately obvious explanation for this interaction, so itâs not âwrong signâ, and itâs fine to leave in the model. <!-- This makes sense, because in languages with "short lag" stops (like French p/t/k), duration of these stops depends minimally on speech rate.   --> <!-- So, we flag the `speechrate:c1` term for inclusion. --></p>
<!-- TODO future: include reference here and make more understandable if not a phonetician -->
<p><code>func</code> also has a large effect, and is of primary interest, so we consider its potential interactions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m3 &lt;-<span class="st"> </span><span class="kw">lm</span>(syldur~func*(speechrate+c1+v), df)
<span class="kw">summary</span>(m3)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = syldur ~ func * (speechrate + c1 + v), data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -132.663  -21.550    1.495   18.620  101.766 
## 
## Coefficients: (4 not defined because of singularities)
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      223.1977    11.2756  19.795  &lt; 2e-16 ***
## funcf            -46.3965    14.9707  -3.099  0.00204 ** 
## speechrate       -11.8593     1.4817  -8.004 7.47e-15 ***
## c1k              -18.3159     7.5125  -2.438  0.01509 *  
## c1p              -11.6366     6.9622  -1.671  0.09522 .  
## c1s               -7.8271     5.5722  -1.405  0.16070    
## c1t              -13.3023     6.4106  -2.075  0.03846 *  
## vu                11.5966     6.8487   1.693  0.09098 .  
## vy                 0.8120     4.3953   0.185  0.85350    
## funcf:speechrate   3.4852     1.9453   1.792  0.07376 .  
## funcf:c1k          0.3863    10.7540   0.036  0.97136    
## funcf:c1p              NA         NA      NA       NA    
## funcf:c1s         19.3740     8.7143   2.223  0.02661 *  
## funcf:c1t              NA         NA      NA       NA    
## funcf:vu               NA         NA      NA       NA    
## funcf:vy               NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 31.22 on 538 degrees of freedom
## Multiple R-squared:  0.2941, Adjusted R-squared:  0.2797 
## F-statistic: 20.38 on 11 and 538 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Thereâs a significant <code>func</code>:<code>c1</code> interaction. Itâs not clear what is the ârightâ or âwrongâ sign here, but since <code>func</code> is of primary interest, it is prudent to include this term in the model, in case itâs controlling for something that might otherwise spuriously give a significant <code>func</code> effect.</p>
<p>Our final model is then:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m4 &lt;-<span class="st"> </span><span class="kw">lm</span>(syldur~speechrate+c1+func+speechrate:c1+func:c1, df)
<span class="kw">summary</span>(m4)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = syldur ~ speechrate + c1 + func + speechrate:c1 + 
##     func:c1, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -129.449  -19.780    0.762   21.343  104.048 
## 
## Coefficients: (2 not defined because of singularities)
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     255.813     20.798  12.300  &lt; 2e-16 ***
## speechrate      -16.508      2.991  -5.520  5.3e-08 ***
## c1k             -87.803     25.813  -3.401 0.000720 ***
## c1p             -71.475     37.858  -1.888 0.059566 .  
## c1s             -41.730     23.963  -1.741 0.082180 .  
## c1t             -64.292     24.776  -2.595 0.009719 ** 
## funcf           -21.398      5.558  -3.850 0.000132 ***
## speechrate:c1k   10.350      3.559   2.908 0.003785 ** 
## speechrate:c1p    9.105      5.564   1.637 0.102310    
## speechrate:c1s    5.058      3.422   1.478 0.139894    
## speechrate:c1t    7.322      3.498   2.093 0.036832 *  
## c1k:funcf        -1.833      8.565  -0.214 0.830655    
## c1p:funcf            NA         NA      NA       NA    
## c1s:funcf        17.373      7.218   2.407 0.016427 *  
## c1t:funcf            NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 31.14 on 537 degrees of freedom
## Multiple R-squared:  0.2987, Adjusted R-squared:  0.283 
## F-statistic: 19.06 on 12 and 537 DF,  p-value: &lt; 2.2e-16</code></pre>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li><p>What do you conclude about the research question, based on the final model?</p></li>
<li><p>What does the signifcant <code>func</code>:<code>c1</code> interaction mean for the research question? (Does the effect of <code>func</code> have the same direction regardless of <code>c1</code>?)</p></li>
</ul>
</blockquote>
</div>
</div>
<div id="interpretability-issues" class="section level3">
<h3><span class="header-section-number">3.5.4</span> Interpretability issues</h3>
<p>A crucial aspect of interpreting regression model results is being able to interpret and compare coefficient values.</p>
<p>Consider this model for <a href="#c2engdata">the <code>english</code> data</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m6 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>WrittenFrequency +<span class="st"> </span>LengthInLetters +<span class="st"> </span>AgeSubject, english)
<span class="kw">summary</span>(m6)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ WrittenFrequency + LengthInLetters + 
##     AgeSubject, data = english)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.34438 -0.06041 -0.00695  0.05241  0.45157 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       6.8293072  0.0079946 854.245   &lt;2e-16 ***
## WrittenFrequency -0.0368919  0.0007045 -52.366   &lt;2e-16 ***
## LengthInLetters   0.0038897  0.0015428   2.521   0.0117 *  
## AgeSubjectyoung  -0.2217215  0.0025915 -85.556   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.08758 on 4564 degrees of freedom
## Multiple R-squared:  0.6887, Adjusted R-squared:  0.6885 
## F-statistic:  3366 on 3 and 4564 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Two issues make the coefficients difficult to interpret:</p>
<ol style="list-style-type: decimal">
<li><p>The <strong>intercept value</strong> is not interesting or meaningful</p>
<ul>
<li><p>Predicted response when all <span class="math inline">\(X\)</span> are 0.</p></li>
<li><p>What does <code>WrittenFrequency</code>=0, <code>LengthInLetters</code>=0 mean?</p></li>
</ul></li>
<li><p><strong>Coefficient values arenât comparable</strong>, because predictors are on different scales.</p>
<ul>
<li><p>Change of 1 in <code>AgeSubject</code> covers 100% of the data, while change of 1 in <code>WrittenFrequency</code> only covers part of the data.</p></li>
<li><p>We need comparable coefficient values to help assess <em>effect size</em>.</p></li>
</ul></li>
</ol>
<div id="solutions" class="section level4">
<h4><span class="header-section-number">3.5.4.1</span> Solutions</h4>
<p>We can address these two issues by transforming predictors before fitting the model. There are two common ways to <em>standardize</em> predictors, considering for now just continuous predictors and binary factors (no multi-level factors):</p>
<ol style="list-style-type: decimal">
<li><p><em>z-score</em>: subtract mean (center) and divide by standard deviation</p>
<ul>
<li><p>This measure is similar to Cohenâs <span class="math inline">\(d\)</span> for binary predictors (which weâll discuss soon)</p></li>
<li><p><code>scale()</code> function in R</p></li>
</ul></li>
<li><p>Alternative method (<span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2007data">2007</a>)</span> Â§4.2):</p>
<ul>
<li><p>Continuous predictors: center and divide by <strong>2</strong> SD.</p></li>
<li><p>Binary predictors: transform to have mean 0 and difference of 1 between values (i.e. -0.5/0.5 for balanced data)</p></li>
<li><p>Interpretation of âunit change of 1â is similar for continuous and binary predictors <span class="math inline">\(\implies\)</span> can compare effect sizes.</p></li>
<li><p>Use <code>rescale()</code> in the <code>arm</code> package.</p></li>
</ul></li>
</ol>
<p>Method 2, while less standard, has advantages discussed by <span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2007data">2007</a>)</span>, and weâll use it going forward.</p>
<p>New interpretations of coefficients:</p>
<ul>
<li><p><strong>Intercept</strong>: predicted <span class="math inline">\(Y\)</span> when all predictors are held at mean value</p></li>
<li><p><strong>Main effect</strong> coefficients for continuous predictors: predicted change in <span class="math inline">\(Y\)</span> when predictor changed by 2 SD, with other predictors held at <strong>mean</strong> values.</p></li>
<li><p><strong>Main effect</strong> coefficients for binary predictors: difference between the two levels. (Which is the same as 2 SD.)</p></li>
</ul>
<p><strong>Practical notes</strong>:</p>
<ul>
<li><p>When you report standardization of variables in a paper, report the information needed to replicate what you didânot the same of particular software package or function used. For example, youâd write âcontinuous variables were standardized by centering and dividing by two standard deviationsâ, not âcontinuous variables were standardized using the <code>rescale()</code> function in the <code>arm</code> packageâ. The latter kind of report is common and unhelpful.</p></li>
<li><p>âRescalingâ, ârescaled variablesâ etc. are <strong>not</strong> standard terminology, and should not be used in any write-up. Instead, refer to âstandardizedâ variables, defining at some point what âstandardizedâ means.</p></li>
</ul>
</div>
<div id="example-13" class="section level4 unnumbered">
<h4>Example</h4>
<p>Model of <code>RTlexdec</code> as a function of <code>WrittenFrequency</code>, <code>LengthInLetters</code>, <code>AgeSubject</code>, using raw and standardized predictors:</p>
<p>Before standardizing:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m6)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ WrittenFrequency + LengthInLetters + 
##     AgeSubject, data = english)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.34438 -0.06041 -0.00695  0.05241  0.45157 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       6.8293072  0.0079946 854.245   &lt;2e-16 ***
## WrittenFrequency -0.0368919  0.0007045 -52.366   &lt;2e-16 ***
## LengthInLetters   0.0038897  0.0015428   2.521   0.0117 *  
## AgeSubjectyoung  -0.2217215  0.0025915 -85.556   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.08758 on 4564 degrees of freedom
## Multiple R-squared:  0.6887, Adjusted R-squared:  0.6885 
## F-statistic:  3366 on 3 and 4564 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>After standardizing:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">english2 &lt;-<span class="st"> </span><span class="kw">mutate</span>(english,
       <span class="dt">WrittenFrequency =</span> <span class="kw">rescale</span>(WrittenFrequency),
       <span class="dt">LengthInLetters  =</span> <span class="kw">rescale</span>(LengthInLetters),
       <span class="dt">AgeSubject       =</span> <span class="kw">rescale</span>(AgeSubject))
m7 &lt;-<span class="st">  </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>WrittenFrequency +<span class="st"> </span>LengthInLetters +<span class="st"> </span>AgeSubject, english2)
<span class="kw">summary</span>(m7)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ WrittenFrequency + LengthInLetters + 
##     AgeSubject, data = english2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.34438 -0.06041 -0.00695  0.05241  0.45157 
## 
## Coefficients:
##                   Estimate Std. Error  t value Pr(&gt;|t|)    
## (Intercept)       6.550097   0.001296 5055.020   &lt;2e-16 ***
## WrittenFrequency -0.136025   0.002598  -52.366   &lt;2e-16 ***
## LengthInLetters   0.006549   0.002598    2.521   0.0117 *  
## AgeSubject       -0.221721   0.002592  -85.556   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.08758 on 4564 degrees of freedom
## Multiple R-squared:  0.6887, Adjusted R-squared:  0.6885 
## F-statistic:  3366 on 3 and 4564 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Interpretations of coefficients in the model using standardized predictors:</p>
<ul>
<li><p><code>(Intercept)</code> row: Mean RT at mean <span class="math inline">\(X_i\)</span>, for both old and young participants</p></li>
<li><p><code>WrittenFrequency</code>, <code>LengthInLetters</code> rows: Predicted change between mean(<span class="math inline">\(X_i\)</span>) and lower or upper bound of <span class="math inline">\(X_i\)</span>âs range</p></li>
<li><p><code>AgeSubject</code> row: Difference in group means between young and old participants</p></li>
</ul>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li><p>How do these interpretations differ from the coefficient interpretations for the model using raw predictors (<code>m6</code>)?</p></li>
<li><p>What is the relative importance of the three predictors for predicting reaction time?</p></li>
</ul>
</blockquote>
</div>
</div>
<div id="interim-recipe-building-a-multiple-linear-regression-model" class="section level3">
<h3><span class="header-section-number">3.5.5</span> Interim recipe: Building a multiple linear regression model</h3>
<p>Caveat: there is no single recipe for building a regression model of a dataset to address a research question! Nonetheless, here is one possible recipe, which we have given as part of linear regression project directions in classes.<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a></p>
<ol style="list-style-type: decimal">
<li><p><strong>Preliminaries</strong></p>
<ul>
<li><p>State the problem (what are the goals of this analysis?)</p></li>
<li><p>Select the reponse(s) and relevant predictors</p></li>
<li><p>Continuous variables: center and scale, possibly transform to normality</p></li>
<li><p>Categorical variables: center / choose coding scheme</p></li>
</ul></li>
<li><p><strong>Do EDA</strong>:</p>
<ul>
<li><p>What patterns are there that âshouldâ&quot; come out in a statistical model?</p></li>
<li><p>Identify potential outliers</p></li>
<li><p>(if applicable) Possible interactions? By-subject, by-item plots.</p></li>
</ul></li>
<li><p><strong>Choose models to fit</strong>, based on steps 1-2</p></li>
<li><p><strong>Fit candidate model(s)</strong></p></li>
<li><p><strong>Model criticism</strong></p>
<ul>
<li><p>Assess linearity assumption</p></li>
<li><p>Examine distribution of (standardized) residuals</p></li>
<li><p>Examine plots of residuals against fitted values, predictors</p></li>
<li><p>Look for influential points</p></li>
<li><p>Check for collinearity</p></li>
<li><p>(Other steps possibleâespecially checking model <em>robustness</em>âbut not covered in this book.)</p></li>
</ul></li>
<li><p><strong>Revise</strong>: Based on step 5, possibly:</p>
<ul>
<li><p>Exclude some data</p></li>
<li><p>Transform the predictor and/or response</p></li>
<li><p>Then re-fit the model (Step 4)</p></li>
</ul></li>
<li><p><strong>Iterate</strong>: repeat Steps 4-6 until model criticism is satisfactory</p>
<ul>
<li>(0â2 iterations)</li>
</ul></li>
</ol>
</div>
</div>
<div id="c2solns" class="section level2">
<h2><span class="header-section-number">3.6</span> Solutions</h2>
<div id="multiple-linear-regression-solutions" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Multiple linear regression: Solutions</h3>
<p><strong>Q</strong>: What does it mean that this coefficient is positive?</p>
<p><strong>A</strong>: The effect of <code>WrittenFrequency</code> is closer to zero (= less negative) for younger speakers.</p>
<div id="c2sol1" class="section level4 unnumbered">
<h4>Regression equation 1</h4>
<span class="math display">\[\begin{equation*}
  \text{Predicted } \texttt{RTlexdec} = \underbrace{6.552}_{\hat{\beta}_0} + (\underbrace{-0.011}_{\hat{\beta}_1} \cdot 3) + (\underbrace{-0.365}_{\hat{\beta}_2} \cdot \underbrace{0}_{\texttt{AgeSubject==&#39;old&#39;}}) + (\underbrace{-0.005}_{\hat{\beta}_3} \cdot 3 \cdot \underbrace{0}_{\texttt{AgeSubject==&#39;old&#39;}}) = 6.519
\end{equation*}\]</span>
</div>
<div id="c2sol2" class="section level4 unnumbered">
<h4>Regression equation 2</h4>
<span class="math display">\[\begin{equation*}
  \text{Predicted } \texttt{RTlexdec} = \underbrace{6.552}_{\hat{\beta}_0} + (\underbrace{-0.011}_{\hat{\beta}_1} \cdot 3) + (\underbrace{-0.365}_{\hat{\beta}_2} \cdot \underbrace{1}_{\texttt{AgeSubject==&#39;young&#39;}}) + (\underbrace{-0.005}_{\hat{\beta}_3} \cdot 3 \cdot \underbrace{1}_{\texttt{AgeSubject==&#39;young&#39;}}) = 6.139
\end{equation*}\]</span>
</div>
</div>
<div id="linear-regression-assumptions-solutions" class="section level3">
<h3><span class="header-section-number">3.6.2</span> Linear regression assumptions: Solutions</h3>
<p><strong>Q</strong>: Is this the case?</p>
<p><strong>A</strong>: It looks like <code>rintensity</code> and <code>rpitch</code> have a roughly linear relationship, while the relationships between <code>rpitch</code> and <code>rduration</code> as well as between <code>rduration</code> and <code>rpitch</code> are nonlinear.</p>
<hr />
<p><strong>Q</strong>: Why do the residuals have this distribution?</p>
<p><strong>A</strong>: The distribution of <span class="math inline">\(Y\)</span> is highly right-skewed: there is a lot of data with <span class="math inline">\(Y \approx 1\)</span>, and a long tail of data with <span class="math inline">\(Y \in [2,5]\)</span> (examine <code>hist(data$rhymeRating)</code>). As a result, the fitted line is near <span class="math inline">\(Y=1\)</span> for most values of <span class="math inline">\(X\)</span>, so the residuals also have a right-skewed distribution: points with <span class="math inline">\(Y \approx 1\)</span> correspond to residuals near the mode, and other points correspond to residuals in the right tail. In short, the residual distribution is highly skewed because the distribution of the response (<span class="math inline">\(Y\)</span>) is highly skewed.</p>
<hr />
<p><strong>Q</strong>: Is the linearity assumption met in each case?</p>
<p><strong>A</strong>: No (left), Yes (right). The linearity assumption isnât met when <span class="math inline">\(Y\)</span> is untransformed word frequency â or at least itâs unclear â whereas the assumption looks safe for log-transformed <span class="math inline">\(Y\)</span>.</p>
<hr />
</div>
<div id="model-comparison-solutions" class="section level3">
<h3><span class="header-section-number">3.6.3</span> Model comparison: Solutions</h3>
<!-- TODO: future -->
<!-- #### Solutions to [25 potential predictors](#c2tfpp) exercise: {-} -->
<!-- TODO ^^ -->
<p><strong>Q</strong>: What are the reduced model and full model in the <span class="math inline">\(F\)</span> test reported at the bottom of every linear regressionâs output?</p>
<p><strong>A</strong>: The full model is the model you fitted in the linear regression. The reduced model is the model including only an intercept. For example, in this model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(RTlexdec ~<span class="st"> </span>WrittenFrequency +<span class="st"> </span>AgeSubject +<span class="st"> </span>LengthInLetters, english) 
<span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RTlexdec ~ WrittenFrequency + AgeSubject + LengthInLetters, 
##     data = english)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.34438 -0.06041 -0.00695  0.05241  0.45157 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       6.8293072  0.0079946 854.245   &lt;2e-16 ***
## WrittenFrequency -0.0368919  0.0007045 -52.366   &lt;2e-16 ***
## AgeSubjectyoung  -0.2217215  0.0025915 -85.556   &lt;2e-16 ***
## LengthInLetters   0.0038897  0.0015428   2.521   0.0117 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.08758 on 4564 degrees of freedom
## Multiple R-squared:  0.6887, Adjusted R-squared:  0.6885 
## F-statistic:  3366 on 3 and 4564 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The <span class="math inline">\(F\)</span> test is for the comparison with the model <code>m0 &lt;- lm(RTlexdec ~ 1, english)</code>.</p>
<hr />
<p><strong>Q</strong>: Which model of <code>m1</code>, <code>m2</code>, <code>m3</code> (from <a href="linear-regression.html#c2ex4">the example above</a>) would you choose based on nested model comparison (<span class="math inline">\(F\)</span> test)?</p>
<p><strong>A</strong>: Model 3 &gt; Model 2 &gt; Model 1</p>
<hr />
<p><strong>Q</strong>: How do these interpretations differ from the coefficient interpretations for the model using raw predictors (m6)? What is the relative importance of the three predictors for predicting reaction time?</p>
<p><strong>A</strong>:</p>
<ul>
<li><p>Original model: change in RT per change of 1 in word frequency</p></li>
<li><p>New model: change in RT per change of 2 SD in word frequency</p></li>
</ul>
<p>The interpretation of the <code>LengthInLetters</code> coefficient is:</p>
<ul>
<li><p>Original model: change in RT for each additional letter in the word</p></li>
<li><p>New model: change in RT per change of 2 SD word length (in letters)</p></li>
</ul>
<p>The interpretation of the <code>AgeSubject</code> coefficient is the same in the two models. Although <code>AgeSubject</code> is a two-level factor in one model and a centered numeric variable in the other model, the regression coefficient captures the difference between the two levels (old and young speakers) in each case.</p>
<p>The relative importance of the three predictors follows from the magnitudes of the coefficients in the model using standardized predictors: <code>AgeSubject</code> &gt; <code>WrittenFrequency</code> &gt; <code>LengthInLetters</code>.</p>
<!-- TODO: future -->
<!-- #### Solutions to [Phrase medial devoicing](#c2ex5) exercise questions: -->
<!-- PART1: -->
<!-- **Q**: Which model ends up being chosen, and what do you conclude about the research question? -->
<!-- **A**: TODO -->
<!-- PART2:  -->
<!-- **Q**: Which model ends up being chosen, and what do you conclude about the research question? -->
<!-- **A**: TODO -->
<!-- **Q**: What would be different about your answer to the research question using stepwise backwards selection versus nested model comparison? Why? -->
<!-- **A**: TODO -->
<!-- PART3: -->
<!-- **Q**: What do you conclude about the research question, based on the final model? -->
<!-- **A**: TODO -->
<!-- **Q**:  What does the signifcant `func`:`c1` interaction mean for the research question?  (Does the effect of `func` have the same direction regardless of `c1`?) -->
<!-- **A**: TODO -->
<!-- [^1]: The studentized and standardized residuals, or "externally studentized" and "internally studentized" residuals (in @chatterjee2012regression), differ slightly in how they estimate the error variance:  a leave-one-out estimate versus an estimate using all observations. This difference shouldn't matter much except when certain observations are highly influential or in small datasets. -->
<!-- [^2]: In technical terms: the *design matrix* $X$ (where each column is the values of predictor $i$, across all observations) is not *full rank*, and the matrix $X^T X$, which is used to find the least-squares regression coefficient estimates, is *singular* (alternatively, there is a *singularity*)  Even if you don't understand the math of linear regression, it is useful to know that when you encounter these terms in R output it usually means some variable can be perfectly predicted from the others, and this is a problem. -->
<!-- [^3]: Note that you can have $R^2<1$ even when one predictor is completely predictable from other predictors---as long as this dependence is not a linear function.  Sometimes it does not make conceptual sense to have two predictors related in this way in a model (e.g. VOT and log(VOT)), while in other settings it does (e.g. linear and quadratic terms for the same variable). -->
<!-- [^4]: However, when standard errors are high, the coefficient estimates themselves are by definition very uncertain, so in this sense collinearity "affects" coefficient values. -->
<!-- [^5]: For example: are there terms that should have been included, based on previous work---regardless of the result of a model comparison?  Are there higher-order interactions with borderline significance, that could be spuriously included due to automated selection? -->

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-chatterjee2012regression">
<p>Chatterjee, S., &amp; Hadi, A. (2012). <em>Regression Analysis by Example</em> (5th ed.). Hoboken, NJ: John Wiley &amp; Sons.</p>
</div>
<div id="ref-vasishth2011foundations">
<p>Vasishth, S., &amp; Broe, M. B. (2011). <em>The foundations of statistics: A simulation-based approach</em>. Springer.</p>
</div>
<div id="ref-levy2012probabilistic">
<p>Levy, R. (2012). <em>Probabilistic methods in the study of language</em>.</p>
</div>
<div id="ref-gelman2007data">
<p>Gelman, A., &amp; Hill, J. (2007). <em>Data analysis using regression and multilevel/hierarchical models</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-baayen2008analyzing">
<p>Baayen, R. (2008). <em>Analyzing linguistic data</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-belsley1980regression">
<p>Belsley, K., D.A., &amp; Welsch, R. (1980). <em>Regression Diagnostics. Identifying Influential Data and Sources of Collinearity</em>. New York: John Wiley &amp; Sons.</p>
</div>
<div id="ref-wurm2014residualizing">
<p>Wurm, L. H., &amp; Fisicaro, S. A. (2014). What residualizing predictors in regression analyses does (and what it does not do). <em>Journal of Memory and Language</em>, <em>72</em>, 37â48.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="13">
<li id="fn13"><p>The studentized and standardized residuals, or âexternally studentizedâ and âinternally studentizedâ residuals (in <span class="citation">Chatterjee &amp; Hadi (<a href="#ref-chatterjee2012regression">2012</a>)</span>), differ slightly in how they estimate the error variance: a leave-one-out estimate versus an estimate using all observations. This difference shouldnât matter much except when certain observations are highly influential or in small datasets.<a href="linear-regression.html#fnref13">â©</a></p></li>
<li id="fn14"><p>Or another method that accounts for non-independence of errors, such as repeated measures ANOVA.<a href="linear-regression.html#fnref14">â©</a></p></li>
<li id="fn15"><p>In technical terms: the <em>design matrix</em> <span class="math inline">\(X\)</span> (where each column is the values of predictor <span class="math inline">\(i\)</span>, across all observations) is not <em>full rank</em>, and the matrix <span class="math inline">\(X^T X\)</span>, which is used to find the least-squares regression coefficient estimates, is <em>singular</em> (alternatively, there is a <em>singularity</em>) Even if you donât care about the math of linear regression, it is useful to know that when you encounter these terms in R output it usually means some variable can be perfectly predicted from the others, and this is a problem.<a href="linear-regression.html#fnref15">â©</a></p></li>
<li id="fn16"><p>Note that you can have <span class="math inline">\(R^2&lt;1\)</span> even when one predictor is completely predictable from other predictorsâas long as this dependence is not a linear function. Sometimes it does not make conceptual sense to have two predictors related in this way in a model (e.g.Â VOT and log(VOT)), while in other settings it does (e.g.Â linear and quadratic terms for the same variable).<a href="linear-regression.html#fnref16">â©</a></p></li>
<li id="fn17"><p>However, when standard errors are high, the coefficient estimates themselves are by definition very uncertain, so in this sense collinearity âaffectsâ coefficient values.<a href="linear-regression.html#fnref17">â©</a></p></li>
<li id="fn18"><p>For example: are there terms that should have been included, based on previous workâregardless of the result of a model comparison? Are there higher-order interactions with borderline significance, that could be spuriously included due to automated selection?<a href="linear-regression.html#fnref18">â©</a></p></li>
<li id="fn19"><p>Note that this recipe explicitly includes exploratory data analysis, which may not be appropriate depending on your analytic philosophy or conventions in your subfield.<a href="linear-regression.html#fnref19">â©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hypothesis-testing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cda.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
